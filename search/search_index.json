{
    "docs": [
        {
            "location": "/",
            "text": "hIPPYlib - Inverse Problem PYthon library\n\n\n\n\n\n\n \n\n\nhIPPYlib implements state-of-the-art \nscalable\n \nadjoint-based\n algorithms for PDE-based \ndeterministic and Bayesian inverse problems\n. It builds on \nFEniCS\n for the discretization of the PDE and on \nPETSc\n for scalable and efficient linear algebra operations and solvers.\n\n\nFeatures\n\n\n\n\nFriendly, compact, near-mathematical FEniCS notation to express the PDE and likelihood in weak form\n\n\nAutomatic generation of efficient code for the discretization of weak forms using FEniCS\n\n\nSymbolic differentiation of weak forms to generate derivatives and adjoint information\n\n\nGlobalized Inexact Newton-CG method to solve the inverse problem\n\n\nLow rank representation of the posterior covariace using randomized algorithms\n\n\n\n\nSee also our \ntutorial\n and list of related \npublications\n. For additional resources and tutorials please see the teaching material for the \n2018 Gene Golub SIAM Summer School\n on \nInverse Problems: Systematic Integration of Data with Models under Uncertainty\n available \nhere\n.\n\n\nThe complete API reference is available \nhere\n.\n\n\nLatest Release\n\n\n\n\nDevelopment version\n\n\nDownload \nhippylib-2.2.0.zip\n\n\nPrevious releases\n\n\n\n\nNews\n\n\n\n\nPost-Doctoral Scholar position\n opening in Prof. Petra group at UC Merced.\n\n\n\n\nContact\n\n\nDeveloped by the \nhIPPYlib team\n at \nUT Austin\n and \nUC Merced\n.\n\n\nPlease cite as \n\n\n@article{VillaPetraGhattas2016,\ntitle = \"{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Bayesian Inverse Problems}\",\nauthor = {Villa, U. and Petra, N. and Ghattas, O.},\nyear = {2016},\nurl = {http://hippylib.github.io},\ndoi = {10.5281/zenodo.596931}\n}\n\n@article{VillaPetraGhattas2018,\ntitle = \"{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Bayesian Inverse Problems}\",\nauthor = {Villa, U. and Petra, N. and Ghattas, O.},\njournal = {Journal of Open Source Software},\nvolume = {3},\nnumber = {30},\npage = {940},\ndoi  = {10.21105/joss.00940},\nyear = {2018}\n}",
            "title": "Home"
        },
        {
            "location": "/#hippylib-inverse-problem-python-library",
            "text": "hIPPYlib implements state-of-the-art  scalable   adjoint-based  algorithms for PDE-based  deterministic and Bayesian inverse problems . It builds on  FEniCS  for the discretization of the PDE and on  PETSc  for scalable and efficient linear algebra operations and solvers.",
            "title": "hIPPYlib - Inverse Problem PYthon library"
        },
        {
            "location": "/#features",
            "text": "Friendly, compact, near-mathematical FEniCS notation to express the PDE and likelihood in weak form  Automatic generation of efficient code for the discretization of weak forms using FEniCS  Symbolic differentiation of weak forms to generate derivatives and adjoint information  Globalized Inexact Newton-CG method to solve the inverse problem  Low rank representation of the posterior covariace using randomized algorithms   See also our  tutorial  and list of related  publications . For additional resources and tutorials please see the teaching material for the  2018 Gene Golub SIAM Summer School  on  Inverse Problems: Systematic Integration of Data with Models under Uncertainty  available  here .  The complete API reference is available  here .",
            "title": "Features"
        },
        {
            "location": "/#latest-release",
            "text": "Development version  Download  hippylib-2.2.0.zip  Previous releases",
            "title": "Latest Release"
        },
        {
            "location": "/#news",
            "text": "Post-Doctoral Scholar position  opening in Prof. Petra group at UC Merced.",
            "title": "News"
        },
        {
            "location": "/#contact",
            "text": "Developed by the  hIPPYlib team  at  UT Austin  and  UC Merced .  Please cite as   @article{VillaPetraGhattas2016,\ntitle = \"{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Bayesian Inverse Problems}\",\nauthor = {Villa, U. and Petra, N. and Ghattas, O.},\nyear = {2016},\nurl = {http://hippylib.github.io},\ndoi = {10.5281/zenodo.596931}\n}\n\n@article{VillaPetraGhattas2018,\ntitle = \"{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Bayesian Inverse Problems}\",\nauthor = {Villa, U. and Petra, N. and Ghattas, O.},\njournal = {Journal of Open Source Software},\nvolume = {3},\nnumber = {30},\npage = {940},\ndoi  = {10.21105/joss.00940},\nyear = {2018}\n}",
            "title": "Contact"
        },
        {
            "location": "/tutorial/",
            "text": "Tutorial\n\n\nThese tutorials are the best place to learn about the basic features and the algorithms in \nhIPPYlib\n.\nFor the complete API reference click \nhere\n.\n\n\n\n\nFEniCS101\n notebook illustrates the use of FEniCS for the solution of a linear boundary value problem.\n\n\nPoisson Deterministic\n notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting.\n\n\nSubsurface Bayesian\n notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting.\n\n\nAdvection-Diffusion Bayesian\n notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting.\n\n\nHessian Spectrum\n notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.\n\n\n\n\nThe interactive ipython notebooks are located in the \ntutorial\n folder of the \nhIPPYlib\n release.\n\n\nTo run the notebooks follow these instructions.\n\n\n\n\nOpen a FEniCS terminal and type\n\n\n\n\n$ cd tutorial\n$ jupyter notebook\n\n\n\n\n\n\nA new tab will open in your web-brower showing the notebooks.\n\n\nClick on the notebook you would like to use.\n\n\nTo run all the code in the notebook simply click on Cell --> Run All.\n\n\n\n\nFor more information on installing ipython and using notebooks see \nhere\n.\n\n\nAdditional resources\n\n\nFor additional resources and tutorials please see the teaching material for the\n\n2018 Gene Golub SIAM Summer School\n on \nInverse Problems: Systematic Integration of Data with Models under Uncertainty\n available \nhere\n.\n\n\nTutorial for \nhIPPYlib\n 1.x\n\n\nFor \nhIPPYlib\n version 1.6.0 see \nhere",
            "title": "README"
        },
        {
            "location": "/tutorial/#tutorial",
            "text": "These tutorials are the best place to learn about the basic features and the algorithms in  hIPPYlib .\nFor the complete API reference click  here .   FEniCS101  notebook illustrates the use of FEniCS for the solution of a linear boundary value problem.  Poisson Deterministic  notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting.  Subsurface Bayesian  notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting.  Advection-Diffusion Bayesian  notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting.  Hessian Spectrum  notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.   The interactive ipython notebooks are located in the  tutorial  folder of the  hIPPYlib  release.  To run the notebooks follow these instructions.   Open a FEniCS terminal and type   $ cd tutorial\n$ jupyter notebook   A new tab will open in your web-brower showing the notebooks.  Click on the notebook you would like to use.  To run all the code in the notebook simply click on Cell --> Run All.   For more information on installing ipython and using notebooks see  here .",
            "title": "Tutorial"
        },
        {
            "location": "/tutorial/#additional-resources",
            "text": "For additional resources and tutorials please see the teaching material for the 2018 Gene Golub SIAM Summer School  on  Inverse Problems: Systematic Integration of Data with Models under Uncertainty  available  here .",
            "title": "Additional resources"
        },
        {
            "location": "/tutorial/#tutorial-for-hippylib-1x",
            "text": "For  hIPPYlib  version 1.6.0 see  here",
            "title": "Tutorial for hIPPYlib 1.x"
        },
        {
            "location": "/tutorials/1_FEniCS101/",
            "text": "FEniCS101 Tutorial\n\n\nIn this tutorial we consider the boundary value problem (BVP)\n\n\n\n\n\\begin{eqnarray*}\n- \\nabla \\cdot (k \\nabla u) = f &      \\text{ in } \\Omega,\\\\\nu = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\\nk \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom},\n\\end{eqnarray*}\n\n\n\n\nwhere \n\\Omega = (0,1) \\times (0,1)\n, \n\\Gamma_D\n and and \n\\Gamma_N\n are the union of\nthe left and right, and top and bottom boundaries of \n\\Omega\n,\nrespectively.\n\n\nHere\n\n\\begin{eqnarray*}\nk(x,y) = 1  & \\text{ on } \\Omega\\\\\nf(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\\nu_0(x,y)      = 0 & \\text{ on } \\Gamma_D, \\\\\n\\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right.\n& \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array}\n\\end{eqnarray*}\n\n\n\n\nThe exact solution is\n\n u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right). \n\n\n\n\nWeak formulation\n\n\nLet us define the Hilbert spaces \nV_{u_0}, V_0 \\in \\Omega\n as\n\n V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\},\n\n\n V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}.\n\n\n\n\nTo obtain the weak formulation, we multiply the PDE by an arbitrary function \nv \\in V_0\n and integrate over the domain \n\\Omega\n leading to\n\n\n\n\n -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. \n\n\n\n\nThen, integration by parts the non-conforming term gives\n\n\n\n\n \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. \n\n\n\n\nFinally by recalling that \n v = 0 \n on \n\\Gamma_D\n and that \nk \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma \n on \n\\Gamma_N\n, we find the weak formulation:\n\n\nFind * \nu \\in V_{u_0}\n \nsuch that*\n\n \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0. \n\n\n\n\n1. Load modules\n\n\nTo start we load the following modules:\n\n\n\n\n\n\ndolfin: the python/C++ interface to FEniCS\n\n\n\n\n\n\nmath\n: the python module for mathematical functions\n\n\n\n\n\n\nnumpy\n: a python package for linear algebra\n\n\n\n\n\n\nmatplotlib\n: a python package used for plotting the results\n\n\n\n\n\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom dolfin import *\n\nimport math\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import nb\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\n\n\n\n2. Define the mesh and the finite element space\n\n\nWe construct a triangulation (mesh) \n\\mathcal{T}_h\n of the computational domain \n\\Omega := [0, 1]^2\n with \nn\n elements in each direction.\n\n\nOn the mesh \n\\mathcal{T}_h\n, we then define the finite element space \nV_h \\subset H^1(\\Omega)\n consisting of globally continuous piecewise polinomials functions. The \ndegree\n variable defines the polinomial degree.\n\n\nn = 16\ndegree = 1\nmesh = UnitSquareMesh(n, n)\nnb.plot(mesh)\n\nVh  = FunctionSpace(mesh, 'Lagrange', degree)\nprint( \"dim(Vh) = \", Vh.dim() )\n\n\n\n\ndim(Vh) =  289\n\n\n\n\n\n3. Define boundary labels\n\n\nTo partition the boundary of \n\\Omega\n in the subdomains \n\\Gamma_{\\rm top}\n, \n\\Gamma_{\\rm bottom}\n, \n\\Gamma_{\\rm left}\n, \n\\Gamma_{\\rm right}\n we assign a unique label \nboundary_parts\n to each of part of \n\\partial \\Omega\n.\n\n\nclass TopBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1] - 1) < DOLFIN_EPS\n\nclass BottomBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1]) < DOLFIN_EPS\n\nclass LeftBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0]) < DOLFIN_EPS\n\nclass RightBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0] - 1) < DOLFIN_EPS\n\nboundary_parts = FacetFunction(\"size_t\", mesh)\nboundary_parts.set_all(0)\n\nGamma_top = TopBoundary()\nGamma_top.mark(boundary_parts, 1)\nGamma_bottom = BottomBoundary()\nGamma_bottom.mark(boundary_parts, 2)\nGamma_left = LeftBoundary()\nGamma_left.mark(boundary_parts, 3)\nGamma_right = RightBoundary()\nGamma_right.mark(boundary_parts, 4)\n\n\n\n\n4. Define the coefficients of the PDE and the boundary conditions\n\n\nWe first define the coefficients of the PDE using the \nConstant\n and \nExpression\n classes. \nConstant\n is used to define coefficients that do not depend on the space coordinates, \nExpression\n is used to define coefficients that are a known function of the space coordinates \nx[0]\n (x-axis direction) and \nx[1]\n (y-axis direction).\n\n\nIn the finite element method community, Dirichlet boundary conditions are also known as \nessential\n boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class \nDirichletBC\n to indicate this type of condition.\n\n\nOn the other hand, Newman boundary conditions are also known as \nnatural\n boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure \nds[i]\n to integrate over the portion of the boundary marked with label \ni\n.\n\n\nu_L = Constant(0.)\nu_R = Constant(0.)\n\nsigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5)\nsigma_top    = Constant(0.)\n\nf = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5)\n\nbcs = [DirichletBC(Vh, u_L, boundary_parts, 3),\n       DirichletBC(Vh, u_R, boundary_parts, 4)]\n\nds = Measure(\"ds\", subdomain_data=boundary_parts)\n\n\n\n\n5. Define and solve the variational problem\n\n\nWe also define two special types of functions: the \nTrialFunction\n \nu\n and the \nTestFunction\n \nv\n. These special types of function are used by \nFEniCS\n to generate the finite element vectors and matrices which stem from the weak formulation of the PDE.\n\n\nMore specifically, by denoting by \n\\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)}\n the finite element basis for the space \nV_h\n, a function \nu_h \\in V_h\n can be written as\n\n u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), \n\nwhere \n{\\rm u}_i\n represents the coefficients in the finite element expansion of \nu_h\n.\n\n\nWe then define\n\n\n\n\n\n\nthe bilinear form \na(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx \n;\n\n\n\n\n\n\nthe linear form \nL(v_h) = \\int_\\Omega f v_h dx + + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds \n.\n\n\n\n\n\n\nWe can then solve the variational problem\n\n\nFind \nu_h \\in V_h\n\n\n such that\n\n\n a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h \n\n\n\n\nusing directly the built-in \nsolve\n method in FEniCS.\n\n\nNOTE:\n As an alternative one can also assemble the finite element matrix \nA\n and the right hand side \nb\n that stems from the discretization of \na\n and \nL\n, and then solve the linear system\n\n A {\\rm u} = {\\rm b}, \n\nwhere\n\n\n\n\n\n\n\n\n{\\rm u}\n is the vector collecting the coefficient of the finite element expasion of \nu_h\n,\n\n\n\n\n\n\nthe entries of the matrix A are such that \nA_{ij} = a(\\phi_j, \\phi_i)\n,\n\n\n\n\n\n\nthe entries of the right hand side b are such that \nb_i = L(\\phi_i)\n.\n\n\n\n\n\n\nu = TrialFunction(Vh)\nv = TestFunction(Vh)\na = inner(nabla_grad(u), nabla_grad(v))*dx\nL = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n\nuh = Function(Vh)\n\n#solve(a == L, uh, bcs=bcs)\nA, b = assemble_system(a,L, bcs=bcs)\nsolve(A, uh.vector(), b, \"cg\")\n\nnb.plot(uh)\n\n\n\n\n<matplotlib.collections.TriMesh at 0x7f9ae232f630>\n\n\n\n\n\n6. Compute the discretization error\n\n\nFor this problem, the exact solution is known.\nWe can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution \nu_h\n and the exact solution \nu_{\\rm ex}\n)\n\n \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, \n \nand\n\n \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. \n\n\n\n\nu_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5)\ngrad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5)\n\nerr_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\nerr_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\nerr_H1 = sqrt( err_L2**2 + err_grad**2)\n\nprint (\"|| u_h - u_e ||_L2 = \", err_L2)\nprint (\"|| u_h - u_e ||_H1 = \", err_H1)\n\n\n\n\n|| u_h - u_e ||_L2 =  0.008805253722075487\n|| u_h - u_e ||_H1 =  0.3967189525141412\n\n\n\n7. Convergence of the finite element method\n\n\nWe now verify numerically a well-known convergence result for the finite element method.\n\n\nLet denote with \ns\n the polynomial degree of the finite element space, and assume that the solution \nu_{\\rm ex}\n is at least in \nH^{s+1}(\\Omega)\n. Then we have\n\n \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. \n\n\n\n\nIn the code below, the function \ncompute(n, degree)\n solves the PDE using a mesh with \nn\n elements in each direction and finite element spaces of polinomial order \ndegree\n.\n\n\nThe figure below shows the discretization errors in the \nH^1\n and \nL^2\n as a function of the mesh size \nh\n (\nh = \\frac{1}{n}\n) for piecewise linear (P1, \ns=1\n) and piecewise quadratic (P2, \ns=2\n) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular:\n\n\n\n\n\n\nfor piecewise linear finite element P1 we observe first order convergence in the \nH^1\n-norm and second order convergence in the \nL^2\n-norm;\n\n\n\n\n\n\nfor piecewise quadratic finite element P2 we observe second order convergence in the \nH^1\n-norm and third order convergence in the \nL^2\n-norm.\n\n\n\n\n\n\ndef compute(n, degree):\n    mesh = UnitSquareMesh(n, n)\n    Vh  = FunctionSpace(mesh, 'Lagrange', degree)\n    boundary_parts = FacetFunction(\"size_t\", mesh)\n    boundary_parts.set_all(0)\n\n    Gamma_top = TopBoundary()\n    Gamma_top.mark(boundary_parts, 1)\n    Gamma_bottom = BottomBoundary()\n    Gamma_bottom.mark(boundary_parts, 2)\n    Gamma_left = LeftBoundary()\n    Gamma_left.mark(boundary_parts, 3)\n    Gamma_right = RightBoundary()\n    Gamma_right.mark(boundary_parts, 4)\n\n    bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)]\n    ds = Measure(\"ds\", subdomain_data=boundary_parts)\n\n    u = TrialFunction(Vh)\n    v = TestFunction(Vh)\n    a = inner(nabla_grad(u), nabla_grad(v))*dx\n    L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n    uh = Function(Vh)\n    solve(a == L, uh, bcs=bcs)\n    err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\n    err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\n    err_H1 = sqrt( err_L2**2 + err_grad**2)\n\n    return err_L2, err_H1\n\nnref = 5\nn = 8*np.power(2,np.arange(0,nref))\nh = 1./n\n\nerr_L2_P1 = np.zeros(nref)\nerr_H1_P1 = np.zeros(nref)\nerr_L2_P2 = np.zeros(nref)\nerr_H1_P2 = np.zeros(nref)\n\nfor i in range(nref):\n    err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1)\n    err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nplt.loglog(h, err_H1_P1, '-or', label = \"H1 error\")\nplt.loglog(h, err_L2_P1, '-*b', label = \"L2 error\")\nplt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label = \"First Order\")\nplt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label = \"Second Order\")\nplt.xlabel(\"Mesh size h\")\nplt.ylabel(\"Error\")\nplt.title(\"P1 Finite Element\")\nplt.legend(loc='lower right')\n\n\nplt.subplot(122)\nplt.loglog(h, err_H1_P2, '-or', label = \"H1 error\")\nplt.loglog(h, err_L2_P2, '-*b', label = \"L2 error\")\nplt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label = \"Second Order\")\nplt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label = \"Third Order\")\nplt.xlabel(\"Mesh size h\")\nplt.ylabel(\"Error\")\nplt.title(\"P2 Finite Element\")\nplt.legend(loc='lower right')\n\nplt.show()\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\n\nAll Rights reserved.\n\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "FEniCS101"
        },
        {
            "location": "/tutorials/1_FEniCS101/#fenics101-tutorial",
            "text": "In this tutorial we consider the boundary value problem (BVP)   \\begin{eqnarray*}\n- \\nabla \\cdot (k \\nabla u) = f &      \\text{ in } \\Omega,\\\\\nu = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\\nk \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom},\n\\end{eqnarray*}   where  \\Omega = (0,1) \\times (0,1) ,  \\Gamma_D  and and  \\Gamma_N  are the union of\nthe left and right, and top and bottom boundaries of  \\Omega ,\nrespectively.  Here \\begin{eqnarray*}\nk(x,y) = 1  & \\text{ on } \\Omega\\\\\nf(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\\nu_0(x,y)      = 0 & \\text{ on } \\Gamma_D, \\\\\n\\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right.\n& \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array}\n\\end{eqnarray*}   The exact solution is  u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right).",
            "title": "FEniCS101 Tutorial"
        },
        {
            "location": "/tutorials/1_FEniCS101/#weak-formulation",
            "text": "Let us define the Hilbert spaces  V_{u_0}, V_0 \\in \\Omega  as  V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\},   V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}.   To obtain the weak formulation, we multiply the PDE by an arbitrary function  v \\in V_0  and integrate over the domain  \\Omega  leading to    -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0.    Then, integration by parts the non-conforming term gives    \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0.    Finally by recalling that   v = 0   on  \\Gamma_D  and that  k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma   on  \\Gamma_N , we find the weak formulation:  Find *  u \\in V_{u_0}   such that*  \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0.",
            "title": "Weak formulation"
        },
        {
            "location": "/tutorials/1_FEniCS101/#1-load-modules",
            "text": "To start we load the following modules:    dolfin: the python/C++ interface to FEniCS    math : the python module for mathematical functions    numpy : a python package for linear algebra    matplotlib : a python package used for plotting the results    from __future__ import absolute_import, division, print_function\n\nfrom dolfin import *\n\nimport math\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import nb\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorials/1_FEniCS101/#2-define-the-mesh-and-the-finite-element-space",
            "text": "We construct a triangulation (mesh)  \\mathcal{T}_h  of the computational domain  \\Omega := [0, 1]^2  with  n  elements in each direction.  On the mesh  \\mathcal{T}_h , we then define the finite element space  V_h \\subset H^1(\\Omega)  consisting of globally continuous piecewise polinomials functions. The  degree  variable defines the polinomial degree.  n = 16\ndegree = 1\nmesh = UnitSquareMesh(n, n)\nnb.plot(mesh)\n\nVh  = FunctionSpace(mesh, 'Lagrange', degree)\nprint( \"dim(Vh) = \", Vh.dim() )  dim(Vh) =  289",
            "title": "2. Define the mesh and the finite element space"
        },
        {
            "location": "/tutorials/1_FEniCS101/#3-define-boundary-labels",
            "text": "To partition the boundary of  \\Omega  in the subdomains  \\Gamma_{\\rm top} ,  \\Gamma_{\\rm bottom} ,  \\Gamma_{\\rm left} ,  \\Gamma_{\\rm right}  we assign a unique label  boundary_parts  to each of part of  \\partial \\Omega .  class TopBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1] - 1) < DOLFIN_EPS\n\nclass BottomBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1]) < DOLFIN_EPS\n\nclass LeftBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0]) < DOLFIN_EPS\n\nclass RightBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0] - 1) < DOLFIN_EPS\n\nboundary_parts = FacetFunction(\"size_t\", mesh)\nboundary_parts.set_all(0)\n\nGamma_top = TopBoundary()\nGamma_top.mark(boundary_parts, 1)\nGamma_bottom = BottomBoundary()\nGamma_bottom.mark(boundary_parts, 2)\nGamma_left = LeftBoundary()\nGamma_left.mark(boundary_parts, 3)\nGamma_right = RightBoundary()\nGamma_right.mark(boundary_parts, 4)",
            "title": "3. Define boundary labels"
        },
        {
            "location": "/tutorials/1_FEniCS101/#4-define-the-coefficients-of-the-pde-and-the-boundary-conditions",
            "text": "We first define the coefficients of the PDE using the  Constant  and  Expression  classes.  Constant  is used to define coefficients that do not depend on the space coordinates,  Expression  is used to define coefficients that are a known function of the space coordinates  x[0]  (x-axis direction) and  x[1]  (y-axis direction).  In the finite element method community, Dirichlet boundary conditions are also known as  essential  boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class  DirichletBC  to indicate this type of condition.  On the other hand, Newman boundary conditions are also known as  natural  boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure  ds[i]  to integrate over the portion of the boundary marked with label  i .  u_L = Constant(0.)\nu_R = Constant(0.)\n\nsigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5)\nsigma_top    = Constant(0.)\n\nf = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5)\n\nbcs = [DirichletBC(Vh, u_L, boundary_parts, 3),\n       DirichletBC(Vh, u_R, boundary_parts, 4)]\n\nds = Measure(\"ds\", subdomain_data=boundary_parts)",
            "title": "4. Define the coefficients of the PDE and the boundary conditions"
        },
        {
            "location": "/tutorials/1_FEniCS101/#5-define-and-solve-the-variational-problem",
            "text": "We also define two special types of functions: the  TrialFunction   u  and the  TestFunction   v . These special types of function are used by  FEniCS  to generate the finite element vectors and matrices which stem from the weak formulation of the PDE.  More specifically, by denoting by  \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)}  the finite element basis for the space  V_h , a function  u_h \\in V_h  can be written as  u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x),  \nwhere  {\\rm u}_i  represents the coefficients in the finite element expansion of  u_h .  We then define    the bilinear form  a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx  ;    the linear form  L(v_h) = \\int_\\Omega f v_h dx + + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds  .    We can then solve the variational problem  Find  u_h \\in V_h   such that   a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h    using directly the built-in  solve  method in FEniCS.  NOTE:  As an alternative one can also assemble the finite element matrix  A  and the right hand side  b  that stems from the discretization of  a  and  L , and then solve the linear system  A {\\rm u} = {\\rm b},  \nwhere     {\\rm u}  is the vector collecting the coefficient of the finite element expasion of  u_h ,    the entries of the matrix A are such that  A_{ij} = a(\\phi_j, \\phi_i) ,    the entries of the right hand side b are such that  b_i = L(\\phi_i) .    u = TrialFunction(Vh)\nv = TestFunction(Vh)\na = inner(nabla_grad(u), nabla_grad(v))*dx\nL = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n\nuh = Function(Vh)\n\n#solve(a == L, uh, bcs=bcs)\nA, b = assemble_system(a,L, bcs=bcs)\nsolve(A, uh.vector(), b, \"cg\")\n\nnb.plot(uh)  <matplotlib.collections.TriMesh at 0x7f9ae232f630>",
            "title": "5. Define and solve the variational problem"
        },
        {
            "location": "/tutorials/1_FEniCS101/#6-compute-the-discretization-error",
            "text": "For this problem, the exact solution is known.\nWe can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution  u_h  and the exact solution  u_{\\rm ex} )  \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx },   \nand  \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}.    u_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5)\ngrad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5)\n\nerr_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\nerr_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\nerr_H1 = sqrt( err_L2**2 + err_grad**2)\n\nprint (\"|| u_h - u_e ||_L2 = \", err_L2)\nprint (\"|| u_h - u_e ||_H1 = \", err_H1)  || u_h - u_e ||_L2 =  0.008805253722075487\n|| u_h - u_e ||_H1 =  0.3967189525141412",
            "title": "6. Compute the discretization error"
        },
        {
            "location": "/tutorials/1_FEniCS101/#7-convergence-of-the-finite-element-method",
            "text": "We now verify numerically a well-known convergence result for the finite element method.  Let denote with  s  the polynomial degree of the finite element space, and assume that the solution  u_{\\rm ex}  is at least in  H^{s+1}(\\Omega) . Then we have  \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}.    In the code below, the function  compute(n, degree)  solves the PDE using a mesh with  n  elements in each direction and finite element spaces of polinomial order  degree .  The figure below shows the discretization errors in the  H^1  and  L^2  as a function of the mesh size  h  ( h = \\frac{1}{n} ) for piecewise linear (P1,  s=1 ) and piecewise quadratic (P2,  s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular:    for piecewise linear finite element P1 we observe first order convergence in the  H^1 -norm and second order convergence in the  L^2 -norm;    for piecewise quadratic finite element P2 we observe second order convergence in the  H^1 -norm and third order convergence in the  L^2 -norm.    def compute(n, degree):\n    mesh = UnitSquareMesh(n, n)\n    Vh  = FunctionSpace(mesh, 'Lagrange', degree)\n    boundary_parts = FacetFunction(\"size_t\", mesh)\n    boundary_parts.set_all(0)\n\n    Gamma_top = TopBoundary()\n    Gamma_top.mark(boundary_parts, 1)\n    Gamma_bottom = BottomBoundary()\n    Gamma_bottom.mark(boundary_parts, 2)\n    Gamma_left = LeftBoundary()\n    Gamma_left.mark(boundary_parts, 3)\n    Gamma_right = RightBoundary()\n    Gamma_right.mark(boundary_parts, 4)\n\n    bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)]\n    ds = Measure(\"ds\", subdomain_data=boundary_parts)\n\n    u = TrialFunction(Vh)\n    v = TestFunction(Vh)\n    a = inner(nabla_grad(u), nabla_grad(v))*dx\n    L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n    uh = Function(Vh)\n    solve(a == L, uh, bcs=bcs)\n    err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\n    err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\n    err_H1 = sqrt( err_L2**2 + err_grad**2)\n\n    return err_L2, err_H1\n\nnref = 5\nn = 8*np.power(2,np.arange(0,nref))\nh = 1./n\n\nerr_L2_P1 = np.zeros(nref)\nerr_H1_P1 = np.zeros(nref)\nerr_L2_P2 = np.zeros(nref)\nerr_H1_P2 = np.zeros(nref)\n\nfor i in range(nref):\n    err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1)\n    err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nplt.loglog(h, err_H1_P1, '-or', label = \"H1 error\")\nplt.loglog(h, err_L2_P1, '-*b', label = \"L2 error\")\nplt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label = \"First Order\")\nplt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label = \"Second Order\")\nplt.xlabel(\"Mesh size h\")\nplt.ylabel(\"Error\")\nplt.title(\"P1 Finite Element\")\nplt.legend(loc='lower right')\n\n\nplt.subplot(122)\nplt.loglog(h, err_H1_P2, '-or', label = \"H1 error\")\nplt.loglog(h, err_L2_P2, '-*b', label = \"L2 error\")\nplt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label = \"Second Order\")\nplt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label = \"Third Order\")\nplt.xlabel(\"Mesh size h\")\nplt.ylabel(\"Error\")\nplt.title(\"P2 Finite Element\")\nplt.legend(loc='lower right')\n\nplt.show()   Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. \nAll Rights reserved. \nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "7. Convergence of the finite element method"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/",
            "text": "Coefficient field inversion in an elliptic partial differential equation\n\n\nWe consider the estimation of a coefficient in an elliptic partial\ndifferential equation as a model problem. Depending on the\ninterpretation of the unknowns and the type of measurements, this\nmodel problem arises, for instance, in inversion for groundwater flow\nor heat conductivity.  It can also be interpreted as finding a\nmembrane with a certain spatially varying stiffness. Let\n\n\\Omega\\subset\\mathbb{R}^n\n, \nn\\in\\{1,2,3\\}\n be an open, bounded\ndomain and consider the following problem:\n\n\n\n\n\n\\min_{m} J(m):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx,\n\n\n\n\n\nwhere \nu\n is the solution of\n\n\n\n\n\n\\begin{split}\n\\quad -\\nabla\\cdot(\\exp(m)\\nabla u) &= f \\text{ in }\\Omega,\\\\\nu &= 0 \\text{ on }\\partial\\Omega.\n\\end{split}\n\n\n\n\n\nHere \nm\\in U_{ad}:=\\{m\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\}\n the unknown coefficient field, \nu_d\n denotes (possibly noisy) data, \nf\\in H^{-1}(\\Omega)\n a given force, and \n\\gamma\\ge 0\n the regularization parameter.\n\n\nThe variational (or weak) form of the state equation:\n\n\nFind \nu\\in H_0^1(\\Omega)\n such that \n\n\n\n\n(\\exp(m)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega),\n\n\n\n\nwhere \nH_0^1(\\Omega)\n is the space of functions vanishing on \n\\partial\\Omega\n with square integrable derivatives. Here, \n(\\cdot\\,,\\cdot)\n denotes the \nL^2\n-inner product, i.e, for scalar functions \nu,v \\in L^2(\\Omega)\n  we denote \n\n\n\n\n(u,v) := \\int_\\Omega u(x) v(x) \\,dx.\n\n\n\n\nGradient evaluation:\n\n\nThe Lagrangian functional \n\\mathscr{L}:H_0^1(\\Omega)\\times H^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R}\n is given by\n\n\n\n\n\n\\mathscr{L}(u,m,p):= \\frac{1}{2}(u-u_d,u-u_d) +\n\\frac{\\gamma}{2}(\\nabla m, \\nabla m) +  (\\exp(m)\\nabla u,\\nabla p) - (f,p).\n\n\n\n\n\nThen the gradient of the cost functional \n\\mathcal{J}(m)\n with respect to the parameter \nm\n is\n\n\n\n\n\n    \\mathcal{G}(m)(\\tilde m) := \\gamma(\\nabla m, \\nabla \\tilde{m}) +\n     (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) \\quad \\forall \\tilde{m} \\in H^1(\\Omega),\n\n\n\n\n\nwhere \nu \\in H_0^1(\\Omega)\n is the solution of the forward problem,\n\n\n\n\n \\mathscr{L}_p(u,m,p)(\\tilde{p})  := (\\exp(m)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) = 0\n\\quad \\forall \\tilde{p} \\in H_0^1(\\Omega), \n\n\n\n\nand \np \\in H_0^1(\\Omega)\n is the solution of the adjoint problem,\n\n\n\n\n \\mathscr{L}_u(u,m,p)(\\tilde{u}) := (\\exp(m)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) = 0\n\\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).\n\n\n\n\nHessian action:\n\n\nTo evaluate the action \n\\mathcal{H}(m)(\\hat{m})\n of the Hessian is a given direction \n\\hat{m}\n , we consider variations of the meta-Lagrangian functional\n\n\n\n\n\n\\begin{aligned}\n\\mathscr{L}^H(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}) := & {} & {} \\\\\n{} & \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) & \\text{gradient}\\\\\n{} & + (\\exp(m)\\nabla u, \\nabla \\hat{p}) - (f,\\hat{p}) & \\text{forward eq}\\\\\n{} & + (\\exp(m)\\nabla p, \\nabla \\hat{u}) + (u-u_d,\\hat{u}) & \\text{adjoint eq}.\n\\end{aligned}\n\n\n\n\n\nThen action of the Hessian is a given direction \n\\hat{m}\n is\n\n\n\n\n\n\\begin{aligned}\n(\\tilde{m}, \\mathcal{H}(m)(\\hat{m}) ) & := \\mathscr{L}^H_m(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{m}) \\\\\n{} & =\n(\\tilde{m} \\exp(m) \\nabla \\hat{u}, \\nabla{p}) + \\gamma (\\nabla \\hat{m}, \\nabla \\tilde{m}) + (\\tilde{m} \\hat{m} \\exp(m)\\nabla u, \\nabla p) + (\\tilde{m}\\exp(m) \\nabla u, \\nabla \\hat{p}) \\quad \\forall \\tilde{m} \\in H^1(\\Omega),\n\\end{aligned}\n\n\n\n\n\nwhere \n\n\n\n\n\n\n\n\nu\\in H^1_0(\\Omega)\n and \np \\in H^1_0(\\Omega)\n are the solution of the forward and adjoint problem, respectively;\n\n\n\n\n\n\n\n\n\\hat{u} \\in H^1_0(\\Omega)\n is the solution of the incremental forward problem,\n\n\n\n\n\n\n\n\n\n\\mathscr{L}^H_p(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{p}) := (\\exp(m) \\nabla \\hat{u}, \\nabla \\tilde{p}) + (\\hat{m} \\exp(m) \\nabla u, \\nabla \\tilde p) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega);\n\n\n\n\n\n\n\nand \n\\hat{p} \\in H^1_0(\\Omega)\n is the solution of the incremental adjoint problem,\n\n\n\\mathscr{L}^H_u(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{u}) := (\\hat{u}, \\tilde{u}) + (\\hat{m} \\exp(m)\\nabla p, \\nabla \\tilde{u}) + (\\exp(m) \\nabla \\tilde u, \\nabla \\hat{p}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).\n\n\n\n\n\n\n\nInexact Newton-CG:\n\n\nWritten in abstract form, the Newton Method computes an update direction \n\\hat{m}_k\n by solving the linear system \n\n\n\n\n\n(\\tilde{m}, \\mathcal{H}(m_k)(\\hat{m}_k) ) = -\\mathcal{G}(m_k)(\\tilde m) \\quad \\forall \\tilde{m} \\in H^1(\\Omega),\n\n\n\n\n\nwhere the evaluation of the gradient \n\\mathcal{G}(m_k)\n involve the solution \nu_k\n and \np_k\n of the forward and adjoint problem (respectively) for \nm = m_k\n.\nSimilarly, the Hessian action \n\\mathcal{H}(m_k)(\\hat{m}_k)\n requires to additional solve the incremental forward and adjoint problems.\n\n\nDiscrete Newton system:\n\n\n\n\n\n\\def\\tu{\\tilde u}\n\\def\\tm{\\tilde m}\n\\def\\tp{\\tilde p}\n\\def\\hu{\\hat u}\n\\def\\hp{\\hat p}\n\\def\\hm{\\hat m}\n\n\n\n\n\n\n\n\n\\def\\bu{{\\bf u}}\n\\def\\bm{{\\bf m}}\n\\def\\bp{{\\bf p}}\n\\def\\btu{{\\bf \\tilde u}}\n\\def\\btm{{\\bf \\tilde m}}\n\\def\\btp{{\\bf \\tilde p}}\n\\def\\bhu{{\\bf \\hat u}}\n\\def\\bhm{{\\bf \\hat m}}\n\\def\\bhp{{\\bf \\hat p}}\n\\def\\bg{{\\bf g}}\n\n\n\n\n\n\n\n\n\\def\\bA{{\\bf A}}\n\\def\\bC{{\\bf C}}\n\\def\\bH{{\\bf H}}\n\\def\\bR{{\\bf R}}\n\\def\\bW{{\\bf W}}\n\n\n\n\n\nLet us denote the vectors corresponding to the discretization of the functions \nu_k, m_k, p_k\n by \n\\bu_k, \\bm_k, \\bp_k\n and of the functions \n\\hu_k, \\hm_k, \\hp_k\n by \n\\bhu_k, \\bhm_k,\\bhp_k\n.\n\n\nThen, the discretization of the above system is given by the following symmetric linear system:\n\n\n\n\n\n  \\bH_k \\, \\bhm_k = -\\bg_k.\n\n\n\n\n\nThe gradient \n\\bg_k\n is computed using the following three steps\n\n\n\n\nGiven \n\\bm_k\n we solve the forward problem\n\n\n\n\n\n\n \\bA_k \\bu_k = {\\bf f}, \n\n\n\n\nwhere \n\\bA_k \\bu_k\n stems from the discretization \n(\\exp(m_k)\\nabla u_k, \\nabla \\tilde{p})\n, and \n{\\bf f}\n stands for the discretization of the right hand side \nf\n.\n\n\n\n\nGiven \n\\bm_k\n and \n\\bu_k\n solve the adjoint problem\n\n\n\n\n\n\n \\bA_k^T \\bp_k = - \\bW_{\\scriptsize\\mbox{uu}}\\,(\\bu_k-\\bu_d) \n\n\n\n\nwhere \n\\bA_k^T \\bp_k\n stems from the discretization of \n(\\exp(m_k)\\nabla \\tilde{u}, \\nabla p_k)\n, \n\\bW_{\\scriptsize\\mbox{uu}}\n is the mass matrix corresponding to the \nL^2\n inner product in the state space, and \n\\bu_d\n stems from the data.\n\n\n\n\nDefine the gradient \n\n\n\n\n\n\n \\bg_k = \\bR \\bm_k + \\bC_k^T \\bp_k, \n\n\n\n\nwhere \n\\bR\n is the matrix stemming from discretization of the regularization operator \n\\gamma ( \\nabla \\hat{m}, \\nabla \\tilde{m})\n, and \n\\bC_k\n stems from discretization of the term \n(\\tilde{m}\\exp(m_k)\\nabla u_k, \\nabla p_k)\n.\n\n\nSimilarly the action of the Hessian \n\\bH_k \\, \\bhm_k\n in a direction \n\\bhm_k\n (by using the CG algorithm we only need the action of \n\\bH_k\n to solve the Newton step) is given by\n\n\n\n\nSolve the incremental forward problem\n\n\n\n\n\n\n \\bA_k \\bhu_k = -\\bC_k \\bhm_k, \n\n\n\n\nwhere \n\\bC_k \\bm_k\n stems from discretization of \n(\\hat{m} \\exp(m_k) \\nabla u_k, \\nabla \\tilde p)\n.\n\n\n\n\nSolve the incremental adjoint problem\n\n\n\n\n\n\n \\bA_k^T \\bhp_k = -(\\bW_{\\scriptsize\\mbox{uu}} \\bhu_k + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k),\n\n\n\n\nwhere \n\\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k\n stems for the discretization of \n(\\hat{m}_k \\exp(m_k)\\nabla p_k, \\nabla \\tilde{u})\n.\n\n\n\n\nDefine the Hessian action\n\n\n\n\n\n\n\n  \\bH_k \\, \\bhm = \\underbrace{(\\bR + \\bW_{\\scriptsize\\mbox{mm}})}_{\\text{Hessian of the regularization}} \\bhm +\n    \\underbrace{(\\bC_k^{T}\\bA_k^{-T} (\\bW_{\\scriptsize\\mbox{uu}}\n    \\bA_k^{-1} \\bC_k - \\bW_{\\scriptsize\\mbox{um}}) -\n    \\bW_{\\scriptsize\\mbox{mu}} \\bA_k^{-1}\n    \\bC_k)}_{\\text{Hessian of the data misfit}}\\;\\bhm.\n\n\n\n\n\nGoals:\n\n\nBy the end of this notebook, you should be able to:\n\n\n\n\nsolve the forward and adjoint Poisson equations\n\n\nunderstand the inverse method framework\n\n\nvisualise and understand the results\n\n\nmodify the problem and code\n\n\n\n\nMathematical tools used:\n\n\n\n\nFinite element method\n\n\nDerivation of gradiant and Hessian via the adjoint method\n\n\ninexact Newton-CG\n\n\nArmijo line search\n\n\n\n\nList of software used:\n\n\n\n\nFEniCS\n, a parallel finite element element library for the discretization of partial differential equations\n\n\nPETSc\n, for scalable and efficient linear algebra operations and solvers\n\n\nMatplotlib\n, a python package used for plotting the results\n\n\n\n\nSet up\n\n\nImport dependencies\n\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom dolfin import *\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\n\n\n\nModel set up:\n\n\nAs in the introduction, the first thing we need to do is set up the numerical model.  In this cell, we set the mesh, the finite element functions \nu, p, g\n corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization.\n\n\n# create mesh and define function spaces\nnx = 64\nny = 64\nmesh = UnitSquareMesh(nx, ny)\nVm = FunctionSpace(mesh, 'Lagrange', 1)\nVu = FunctionSpace(mesh, 'Lagrange', 2)\n\n# The true and inverted parameter\nmtrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5),Vm)\nm = interpolate(Expression(\"log(2.0)\", degree=1),Vm)\n\n# define function for state and adjoint\nu = Function(Vu)\np = Function(Vu)\n\n# define Trial and Test Functions\nu_trial, p_trial, m_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Vm)\nu_test, p_test, m_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Vm)\n\n# initialize input functions\nf = Constant(\"1.0\")\nu0 = Constant(\"0.0\")\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on')\nnb.plot(mtrue,subplot_loc=122, mytitle=\"True parameter field\")\nplt.show()\n\n\n\n\n\n\n# set up dirichlet boundary conditions\ndef boundary(x,on_boundary):\n    return on_boundary\n\nbc_state = DirichletBC(Vu, u0, boundary)\nbc_adj = DirichletBC(Vu, Constant(0.), boundary)\n\n\n\n\nSet up synthetic observations:\n\n\n\n\nPropose a coefficient field \nm_{\\rm true}\n shown above\n\n\n\n\nThe weak form of the pde: \n    Find \nu\\in H_0^1(\\Omega)\n such that \n\\underbrace{(\\exp(m_{\\rm true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega)\n.\n\n\n\n\n\n\nPerturb the solution: \nu = u + \\eta\n, where \n\\eta \\sim \\mathcal{N}(0, \\sigma)\n\n\n\n\n\n\n\n\n# noise level\nnoise_level = 0.05\n\n# weak form for setting up the synthetic observations\na_goal = inner(exp(mtrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_goal = f * u_test * dx\n\n# solve the forward/state problem to generate synthetic observations\ngoal_A, goal_b = assemble_system(a_goal, L_goal, bc_state)\n\nutrue = Function(Vu)\nsolve(goal_A, utrue.vector(), goal_b)\n\nud = Function(Vu)\nud.assign(utrue)\n\n# perturb state solution and create synthetic measurements ud\n# ud = u + ||u||/SNR * random.normal\nMAX = ud.vector().norm(\"linf\")\nnoise = Vector()\ngoal_A.init_vector(noise,1)\nparRandom.normal(noise_level * MAX, noise)\nbc_adj.apply(noise)\n\nud.vector().axpy(1., noise)\n\n# plot\nnb.multi1_plot([utrue, ud], [\"State solution with mtrue\", \"Synthetic observations\"])\nplt.show()\n\n\n\n\n\n\nThe cost function evaluation:\n\n\n\n\n\nJ(m):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text{misfit} } + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx}_{\\text{reg}}\n\n\n\n\n\nIn the code below, \n\\bW\n and \n\\bR\n are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively.\n\n\n# regularization parameter\ngamma = 1e-8\n\n# weak for for setting up the misfit and regularization compoment of the cost\nW_equ   = inner(u_trial, u_test) * dx\nR_equ   = gamma * inner(nabla_grad(m_trial), nabla_grad(m_test)) * dx\n\nW = assemble(W_equ)\nR = assemble(R_equ)\n\n# refine cost function\ndef cost(u, ud, m, W, R):\n    diff = u.vector() - ud.vector()\n    reg = 0.5 * m.vector().inner(R*m.vector() ) \n    misfit = 0.5 * diff.inner(W * diff)\n    return [reg + misfit, misfit, reg]\n\n\n\n\nSetting up the state equations, right hand side for the adjoint and the necessary matrices:\n\n\n# weak form for setting up the state equation\na_state = inner(exp(m) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_state = f * u_test * dx\n\n# weak form for setting up the adjoint equation\na_adj = inner(exp(m) * nabla_grad(p_trial), nabla_grad(p_test)) * dx\nL_adj = -inner(u - ud, p_test) * dx\n\n# weak form for setting up matrices\nWum_equ = inner(exp(m) * m_trial * nabla_grad(p_test), nabla_grad(p)) * dx\nC_equ   = inner(exp(m) * m_trial * nabla_grad(u), nabla_grad(u_test)) * dx\nWmm_equ = inner(exp(m) * m_trial * m_test *  nabla_grad(u),  nabla_grad(p)) * dx\n\nM_equ   = inner(m_trial, m_test) * dx\n\n# assemble matrix M\nM = assemble(M_equ)\n\n\n\n\nInitial guess\n\n\nWe solve the state equation and compute the cost functional for the initial guess of the parameter \na_ini\n\n\n# solve state equation\nstate_A, state_b = assemble_system (a_state, L_state, bc_state)\nsolve (state_A, u.vector(), state_b)\n\n# evaluate cost\n[cost_old, misfit_old, reg_old] = cost(u, ud, m, W, R)\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(m,subplot_loc=121, mytitle=\"m_ini\", vmin=mtrue.vector().min(), vmax=mtrue.vector().max())\nnb.plot(u,subplot_loc=122, mytitle=\"u(m_ini)\")\nplt.show()\n\n\n\n\n\n\nThe reduced Hessian apply to a vector \n\\bhm\n:\n\n\nHere we describe how to apply the reduced Hessian operator to a vector \n\\bhm\n. For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined.\n\n\nFor this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm.\n\n\nThe Hessian apply reads:\n\n\n\\begin{align}\n\\bhu &= -\\bA^{-1} \\bC \\bhm\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bA^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bhu +\n\\bW_{\\scriptsize\\mbox{um}}\\,\\bhm) & \\text{adjoint}\\\\\n\\bH \\bhm &= (\\bR + \\bW_{\\scriptsize\\mbox{mm}})\\bhm + \\bC^T \\bhp + \\bW_{\\scriptsize\\mbox{mu}} \\bhu.\n\\end{align}\n\n\n\n\n\nThe Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \n\\bW_{\\scriptsize\\mbox{um}}\\,\\bhm\n, \n\\bW_{\\scriptsize\\mbox{mm}}\\bf \\bhm\n, and \n\\bW_{\\scriptsize\\mbox{mu}} \\bhu\n:\n\n\n\\begin{align}\n\\bhu &= -\\bA^{-1} \\bC \\bf \\bhm\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bA^{-T} \\bW_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\\n\\bH_{\\rm GN} \\bhm &= \\bR \\bhm + \\bC^T \\bhp.\n\\end{align}\n\n\n\n\n\n# Class HessianOperator to perform Hessian apply to a vector\nclass HessianOperator():\n    cgiter = 0\n    def __init__(self, R, Wmm, C, A, adj_A, W, Wum, gauss_newton_approx=False):\n        self.R = R\n        self.Wmm = Wmm\n        self.C = C\n        self.A = A\n        self.adj_A = adj_A\n        self.W = W\n        self.Wum = Wum\n        self.gauss_newton_approx = gauss_newton_approx\n\n        # incremental state\n        self.du = Vector()\n        self.A.init_vector(self.du,0)\n\n        #incremental adjoint\n        self.dp = Vector()\n        self.adj_A.init_vector(self.dp,0)\n\n        # auxiliary vectors\n        self.CT_dp = Vector()\n        self.C.init_vector(self.CT_dp, 1)\n        self.Wum_du = Vector()\n        self.Wum.init_vector(self.Wum_du, 1)\n\n    def init_vector(self, v, dim):\n        self.R.init_vector(v,dim)\n\n    # Hessian performed on v, output as generic vector y\n    def mult(self, v, y):\n        self.cgiter += 1\n        y.zero()\n        if self.gauss_newton_approx:\n            self.mult_GaussNewton(v,y)\n        else:\n            self.mult_Newton(v,y)\n\n    # define (Gauss-Newton) Hessian apply H * v\n    def mult_GaussNewton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = - (self.W * self.du)\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        # Reg/Prior term\n        self.R.mult(v,y)\n\n        # Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1, self.CT_dp)\n\n    # define (Newton) Hessian apply H * v\n    def mult_Newton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = -(self.W * self.du) -  self.Wum * v\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        #Reg/Prior term\n        self.R.mult(v,y)\n        y.axpy(1., self.Wmm*v)\n\n        #Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1., self.CT_dp)\n        self.Wum.transpmult(self.du, self.Wum_du)\n        y.axpy(1., self.Wum_du)\n\n\n\n\nThe inexact Newton-CG optimization with Armijo line search:\n\n\nWe solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search.\n\n\nThe stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \n\\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau\n).\n\n\nFirst, we compute the gradient by solving the state and adjoint equation for the current parameter \nm\n, and then substituing the current state \nu\n, parameter \nm\n and adjoint \np\n variables in the weak form expression of the gradient:\n\n (g, \\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +(\\tilde{m}\\nabla u, \\nabla p).\n\n\n\n\nThen, we compute the Newton direction \n\\hat m\n by iteratively solving \n\\mathcal{H} {\\hat m} = -g\n.\nThe Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug  (to avoid negative curvature) criteria.\n\n\nFinally, the Armijo line search uses backtracking to find \n\\alpha\n such that a sufficient reduction in the cost functional is achieved.\nMore specifically, we use backtracking to find \n\\alpha\n such that:\n\nJ( m + \\alpha \\hat m ) \\leq J(m) + \\alpha c_{\\rm armijo} (\\hat m,g). \n\n\n\n\n# define parameters for the optimization\ntol = 1e-8\nc = 1e-4\nmaxiter = 12\nplot_on = False\n\n# initialize iter counters\niter = 1\ntotal_cg_iter = 0\nconverged = False\n\n# initializations\ng, m_delta = Vector(), Vector()\nR.init_vector(m_delta,0)\nR.init_vector(g,0)\n\nm_prev = Function(Vm)\n\nprint (\"Nit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\")\n\nwhile iter <  maxiter and not converged:\n\n    # assemble matrix C\n    C =  assemble(C_equ)\n\n    # solve the adoint problem\n    adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj)\n    solve(adjoint_A, p.vector(), adjoint_RHS)\n\n    # assemble W_ua and R\n    Wum = assemble (Wum_equ)\n    Wmm = assemble (Wmm_equ)\n\n    # evaluate the  gradient\n    CT_p = Vector()\n    C.init_vector(CT_p,1)\n    C.transpmult(p.vector(), CT_p)\n    MG = CT_p + R * m.vector()\n    solve(M, g, MG)\n\n    # calculate the norm of the gradient\n    grad2 = g.inner(MG)\n    gradnorm = sqrt(grad2)\n\n    # set the CG tolerance (use Eisenstat\u2013Walker termination criterion)\n    if iter == 1:\n        gradnorm_ini = gradnorm\n    tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini))\n\n    # define the Hessian apply operator (with preconditioner)\n    Hess_Apply = HessianOperator(R, Wmm, C, state_A, adjoint_A, W, Wum, gauss_newton_approx=(iter<6) )\n    P = R + gamma * M\n    Psolver = PETScKrylovSolver(\"cg\", amg_method())\n    Psolver.set_operator(P)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(Hess_Apply)\n    solver.set_preconditioner(Psolver)\n    solver.parameters[\"rel_tolerance\"] = tolcg\n    solver.parameters[\"zero_initial_guess\"] = True\n    solver.parameters[\"print_level\"] = -1\n\n    # solve the Newton system H a_delta = - MG\n    solver.solve(m_delta, -MG)\n    total_cg_iter += Hess_Apply.cgiter\n\n    # linesearch\n    alpha = 1\n    descent = 0\n    no_backtrack = 0\n    m_prev.assign(m)\n    while descent == 0 and no_backtrack < 10:\n        m.vector().axpy(alpha, m_delta )\n\n        # solve the state/forward problem\n        state_A, state_b = assemble_system(a_state, L_state, bc_state)\n        solve(state_A, u.vector(), state_b)\n\n        # evaluate cost\n        [cost_new, misfit_new, reg_new] = cost(u, ud, m, W, R)\n\n        # check if Armijo conditions are satisfied\n        if cost_new < cost_old + alpha * c * MG.inner(m_delta):\n            cost_old = cost_new\n            descent = 1\n        else:\n            no_backtrack += 1\n            alpha *= 0.5\n            m.assign(m_prev)  # reset a\n\n    # calculate sqrt(-G * D)\n    graddir = sqrt(- MG.inner(m_delta) )\n\n    sp = \"\"\n    print( \"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\\n        (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\\n         graddir, sp, gradnorm, sp, alpha, sp, tolcg) )\n\n    if plot_on:\n        nb.multi1_plot([m,u,p], [\"m\",\"u\",\"p\"], same_colorbar=False)\n        plt.show()\n\n    # check for convergence\n    if gradnorm < tol and iter > 1:\n        converged = True\n        print( \"Newton's method converged in \",iter,\"  iterations\")\n        print( \"Total number of CG iterations: \", total_cg_iter)\n\n    iter += 1\n\nif not converged:\n    print( \"Newton's method did not converge in \", maxiter, \" iterations\")\n\n\n\n\nNit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\n 1     1     1.12816e-05   1.12816e-05   1.34063e-11   1.56576e-02   3.79516e-04    1.00   5.000e-01\n 2     1     7.82345e-07   7.82308e-07   3.68084e-11   4.68471e-03   5.35131e-05    1.00   3.755e-01\n 3     1     3.12377e-07   3.12328e-07   4.91890e-11   9.72528e-04   7.14277e-06    1.00   1.372e-01\n 4     6     1.91931e-07   1.61602e-07   3.03286e-08   4.54949e-04   1.00710e-06    1.00   5.151e-02\n 5     1     1.86508e-07   1.56163e-07   3.03445e-08   1.04154e-04   6.18455e-07    1.00   4.037e-02\n 6    12     1.80489e-07   1.37364e-07   4.31244e-08   1.11510e-04   2.15007e-07    1.00   2.380e-02\n 7     5     1.80421e-07   1.38523e-07   4.18984e-08   1.15380e-05   3.88978e-08    1.00   1.012e-02\n 8    15     1.80420e-07   1.38637e-07   4.17830e-08   1.64609e-06   3.27512e-09    1.00   2.938e-03\nNewton's method converged in  8   iterations\nTotal number of CG iterations:  42\n\n\n\nnb.multi1_plot([mtrue, m], [\"mtrue\", \"m\"])\nnb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False)\nplt.show()\n\n\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\n\nAll Rights reserved.\n\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "Poisson Deterministic"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#coefficient-field-inversion-in-an-elliptic-partial-differential-equation",
            "text": "We consider the estimation of a coefficient in an elliptic partial\ndifferential equation as a model problem. Depending on the\ninterpretation of the unknowns and the type of measurements, this\nmodel problem arises, for instance, in inversion for groundwater flow\nor heat conductivity.  It can also be interpreted as finding a\nmembrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n ,  n\\in\\{1,2,3\\}  be an open, bounded\ndomain and consider the following problem:   \n\\min_{m} J(m):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx,   where  u  is the solution of   \n\\begin{split}\n\\quad -\\nabla\\cdot(\\exp(m)\\nabla u) &= f \\text{ in }\\Omega,\\\\\nu &= 0 \\text{ on }\\partial\\Omega.\n\\end{split}   Here  m\\in U_{ad}:=\\{m\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\}  the unknown coefficient field,  u_d  denotes (possibly noisy) data,  f\\in H^{-1}(\\Omega)  a given force, and  \\gamma\\ge 0  the regularization parameter.",
            "title": "Coefficient field inversion in an elliptic partial differential equation"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#the-variational-or-weak-form-of-the-state-equation",
            "text": "Find  u\\in H_0^1(\\Omega)  such that    (\\exp(m)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega),   where  H_0^1(\\Omega)  is the space of functions vanishing on  \\partial\\Omega  with square integrable derivatives. Here,  (\\cdot\\,,\\cdot)  denotes the  L^2 -inner product, i.e, for scalar functions  u,v \\in L^2(\\Omega)   we denote    (u,v) := \\int_\\Omega u(x) v(x) \\,dx.",
            "title": "The variational (or weak) form of the state equation:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#gradient-evaluation",
            "text": "The Lagrangian functional  \\mathscr{L}:H_0^1(\\Omega)\\times H^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R}  is given by   \n\\mathscr{L}(u,m,p):= \\frac{1}{2}(u-u_d,u-u_d) +\n\\frac{\\gamma}{2}(\\nabla m, \\nabla m) +  (\\exp(m)\\nabla u,\\nabla p) - (f,p).   Then the gradient of the cost functional  \\mathcal{J}(m)  with respect to the parameter  m  is   \n    \\mathcal{G}(m)(\\tilde m) := \\gamma(\\nabla m, \\nabla \\tilde{m}) +\n     (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) \\quad \\forall \\tilde{m} \\in H^1(\\Omega),   where  u \\in H_0^1(\\Omega)  is the solution of the forward problem,    \\mathscr{L}_p(u,m,p)(\\tilde{p})  := (\\exp(m)\\nabla u, \\nabla \\tilde{p}) - (f,\\tilde{p}) = 0\n\\quad \\forall \\tilde{p} \\in H_0^1(\\Omega),    and  p \\in H_0^1(\\Omega)  is the solution of the adjoint problem,    \\mathscr{L}_u(u,m,p)(\\tilde{u}) := (\\exp(m)\\nabla p, \\nabla \\tilde{u}) + (u-u_d,\\tilde{u}) = 0\n\\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).",
            "title": "Gradient evaluation:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#hessian-action",
            "text": "To evaluate the action  \\mathcal{H}(m)(\\hat{m})  of the Hessian is a given direction  \\hat{m}  , we consider variations of the meta-Lagrangian functional   \n\\begin{aligned}\n\\mathscr{L}^H(u,m,p; \\hat{u}, \\hat{m}, \\hat{p}) := & {} & {} \\\\\n{} & \\gamma(\\nabla m, \\nabla \\tilde{m}) + (\\tilde{m}\\exp(m)\\nabla u, \\nabla p) & \\text{gradient}\\\\\n{} & + (\\exp(m)\\nabla u, \\nabla \\hat{p}) - (f,\\hat{p}) & \\text{forward eq}\\\\\n{} & + (\\exp(m)\\nabla p, \\nabla \\hat{u}) + (u-u_d,\\hat{u}) & \\text{adjoint eq}.\n\\end{aligned}   Then action of the Hessian is a given direction  \\hat{m}  is   \n\\begin{aligned}\n(\\tilde{m}, \\mathcal{H}(m)(\\hat{m}) ) & := \\mathscr{L}^H_m(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{m}) \\\\\n{} & =\n(\\tilde{m} \\exp(m) \\nabla \\hat{u}, \\nabla{p}) + \\gamma (\\nabla \\hat{m}, \\nabla \\tilde{m}) + (\\tilde{m} \\hat{m} \\exp(m)\\nabla u, \\nabla p) + (\\tilde{m}\\exp(m) \\nabla u, \\nabla \\hat{p}) \\quad \\forall \\tilde{m} \\in H^1(\\Omega),\n\\end{aligned}   where      u\\in H^1_0(\\Omega)  and  p \\in H^1_0(\\Omega)  are the solution of the forward and adjoint problem, respectively;     \\hat{u} \\in H^1_0(\\Omega)  is the solution of the incremental forward problem,     \n\\mathscr{L}^H_p(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{p}) := (\\exp(m) \\nabla \\hat{u}, \\nabla \\tilde{p}) + (\\hat{m} \\exp(m) \\nabla u, \\nabla \\tilde p) = 0 \\quad \\forall \\tilde{p} \\in H_0^1(\\Omega);    and  \\hat{p} \\in H^1_0(\\Omega)  is the solution of the incremental adjoint problem, \n\\mathscr{L}^H_u(u,m,p; \\hat{u}, \\hat{m}, \\hat{p})(\\tilde{u}) := (\\hat{u}, \\tilde{u}) + (\\hat{m} \\exp(m)\\nabla p, \\nabla \\tilde{u}) + (\\exp(m) \\nabla \\tilde u, \\nabla \\hat{p}) = 0 \\quad \\forall \\tilde{u} \\in H_0^1(\\Omega).",
            "title": "Hessian action:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#inexact-newton-cg",
            "text": "Written in abstract form, the Newton Method computes an update direction  \\hat{m}_k  by solving the linear system    \n(\\tilde{m}, \\mathcal{H}(m_k)(\\hat{m}_k) ) = -\\mathcal{G}(m_k)(\\tilde m) \\quad \\forall \\tilde{m} \\in H^1(\\Omega),   where the evaluation of the gradient  \\mathcal{G}(m_k)  involve the solution  u_k  and  p_k  of the forward and adjoint problem (respectively) for  m = m_k .\nSimilarly, the Hessian action  \\mathcal{H}(m_k)(\\hat{m}_k)  requires to additional solve the incremental forward and adjoint problems.",
            "title": "Inexact Newton-CG:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#discrete-newton-system",
            "text": "\\def\\tu{\\tilde u}\n\\def\\tm{\\tilde m}\n\\def\\tp{\\tilde p}\n\\def\\hu{\\hat u}\n\\def\\hp{\\hat p}\n\\def\\hm{\\hat m}    \n\\def\\bu{{\\bf u}}\n\\def\\bm{{\\bf m}}\n\\def\\bp{{\\bf p}}\n\\def\\btu{{\\bf \\tilde u}}\n\\def\\btm{{\\bf \\tilde m}}\n\\def\\btp{{\\bf \\tilde p}}\n\\def\\bhu{{\\bf \\hat u}}\n\\def\\bhm{{\\bf \\hat m}}\n\\def\\bhp{{\\bf \\hat p}}\n\\def\\bg{{\\bf g}}    \n\\def\\bA{{\\bf A}}\n\\def\\bC{{\\bf C}}\n\\def\\bH{{\\bf H}}\n\\def\\bR{{\\bf R}}\n\\def\\bW{{\\bf W}}   Let us denote the vectors corresponding to the discretization of the functions  u_k, m_k, p_k  by  \\bu_k, \\bm_k, \\bp_k  and of the functions  \\hu_k, \\hm_k, \\hp_k  by  \\bhu_k, \\bhm_k,\\bhp_k .  Then, the discretization of the above system is given by the following symmetric linear system:   \n  \\bH_k \\, \\bhm_k = -\\bg_k.   The gradient  \\bg_k  is computed using the following three steps   Given  \\bm_k  we solve the forward problem     \\bA_k \\bu_k = {\\bf f},    where  \\bA_k \\bu_k  stems from the discretization  (\\exp(m_k)\\nabla u_k, \\nabla \\tilde{p}) , and  {\\bf f}  stands for the discretization of the right hand side  f .   Given  \\bm_k  and  \\bu_k  solve the adjoint problem     \\bA_k^T \\bp_k = - \\bW_{\\scriptsize\\mbox{uu}}\\,(\\bu_k-\\bu_d)    where  \\bA_k^T \\bp_k  stems from the discretization of  (\\exp(m_k)\\nabla \\tilde{u}, \\nabla p_k) ,  \\bW_{\\scriptsize\\mbox{uu}}  is the mass matrix corresponding to the  L^2  inner product in the state space, and  \\bu_d  stems from the data.   Define the gradient      \\bg_k = \\bR \\bm_k + \\bC_k^T \\bp_k,    where  \\bR  is the matrix stemming from discretization of the regularization operator  \\gamma ( \\nabla \\hat{m}, \\nabla \\tilde{m}) , and  \\bC_k  stems from discretization of the term  (\\tilde{m}\\exp(m_k)\\nabla u_k, \\nabla p_k) .  Similarly the action of the Hessian  \\bH_k \\, \\bhm_k  in a direction  \\bhm_k  (by using the CG algorithm we only need the action of  \\bH_k  to solve the Newton step) is given by   Solve the incremental forward problem     \\bA_k \\bhu_k = -\\bC_k \\bhm_k,    where  \\bC_k \\bm_k  stems from discretization of  (\\hat{m} \\exp(m_k) \\nabla u_k, \\nabla \\tilde p) .   Solve the incremental adjoint problem     \\bA_k^T \\bhp_k = -(\\bW_{\\scriptsize\\mbox{uu}} \\bhu_k + \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k),   where  \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm_k  stems for the discretization of  (\\hat{m}_k \\exp(m_k)\\nabla p_k, \\nabla \\tilde{u}) .   Define the Hessian action    \n  \\bH_k \\, \\bhm = \\underbrace{(\\bR + \\bW_{\\scriptsize\\mbox{mm}})}_{\\text{Hessian of the regularization}} \\bhm +\n    \\underbrace{(\\bC_k^{T}\\bA_k^{-T} (\\bW_{\\scriptsize\\mbox{uu}}\n    \\bA_k^{-1} \\bC_k - \\bW_{\\scriptsize\\mbox{um}}) -\n    \\bW_{\\scriptsize\\mbox{mu}} \\bA_k^{-1}\n    \\bC_k)}_{\\text{Hessian of the data misfit}}\\;\\bhm.",
            "title": "Discrete Newton system:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#goals",
            "text": "By the end of this notebook, you should be able to:   solve the forward and adjoint Poisson equations  understand the inverse method framework  visualise and understand the results  modify the problem and code",
            "title": "Goals:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#mathematical-tools-used",
            "text": "Finite element method  Derivation of gradiant and Hessian via the adjoint method  inexact Newton-CG  Armijo line search",
            "title": "Mathematical tools used:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#list-of-software-used",
            "text": "FEniCS , a parallel finite element element library for the discretization of partial differential equations  PETSc , for scalable and efficient linear algebra operations and solvers  Matplotlib , a python package used for plotting the results",
            "title": "List of software used:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#set-up",
            "text": "",
            "title": "Set up"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#import-dependencies",
            "text": "from __future__ import absolute_import, division, print_function\n\nfrom dolfin import *\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)",
            "title": "Import dependencies"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#model-set-up",
            "text": "As in the introduction, the first thing we need to do is set up the numerical model.  In this cell, we set the mesh, the finite element functions  u, p, g  corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization.  # create mesh and define function spaces\nnx = 64\nny = 64\nmesh = UnitSquareMesh(nx, ny)\nVm = FunctionSpace(mesh, 'Lagrange', 1)\nVu = FunctionSpace(mesh, 'Lagrange', 2)\n\n# The true and inverted parameter\nmtrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5),Vm)\nm = interpolate(Expression(\"log(2.0)\", degree=1),Vm)\n\n# define function for state and adjoint\nu = Function(Vu)\np = Function(Vu)\n\n# define Trial and Test Functions\nu_trial, p_trial, m_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Vm)\nu_test, p_test, m_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Vm)\n\n# initialize input functions\nf = Constant(\"1.0\")\nu0 = Constant(\"0.0\")\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on')\nnb.plot(mtrue,subplot_loc=122, mytitle=\"True parameter field\")\nplt.show()   # set up dirichlet boundary conditions\ndef boundary(x,on_boundary):\n    return on_boundary\n\nbc_state = DirichletBC(Vu, u0, boundary)\nbc_adj = DirichletBC(Vu, Constant(0.), boundary)",
            "title": "Model set up:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#set-up-synthetic-observations",
            "text": "Propose a coefficient field  m_{\\rm true}  shown above   The weak form of the pde: \n    Find  u\\in H_0^1(\\Omega)  such that  \\underbrace{(\\exp(m_{\\rm true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) .    Perturb the solution:  u = u + \\eta , where  \\eta \\sim \\mathcal{N}(0, \\sigma)     # noise level\nnoise_level = 0.05\n\n# weak form for setting up the synthetic observations\na_goal = inner(exp(mtrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_goal = f * u_test * dx\n\n# solve the forward/state problem to generate synthetic observations\ngoal_A, goal_b = assemble_system(a_goal, L_goal, bc_state)\n\nutrue = Function(Vu)\nsolve(goal_A, utrue.vector(), goal_b)\n\nud = Function(Vu)\nud.assign(utrue)\n\n# perturb state solution and create synthetic measurements ud\n# ud = u + ||u||/SNR * random.normal\nMAX = ud.vector().norm(\"linf\")\nnoise = Vector()\ngoal_A.init_vector(noise,1)\nparRandom.normal(noise_level * MAX, noise)\nbc_adj.apply(noise)\n\nud.vector().axpy(1., noise)\n\n# plot\nnb.multi1_plot([utrue, ud], [\"State solution with mtrue\", \"Synthetic observations\"])\nplt.show()",
            "title": "Set up synthetic observations:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#the-cost-function-evaluation",
            "text": "J(m):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text{misfit} } + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla m|^2\\,dx}_{\\text{reg}}   In the code below,  \\bW  and  \\bR  are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively.  # regularization parameter\ngamma = 1e-8\n\n# weak for for setting up the misfit and regularization compoment of the cost\nW_equ   = inner(u_trial, u_test) * dx\nR_equ   = gamma * inner(nabla_grad(m_trial), nabla_grad(m_test)) * dx\n\nW = assemble(W_equ)\nR = assemble(R_equ)\n\n# refine cost function\ndef cost(u, ud, m, W, R):\n    diff = u.vector() - ud.vector()\n    reg = 0.5 * m.vector().inner(R*m.vector() ) \n    misfit = 0.5 * diff.inner(W * diff)\n    return [reg + misfit, misfit, reg]",
            "title": "The cost function evaluation:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#setting-up-the-state-equations-right-hand-side-for-the-adjoint-and-the-necessary-matrices",
            "text": "# weak form for setting up the state equation\na_state = inner(exp(m) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_state = f * u_test * dx\n\n# weak form for setting up the adjoint equation\na_adj = inner(exp(m) * nabla_grad(p_trial), nabla_grad(p_test)) * dx\nL_adj = -inner(u - ud, p_test) * dx\n\n# weak form for setting up matrices\nWum_equ = inner(exp(m) * m_trial * nabla_grad(p_test), nabla_grad(p)) * dx\nC_equ   = inner(exp(m) * m_trial * nabla_grad(u), nabla_grad(u_test)) * dx\nWmm_equ = inner(exp(m) * m_trial * m_test *  nabla_grad(u),  nabla_grad(p)) * dx\n\nM_equ   = inner(m_trial, m_test) * dx\n\n# assemble matrix M\nM = assemble(M_equ)",
            "title": "Setting up the state equations, right hand side for the adjoint and the necessary matrices:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#initial-guess",
            "text": "We solve the state equation and compute the cost functional for the initial guess of the parameter  a_ini  # solve state equation\nstate_A, state_b = assemble_system (a_state, L_state, bc_state)\nsolve (state_A, u.vector(), state_b)\n\n# evaluate cost\n[cost_old, misfit_old, reg_old] = cost(u, ud, m, W, R)\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(m,subplot_loc=121, mytitle=\"m_ini\", vmin=mtrue.vector().min(), vmax=mtrue.vector().max())\nnb.plot(u,subplot_loc=122, mytitle=\"u(m_ini)\")\nplt.show()",
            "title": "Initial guess"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#the-reduced-hessian-apply-to-a-vector-bhm",
            "text": "Here we describe how to apply the reduced Hessian operator to a vector  \\bhm . For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined.  For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm.  The Hessian apply reads: \n\\begin{align}\n\\bhu &= -\\bA^{-1} \\bC \\bhm\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bA^{-T} (\\bW_{\\scriptsize\\mbox{uu}} \\bhu +\n\\bW_{\\scriptsize\\mbox{um}}\\,\\bhm) & \\text{adjoint}\\\\\n\\bH \\bhm &= (\\bR + \\bW_{\\scriptsize\\mbox{mm}})\\bhm + \\bC^T \\bhp + \\bW_{\\scriptsize\\mbox{mu}} \\bhu.\n\\end{align}   The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators  \\bW_{\\scriptsize\\mbox{um}}\\,\\bhm ,  \\bW_{\\scriptsize\\mbox{mm}}\\bf \\bhm , and  \\bW_{\\scriptsize\\mbox{mu}} \\bhu : \n\\begin{align}\n\\bhu &= -\\bA^{-1} \\bC \\bf \\bhm\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bA^{-T} \\bW_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\\n\\bH_{\\rm GN} \\bhm &= \\bR \\bhm + \\bC^T \\bhp.\n\\end{align}   # Class HessianOperator to perform Hessian apply to a vector\nclass HessianOperator():\n    cgiter = 0\n    def __init__(self, R, Wmm, C, A, adj_A, W, Wum, gauss_newton_approx=False):\n        self.R = R\n        self.Wmm = Wmm\n        self.C = C\n        self.A = A\n        self.adj_A = adj_A\n        self.W = W\n        self.Wum = Wum\n        self.gauss_newton_approx = gauss_newton_approx\n\n        # incremental state\n        self.du = Vector()\n        self.A.init_vector(self.du,0)\n\n        #incremental adjoint\n        self.dp = Vector()\n        self.adj_A.init_vector(self.dp,0)\n\n        # auxiliary vectors\n        self.CT_dp = Vector()\n        self.C.init_vector(self.CT_dp, 1)\n        self.Wum_du = Vector()\n        self.Wum.init_vector(self.Wum_du, 1)\n\n    def init_vector(self, v, dim):\n        self.R.init_vector(v,dim)\n\n    # Hessian performed on v, output as generic vector y\n    def mult(self, v, y):\n        self.cgiter += 1\n        y.zero()\n        if self.gauss_newton_approx:\n            self.mult_GaussNewton(v,y)\n        else:\n            self.mult_Newton(v,y)\n\n    # define (Gauss-Newton) Hessian apply H * v\n    def mult_GaussNewton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = - (self.W * self.du)\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        # Reg/Prior term\n        self.R.mult(v,y)\n\n        # Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1, self.CT_dp)\n\n    # define (Newton) Hessian apply H * v\n    def mult_Newton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = -(self.W * self.du) -  self.Wum * v\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        #Reg/Prior term\n        self.R.mult(v,y)\n        y.axpy(1., self.Wmm*v)\n\n        #Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1., self.CT_dp)\n        self.Wum.transpmult(self.du, self.Wum_du)\n        y.axpy(1., self.Wum_du)",
            "title": "The reduced Hessian apply to a vector \\bhm:"
        },
        {
            "location": "/tutorials/2_PoissonDeterministic/#the-inexact-newton-cg-optimization-with-armijo-line-search",
            "text": "We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search.  The stopping criterion is based on a relative reduction of the norm of the gradient (i.e.  \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ).  First, we compute the gradient by solving the state and adjoint equation for the current parameter  m , and then substituing the current state  u , parameter  m  and adjoint  p  variables in the weak form expression of the gradient:  (g, \\tilde{m}) = \\gamma(\\nabla m, \\nabla \\tilde{m}) +(\\tilde{m}\\nabla u, \\nabla p).   Then, we compute the Newton direction  \\hat m  by iteratively solving  \\mathcal{H} {\\hat m} = -g .\nThe Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug  (to avoid negative curvature) criteria.  Finally, the Armijo line search uses backtracking to find  \\alpha  such that a sufficient reduction in the cost functional is achieved.\nMore specifically, we use backtracking to find  \\alpha  such that: J( m + \\alpha \\hat m ) \\leq J(m) + \\alpha c_{\\rm armijo} (\\hat m,g).    # define parameters for the optimization\ntol = 1e-8\nc = 1e-4\nmaxiter = 12\nplot_on = False\n\n# initialize iter counters\niter = 1\ntotal_cg_iter = 0\nconverged = False\n\n# initializations\ng, m_delta = Vector(), Vector()\nR.init_vector(m_delta,0)\nR.init_vector(g,0)\n\nm_prev = Function(Vm)\n\nprint (\"Nit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\")\n\nwhile iter <  maxiter and not converged:\n\n    # assemble matrix C\n    C =  assemble(C_equ)\n\n    # solve the adoint problem\n    adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj)\n    solve(adjoint_A, p.vector(), adjoint_RHS)\n\n    # assemble W_ua and R\n    Wum = assemble (Wum_equ)\n    Wmm = assemble (Wmm_equ)\n\n    # evaluate the  gradient\n    CT_p = Vector()\n    C.init_vector(CT_p,1)\n    C.transpmult(p.vector(), CT_p)\n    MG = CT_p + R * m.vector()\n    solve(M, g, MG)\n\n    # calculate the norm of the gradient\n    grad2 = g.inner(MG)\n    gradnorm = sqrt(grad2)\n\n    # set the CG tolerance (use Eisenstat\u2013Walker termination criterion)\n    if iter == 1:\n        gradnorm_ini = gradnorm\n    tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini))\n\n    # define the Hessian apply operator (with preconditioner)\n    Hess_Apply = HessianOperator(R, Wmm, C, state_A, adjoint_A, W, Wum, gauss_newton_approx=(iter<6) )\n    P = R + gamma * M\n    Psolver = PETScKrylovSolver(\"cg\", amg_method())\n    Psolver.set_operator(P)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(Hess_Apply)\n    solver.set_preconditioner(Psolver)\n    solver.parameters[\"rel_tolerance\"] = tolcg\n    solver.parameters[\"zero_initial_guess\"] = True\n    solver.parameters[\"print_level\"] = -1\n\n    # solve the Newton system H a_delta = - MG\n    solver.solve(m_delta, -MG)\n    total_cg_iter += Hess_Apply.cgiter\n\n    # linesearch\n    alpha = 1\n    descent = 0\n    no_backtrack = 0\n    m_prev.assign(m)\n    while descent == 0 and no_backtrack < 10:\n        m.vector().axpy(alpha, m_delta )\n\n        # solve the state/forward problem\n        state_A, state_b = assemble_system(a_state, L_state, bc_state)\n        solve(state_A, u.vector(), state_b)\n\n        # evaluate cost\n        [cost_new, misfit_new, reg_new] = cost(u, ud, m, W, R)\n\n        # check if Armijo conditions are satisfied\n        if cost_new < cost_old + alpha * c * MG.inner(m_delta):\n            cost_old = cost_new\n            descent = 1\n        else:\n            no_backtrack += 1\n            alpha *= 0.5\n            m.assign(m_prev)  # reset a\n\n    # calculate sqrt(-G * D)\n    graddir = sqrt(- MG.inner(m_delta) )\n\n    sp = \"\"\n    print( \"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\\n        (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\\n         graddir, sp, gradnorm, sp, alpha, sp, tolcg) )\n\n    if plot_on:\n        nb.multi1_plot([m,u,p], [\"m\",\"u\",\"p\"], same_colorbar=False)\n        plt.show()\n\n    # check for convergence\n    if gradnorm < tol and iter > 1:\n        converged = True\n        print( \"Newton's method converged in \",iter,\"  iterations\")\n        print( \"Total number of CG iterations: \", total_cg_iter)\n\n    iter += 1\n\nif not converged:\n    print( \"Newton's method did not converge in \", maxiter, \" iterations\")  Nit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\n 1     1     1.12816e-05   1.12816e-05   1.34063e-11   1.56576e-02   3.79516e-04    1.00   5.000e-01\n 2     1     7.82345e-07   7.82308e-07   3.68084e-11   4.68471e-03   5.35131e-05    1.00   3.755e-01\n 3     1     3.12377e-07   3.12328e-07   4.91890e-11   9.72528e-04   7.14277e-06    1.00   1.372e-01\n 4     6     1.91931e-07   1.61602e-07   3.03286e-08   4.54949e-04   1.00710e-06    1.00   5.151e-02\n 5     1     1.86508e-07   1.56163e-07   3.03445e-08   1.04154e-04   6.18455e-07    1.00   4.037e-02\n 6    12     1.80489e-07   1.37364e-07   4.31244e-08   1.11510e-04   2.15007e-07    1.00   2.380e-02\n 7     5     1.80421e-07   1.38523e-07   4.18984e-08   1.15380e-05   3.88978e-08    1.00   1.012e-02\n 8    15     1.80420e-07   1.38637e-07   4.17830e-08   1.64609e-06   3.27512e-09    1.00   2.938e-03\nNewton's method converged in  8   iterations\nTotal number of CG iterations:  42  nb.multi1_plot([mtrue, m], [\"mtrue\", \"m\"])\nnb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False)\nplt.show()    Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. \nAll Rights reserved. \nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "The inexact Newton-CG optimization with Armijo line search:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/",
            "text": "\\def\\data{ {\\bf d}_\\rm{obs}}\n\\def\\vec{\\bf}\n\\def\\m{ {\\bf m}}\n\\def\\map{{\\bf m}_{\\text{MAP}}}\n\\def\\postcov{{\\bf \\Gamma}_{\\text{post}}}\n\\def\\prcov{{\\bf \\Gamma}_{\\text{prior}}}\n\\def\\matrix{\\bf}\n\\def\\Hmisfit{{\\bf H}_{\\text{misfit}}}\n\\def\\HT{{\\tilde{\\bf H}}_{\\text{misfit}}}\n\\def\\diag{\\operatorname{diag}}\n\\def\\Vr{{\\matrix V}_r}\n\\def\\Wr{{\\matrix W}_r}\n\\def\\Ir{{\\matrix I}_r}\n\\def\\Dr{{\\matrix D}_r}\n\\def\\H{{\\matrix H} }\n\n\n\n\n\nBayesian quantification of parameter uncertainty:\n\n\nEstimating the Gaussian approximation of posterior pdf of the coefficient parameter field in an elliptic PDE\n\n\nIn this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields our belief (expressed as a probability) that a\nmember of this candidate set is the ``true'' parameter field that\ngave rise to the observed data.\n\n\nFor simplicity, in what follows we give finite-dimensional expressions (i.e., after\ndiscretization of the parameter space) for the Bayesian\nformulation of the inverse problem.\n\n\nBayes' Theorem:\n\n\nThe posterior probability distribution combines the prior pdf\n\n\\pi_{\\text{prior}}(\\m)\n over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf\n\n\\pi_{\\text{like}}(\\data \\; | \\; \\m)\n, which explicitly\nrepresents the probability that a given set of parameters \n\\m\n\nmight give rise to the observed data \n\\data \\in\n\\mathbb{R}^m\n, namely:\n\n\n\n\n\n\\begin{align}\n\\pi_{\\text{post}}(\\m | \\data) \\propto\n\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n\\end{align}\n\n\n\n\n\nNote that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.\n\n\nGaussian prior and noise:\n\n\nThe prior:\n\n\nWe consider a Gaussian prior with mean \n{\\vec m}_{\\text prior}\n and covariance \n\\prcov\n. The covariance is given by the discretization of the inverse of differential operator \n\\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2}\n, where \n\\gamma\n, \n\\delta > 0\n control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem\n\n\nThe likelihood:\n\n\n\n\n\n\\data =  {\\bf f}(\\m) + {\\bf e }, \\;\\;\\;  {\\bf e} \\sim \\mathcal{N}({\\bf 0}, {\\bf \\Gamma}_{\\text{noise}} )\n\n\n\n\n\n\n\n\n\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} ({\\bf f}(\\m) - \\data)^T {\\bf \\Gamma}_{\\text{noise}}^{-1} ({\\bf f}(\\m) - \\data)\\right)\n\n\n\n\n\nHere \n{\\bf f}\n is the parameter-to-observable map that takes a parameter vector \n\\m\n and maps\nit to the space observation vector \n\\data\n.\n\n\nThe posterior:\n\n\n\n\n\n\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(\\m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\prcov^{-1}} \\right)\n\n\n\n\n\nThe Gaussian approximation of the posterior: \n\\mathcal{N}({\\vec \\map},\\bf \\postcov)\n\n\n\n\nThe mean of this posterior distribution, \n{\\vec \\map}\n, is the\nparameter vector maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,\n\n\n\n\n\n\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n\\Big( \n\\frac{1}{2} \\| {\\bf f}(\\m) - \\data \\|^2_{ {\\bf \\Gamma}_{\\text{noise}}^{-1}} \n+\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\prcov^{-1}} \n\\Big).\n\n\n\n\n\nThe posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of \n\\mathcal{J}\n at \n\\map\n, namely\n\n\n\n\n\n\\postcov = \\left(\\Hmisfit(\\map) + \\prcov^{-1} \\right)^{-1}\n\n\n\n\n\nThe generalized eigenvalue problem:\n\n\n\n\n\n \\Hmisfit {\\matrix V} = \\prcov^{-1} {\\matrix V} {\\matrix \\Lambda},\n\n\n\n\n\nwhere \n{\\matrix \\Lambda} = \\diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n}\n\ncontains the generalized eigenvalues and the columns of \n{\\matrix V}\\in\n\\mathbb R^{n\\times n}\n the generalized eigenvectors such that \n\n{\\matrix V}^T \\prcov^{-1} {\\matrix V} = {\\matrix I}\n.\n\n\nRandomized eigensolvers to construct the approximate spectral decomposition:\n\n\nWhen the generalized eigenvalues \n\\{\\lambda_i\\}\n decay rapidly, we can\nextract a low-rank approximation of \n\\Hmisfit\n by retaining only the \nr\n\nlargest eigenvalues and corresponding eigenvectors,\n\n\n\n\n\n \\Hmisfit = \\prcov^{-1} \\Vr {\\matrix{\\Lambda}}_r \\Vr^T \\prcov^{-1},\n\n\n\n\n\nHere, \n\\Vr \\in \\mathbb{R}^{n\\times r}\n contains only the \nr\n\ngeneralized eigenvectors of \n\\Hmisfit\n that correspond to the \nr\n largest eigenvalues,\nwhich are assembled into the diagonal matrix \n{\\matrix{\\Lambda}}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r}\n.\n\n\nThe approximate posterior covariance:\n\n\nUsing the Sherman\u2013Morrison\u2013Woodbury formula, we write\n\n\n\n\n\n\\begin{align}\n  \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1}\n  = \\prcov^{-1}-\\Vr {\\matrix{D}}_r \\Vr^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}\n\n\n\n\n\nwhere \n{\\matrix{D}}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r}\n. The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that \nr\n is\nchosen such that \n\\lambda_r\n is small relative to 1. \n\n\nTherefore we can approximate the posterior covariance as\n\n\n\n\n\n\\postcov \\approx \\prcov - \\Vr {\\matrix{D}}_r \\Vr^T\n\n\n\n\n\nDrawing samples from a Gaussian distribution with covariance \n\\H^{-1}\n\n\n\n\nLet \n{\\bf x}\n be a sample for the prior distribution, i.e. \n{\\bf x} \\sim \\mathcal{N}({\\bf 0}, \\prcov)\n, then, using the low rank approximation of the posterior covariance, we compute a sample \n{\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1})\n as\n\n\n\n\n\n  {\\bf v} = \\big\\{ \\Vr \\big[ ({\\matrix{\\Lambda}}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1}  + {\\bf I} \\big\\} {\\bf x} \n\n\n\n\n\nThis tutorial shows:\n\n\n\n\nDescription of the inverse problem (the forward problem, the prior, and the misfit functional)\n\n\nConvergence of the inexact Newton-CG algorithm\n\n\nLow-rank-based approximation of the posterior covariance (built on a low-rank\napproximation of the Hessian of the data misfit) \n\n\nHow to construct the low-rank approximation of the Hessian of the data misfit\n\n\nHow to apply the inverse and square-root inverse Hessian to a vector efficiently\n\n\nSamples from the Gaussian approximation of the posterior\n\n\n\n\nGoals:\n\n\nBy the end of this notebook, you should be able to:\n\n\n\n\nUnderstand the Bayesian inverse framework\n\n\nVisualise and understand the results\n\n\nModify the problem and code\n\n\n\n\nMathematical tools used:\n\n\n\n\nFinite element method\n\n\nDerivation of gradiant and Hessian via the adjoint method\n\n\ninexact Newton-CG\n\n\nArmijo line search\n\n\nBayes' formula\n\n\nrandomized eigensolvers\n\n\n\n\nList of software used:\n\n\n\n\nFEniCS\n, a parallel finite element element library for the discretization of partial differential equations\n\n\nPETSc\n, for scalable and efficient linear algebra operations and solvers\n\n\nMatplotlib\n, A great python package that I used for plotting many of the results\n\n\nNumpy\n, A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook.\n\n\n\n\n1. Load modules\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)\n\n\n\n\n2. Generate the true parameter\n\n\nThis function generates a random field with a prescribed anysotropic covariance function.\n\n\ndef true_model(Vh, gamma, delta, anis_diff):\n    prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff )\n    noise = dl.Vector()\n    prior.init_vector(noise,\"noise\")\n    parRandom.normal(1., noise)\n    mtrue = dl.Vector()\n    prior.init_vector(mtrue, 0)\n    prior.sample(noise,mtrue)\n    return mtrue\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nWe compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the \nstate\n and \nadjoint\n variable and P1 for the \nparameter\n.\n\n\nndim = 2\nnx = 64\nny = 64\nmesh = dl.UnitSquareMesh(nx, ny)\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh = [Vh2, Vh1, Vh2]\nprint( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n    Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )\n\n\n\n\nNumber of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641\n\n\n\n4. Set up the forward problem\n\n\nTo set up the forward problem we use the \nPDEVariationalProblem\n class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables \nVh\n\n- the pde in weak form \npde_varf\n\n- the boundary conditions \nbc\n for the forward problem and \nbc0\n for the adjoint and incremental problems.\n\n\nThe \nPDEVariationalProblem\n class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.\n\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS)\n\nu_bdr = dl.Expression(\"x[1]\", degree=1)\nu_bdr0 = dl.Constant(0.0)\nbc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Constant(0.0)\n\ndef pde_varf(u,m,p):\n    return dl.exp(m)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n\n\n\n4. Set up the prior\n\n\nTo obtain the synthetic true paramter \nm_{\\rm true}\n we generate a realization from the prior distribution. Here we assume a Gaussian prior with zero average and covariance matrix \n\\mathcal{C} = \\mathcal{A}^{-2}\n, where \n\\mathcal{A}\n is a differential operator of the form\n\n\n\n\n \\mathcal{A} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I. \n\n\n\n\nHere \n\\Theta\n is an s.p.d. anisotropic tensor of the form\n\n\n\n\n \\Theta =\n\\begin{bmatrix}\n\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n\\end{bmatrix} \n\n\n\n\nFor the prior model, we assume that we can measure the log-permeability coefficient at \nN\n locations, and we denote with \nm^1_{\\rm true}\n, \n\\ldots\n, \nm^N_{\\rm true}\n such measures.\nWe also introduce the mollifier functions\n\n \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N,\n\nand we let\n\n \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M},\n\nwhere \np\n is a penalization constant (10 for this problem) and \n \\mathcal{M} = \\sum_{i=1}^N \\delta_i I\n.\n\n\nWe then compute \nm_{\\rm pr}\n, the  mean  of  the  prior  measure,  as  a  regularized\nleast-squares fit of these point observations by solving\n\n\nm_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle m, \\widetilde{\\mathcal{A}} m\\rangle + \\frac{p}{2}\\langle m_{\\rm true} - m, \\mathcal{M}(m_{\\rm true}- m) \\rangle.\n\n\n\n\n\nFinally the prior distribution is \n\\mathcal{N}(m_{\\rm pr}, \\mathcal{C}_{\\rm prior})\n, with \n\\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2}\n.\n\n\ngamma = .1\ndelta = .5\n\nanis_diff = dl.Expression(code_AnisTensor2D, degree=1)\nanis_diff.theta0 = 2.\nanis_diff.theta1 = .5\nanis_diff.alpha = math.pi/4\nmtrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff)\n\nlocations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]])\npen = 1e1\nprior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, mtrue, anis_diff, pen)\n\nprint(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2))    \n\nobjs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [\"True Parameter\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\nmodel = Model(pde,prior, misfit)\n\n\n\n\nPrior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2\n\n\n\n\n\n5. Set up the misfit functional and generate synthetic observations\n\n\nTo setup the observation operator, we generate \nntargets\n random locations where to evaluate the value of the state.\n\n\nTo generate the synthetic observation, we first solve the forward problem using the true parameter \nm_{\\rm true}\n. Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise.\n\nrel_noise\n is the signal to noise ratio.\n\n\nntargets = 300\nrel_noise = 0.01\n\n\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nprint( \"Number of observation points: {0}\".format(ntargets) )\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, mtrue, None]\npde.solveFwd(x[STATE], x, 1e-9)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm(\"linf\")\nnoise_std_dev = rel_noise * MAX\nparRandom.normal_perturb(noise_std_dev, misfit.d)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax)\nnb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax)\nplt.show()\n\n\n\n\nNumber of observation points: 300\n\n\n\n\n\n6. Set up the model and test gradient and Hessian\n\n\nThe model is defined by three component:\n- the \nPDEVariationalProblem\n \npde\n which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n- the \nPrior\n \nprior\n which provides methods to apply the regularization (\nprecision\n) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n- the \nMisfit\n \nmisfit\n which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.\n\n\nTo test gradient and the Hessian of the model we use forward finite differences.\n\n\nmodel = Model(pde, prior, misfit)\n\nm0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER])\n_ = modelVerify(model, m0.vector(), 1e-12)\n\n\n\n\n(yy, H xx) - (xx, H yy) =  -3.58137818086553e-13\n\n\n\n\n\n7. Compute the MAP point\n\n\nWe used the globalized Newtown-CG method to compute the MAP point.\n\n\nm = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[\"rel_tolerance\"] = 1e-6\nsolver.parameters[\"abs_tolerance\"] = 1e-12\nsolver.parameters[\"max_iter\"]      = 25\nsolver.parameters[\"inner_rel_tolerance\"] = 1e-15\nsolver.parameters[\"GN_iter\"] = 5\nsolver.parameters[\"globalization\"] = \"LS\"\nsolver.parameters[\"LS\"][\"c_armijo\"] = 1e-4\n\n\nx = solver.solve([None, m, None])\n\nif solver.converged:\n    print( \"\\nConverged in \", solver.it, \" iterations.\")\nelse:\n    print( \"\\nNot Converged\")\n\nprint( \"Termination reason: \", solver.termination_reasons[solver.reason] )\nprint( \"Final gradient norm: \", solver.final_grad_norm )\nprint( \"Final cost: \", solver.final_cost )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\")\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\")\nplt.show()\n\n\n\n\nIt  cg_it cost            misfit          reg             (g,dm)          ||g||L2        alpha          tolcg         \n  1   1    2.296924e+03    2.296798e+03    1.268357e-01   -2.687891e+03   3.512398e+04   1.000000e+00   5.000000e-01\n  2   2    7.562150e+02    7.533038e+02    2.911243e+00   -3.123660e+03   1.815603e+04   1.000000e+00   5.000000e-01\n  3   3    2.724829e+02    2.658439e+02    6.639027e+00   -9.803242e+02   6.759090e+03   1.000000e+00   4.386744e-01\n  4   2    2.310692e+02    2.237770e+02    7.292206e+00   -8.391690e+01   3.449868e+03   1.000000e+00   3.134003e-01\n  5   8    1.753607e+02    1.636311e+02    1.172959e+01   -1.160805e+02   1.959111e+03   1.000000e+00   2.361716e-01\n  6   2    1.735413e+02    1.617893e+02    1.175202e+01   -3.649454e+00   1.252445e+03   1.000000e+00   1.888328e-01\n  7  14    1.612523e+02    1.416618e+02    1.959055e+01   -2.462072e+01   8.785564e+02   1.000000e+00   1.581550e-01\n  8  11    1.607960e+02    1.409823e+02    1.981368e+01   -9.165649e-01   2.482466e+02   1.000000e+00   8.406976e-02\n  9  17    1.607155e+02    1.400093e+02    2.070616e+01   -1.612168e-01   1.142870e+02   1.000000e+00   5.704224e-02\n 10  21    1.607148e+02    1.400375e+02    2.067724e+01   -1.453391e-03   1.004748e+01   1.000000e+00   1.691323e-02\n 11  31    1.607148e+02    1.400344e+02    2.068038e+01   -2.926008e-06   4.809418e-01   1.000000e+00   3.700364e-03\n\nConverged in  11  iterations.\nTermination reason:  Norm of the gradient less than tolerance\nFinal gradient norm:  0.0009161960275146831\nFinal cost:  160.714767866629\n\n\n\n\n\n8. Compute the low rank Gaussian approximation of the posterior\n\n\nWe used the \ndouble pass\n algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve\n\n\n\n\n \\Hmisfit {\\bf v}_i = \\lambda_i \\prcov^{-1} {\\bf v}_i. \n\n\n\n\nThe Figure shows the largest \nk\n generalized eigenvectors of the Hessian misfit.\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line (\ny=1\n).\nThe effective rank is independent of the mesh size.\n\n\nmodel.setPointForHessianEvaluations(x, gauss_newton_approx=False)\nHmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], misfit_only=True)\nk = 50\np = 20\nprint( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) )\n\nOmega = MultiVector(x[PARAMETER], k+p)\nparRandom.normal(1., Omega)\nlmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nposterior = GaussianLRPosterior(prior, lmbda, V)\nposterior.mean = x[PARAMETER]\n\nplt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15])\n\n\n\n\nSingle/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.\n\n\n\n\n\n\n\n9. Prior and posterior pointwise variance fields\n\n\ncompute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200)\n    print( \"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr) )\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200)\n\nobjs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n        dl.Function(Vh[PARAMETER], post_pw_variance)]\nmytitles = [\"Prior variance\", \"Posterior variance\"]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()\n\n\n\n\nPosterior trace 1.260892e-01; Prior trace 3.949821e-01; Correction trace 2.688929e-01\n\n\n\n\n\n10. Generate samples from Prior and Posterior\n\n\nnsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\"noise\")\ns_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\")\ns_post = dl.Function(Vh[PARAMETER], name=\"sample_post\")\n\npr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\nps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    parRandom.normal(1., noise)\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\n\nAll Rights reserved.\n\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "Subsurface Bayesian"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#bayesian-quantification-of-parameter-uncertainty",
            "text": "",
            "title": "Bayesian quantification of parameter uncertainty:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#estimating-the-gaussian-approximation-of-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde",
            "text": "In this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields our belief (expressed as a probability) that a\nmember of this candidate set is the ``true'' parameter field that\ngave rise to the observed data.  For simplicity, in what follows we give finite-dimensional expressions (i.e., after\ndiscretization of the parameter space) for the Bayesian\nformulation of the inverse problem.",
            "title": "Estimating the Gaussian approximation of posterior pdf of the coefficient parameter field in an elliptic PDE"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#bayes-theorem",
            "text": "The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m)  over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\data \\; | \\; \\m) , which explicitly\nrepresents the probability that a given set of parameters  \\m \nmight give rise to the observed data  \\data \\in\n\\mathbb{R}^m , namely:   \n\\begin{align}\n\\pi_{\\text{post}}(\\m | \\data) \\propto\n\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n\\end{align}   Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.",
            "title": "Bayes' Theorem:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#gaussian-prior-and-noise",
            "text": "",
            "title": "Gaussian prior and noise:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#the-prior",
            "text": "We consider a Gaussian prior with mean  {\\vec m}_{\\text prior}  and covariance  \\prcov . The covariance is given by the discretization of the inverse of differential operator  \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where  \\gamma ,  \\delta > 0  control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem",
            "title": "The prior:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#the-likelihood",
            "text": "\\data =  {\\bf f}(\\m) + {\\bf e }, \\;\\;\\;  {\\bf e} \\sim \\mathcal{N}({\\bf 0}, {\\bf \\Gamma}_{\\text{noise}} )    \n\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} ({\\bf f}(\\m) - \\data)^T {\\bf \\Gamma}_{\\text{noise}}^{-1} ({\\bf f}(\\m) - \\data)\\right)   Here  {\\bf f}  is the parameter-to-observable map that takes a parameter vector  \\m  and maps\nit to the space observation vector  \\data .",
            "title": "The likelihood:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#the-posterior",
            "text": "\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel {\\bf f}(\\m) - \\data \\parallel^{2}_{{\\bf \\Gamma}_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\prcov^{-1}} \\right)",
            "title": "The posterior:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#the-gaussian-approximation-of-the-posterior-mathcalnvec-mapbf-postcov",
            "text": "The mean of this posterior distribution,  {\\vec \\map} , is the\nparameter vector maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,   \n\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n\\Big( \n\\frac{1}{2} \\| {\\bf f}(\\m) - \\data \\|^2_{ {\\bf \\Gamma}_{\\text{noise}}^{-1}} \n+\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\prcov^{-1}} \n\\Big).   The posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of  \\mathcal{J}  at  \\map , namely   \n\\postcov = \\left(\\Hmisfit(\\map) + \\prcov^{-1} \\right)^{-1}",
            "title": "The Gaussian approximation of the posterior: \\mathcal{N}({\\vec \\map},\\bf \\postcov)"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#the-generalized-eigenvalue-problem",
            "text": "\\Hmisfit {\\matrix V} = \\prcov^{-1} {\\matrix V} {\\matrix \\Lambda},   where  {\\matrix \\Lambda} = \\diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} \ncontains the generalized eigenvalues and the columns of  {\\matrix V}\\in\n\\mathbb R^{n\\times n}  the generalized eigenvectors such that  {\\matrix V}^T \\prcov^{-1} {\\matrix V} = {\\matrix I} .",
            "title": "The generalized eigenvalue problem:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#randomized-eigensolvers-to-construct-the-approximate-spectral-decomposition",
            "text": "When the generalized eigenvalues  \\{\\lambda_i\\}  decay rapidly, we can\nextract a low-rank approximation of  \\Hmisfit  by retaining only the  r \nlargest eigenvalues and corresponding eigenvectors,   \n \\Hmisfit = \\prcov^{-1} \\Vr {\\matrix{\\Lambda}}_r \\Vr^T \\prcov^{-1},   Here,  \\Vr \\in \\mathbb{R}^{n\\times r}  contains only the  r \ngeneralized eigenvectors of  \\Hmisfit  that correspond to the  r  largest eigenvalues,\nwhich are assembled into the diagonal matrix  {\\matrix{\\Lambda}}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r} .",
            "title": "Randomized eigensolvers to construct the approximate spectral decomposition:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#the-approximate-posterior-covariance",
            "text": "Using the Sherman\u2013Morrison\u2013Woodbury formula, we write   \n\\begin{align}\n  \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1}\n  = \\prcov^{-1}-\\Vr {\\matrix{D}}_r \\Vr^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}   where  {\\matrix{D}}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r} . The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that  r  is\nchosen such that  \\lambda_r  is small relative to 1.   Therefore we can approximate the posterior covariance as   \n\\postcov \\approx \\prcov - \\Vr {\\matrix{D}}_r \\Vr^T",
            "title": "The approximate posterior covariance:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#drawing-samples-from-a-gaussian-distribution-with-covariance-h-1",
            "text": "Let  {\\bf x}  be a sample for the prior distribution, i.e.  {\\bf x} \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample  {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1})  as   \n  {\\bf v} = \\big\\{ \\Vr \\big[ ({\\matrix{\\Lambda}}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1}  + {\\bf I} \\big\\} {\\bf x}",
            "title": "Drawing samples from a Gaussian distribution with covariance \\H^{-1}"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#this-tutorial-shows",
            "text": "Description of the inverse problem (the forward problem, the prior, and the misfit functional)  Convergence of the inexact Newton-CG algorithm  Low-rank-based approximation of the posterior covariance (built on a low-rank\napproximation of the Hessian of the data misfit)   How to construct the low-rank approximation of the Hessian of the data misfit  How to apply the inverse and square-root inverse Hessian to a vector efficiently  Samples from the Gaussian approximation of the posterior",
            "title": "This tutorial shows:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#goals",
            "text": "By the end of this notebook, you should be able to:   Understand the Bayesian inverse framework  Visualise and understand the results  Modify the problem and code",
            "title": "Goals:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#mathematical-tools-used",
            "text": "Finite element method  Derivation of gradiant and Hessian via the adjoint method  inexact Newton-CG  Armijo line search  Bayes' formula  randomized eigensolvers",
            "title": "Mathematical tools used:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#list-of-software-used",
            "text": "FEniCS , a parallel finite element element library for the discretization of partial differential equations  PETSc , for scalable and efficient linear algebra operations and solvers  Matplotlib , A great python package that I used for plotting many of the results  Numpy , A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook.",
            "title": "List of software used:"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#1-load-modules",
            "text": "from __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#2-generate-the-true-parameter",
            "text": "This function generates a random field with a prescribed anysotropic covariance function.  def true_model(Vh, gamma, delta, anis_diff):\n    prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff )\n    noise = dl.Vector()\n    prior.init_vector(noise,\"noise\")\n    parRandom.normal(1., noise)\n    mtrue = dl.Vector()\n    prior.init_vector(mtrue, 0)\n    prior.sample(noise,mtrue)\n    return mtrue",
            "title": "2. Generate the true parameter"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#3-set-up-the-mesh-and-finite-element-spaces",
            "text": "We compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the  state  and  adjoint  variable and P1 for the  parameter .  ndim = 2\nnx = 64\nny = 64\nmesh = dl.UnitSquareMesh(nx, ny)\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh = [Vh2, Vh1, Vh2]\nprint( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n    Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )  Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641",
            "title": "3. Set up the mesh and finite element spaces"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#4-set-up-the-forward-problem",
            "text": "To set up the forward problem we use the  PDEVariationalProblem  class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables  Vh \n- the pde in weak form  pde_varf \n- the boundary conditions  bc  for the forward problem and  bc0  for the adjoint and incremental problems.  The  PDEVariationalProblem  class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.  def u_boundary(x, on_boundary):\n    return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS)\n\nu_bdr = dl.Expression(\"x[1]\", degree=1)\nu_bdr0 = dl.Constant(0.0)\nbc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Constant(0.0)\n\ndef pde_varf(u,m,p):\n    return dl.exp(m)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)",
            "title": "4. Set up the forward problem"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#4-set-up-the-prior",
            "text": "To obtain the synthetic true paramter  m_{\\rm true}  we generate a realization from the prior distribution. Here we assume a Gaussian prior with zero average and covariance matrix  \\mathcal{C} = \\mathcal{A}^{-2} , where  \\mathcal{A}  is a differential operator of the form    \\mathcal{A} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I.    Here  \\Theta  is an s.p.d. anisotropic tensor of the form    \\Theta =\n\\begin{bmatrix}\n\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n\\end{bmatrix}    For the prior model, we assume that we can measure the log-permeability coefficient at  N  locations, and we denote with  m^1_{\\rm true} ,  \\ldots ,  m^N_{\\rm true}  such measures.\nWe also introduce the mollifier functions  \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N, \nand we let  \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M}, \nwhere  p  is a penalization constant (10 for this problem) and   \\mathcal{M} = \\sum_{i=1}^N \\delta_i I .  We then compute  m_{\\rm pr} , the  mean  of  the  prior  measure,  as  a  regularized\nleast-squares fit of these point observations by solving \nm_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle m, \\widetilde{\\mathcal{A}} m\\rangle + \\frac{p}{2}\\langle m_{\\rm true} - m, \\mathcal{M}(m_{\\rm true}- m) \\rangle.   Finally the prior distribution is  \\mathcal{N}(m_{\\rm pr}, \\mathcal{C}_{\\rm prior}) , with  \\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2} .  gamma = .1\ndelta = .5\n\nanis_diff = dl.Expression(code_AnisTensor2D, degree=1)\nanis_diff.theta0 = 2.\nanis_diff.theta1 = .5\nanis_diff.alpha = math.pi/4\nmtrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff)\n\nlocations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]])\npen = 1e1\nprior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, mtrue, anis_diff, pen)\n\nprint(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(delta, gamma,2))    \n\nobjs = [dl.Function(Vh[PARAMETER],mtrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [\"True Parameter\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\nmodel = Model(pde,prior, misfit)  Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2",
            "title": "4. Set up the prior"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#5-set-up-the-misfit-functional-and-generate-synthetic-observations",
            "text": "To setup the observation operator, we generate  ntargets  random locations where to evaluate the value of the state.  To generate the synthetic observation, we first solve the forward problem using the true parameter  m_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random Gaussian noise. rel_noise  is the signal to noise ratio.  ntargets = 300\nrel_noise = 0.01\n\n\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nprint( \"Number of observation points: {0}\".format(ntargets) )\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, mtrue, None]\npde.solveFwd(x[STATE], x, 1e-9)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm(\"linf\")\nnoise_std_dev = rel_noise * MAX\nparRandom.normal_perturb(noise_std_dev, misfit.d)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax)\nnb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax)\nplt.show()  Number of observation points: 300",
            "title": "5. Set up the misfit functional and generate synthetic observations"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#6-set-up-the-model-and-test-gradient-and-hessian",
            "text": "The model is defined by three component:\n- the  PDEVariationalProblem   pde  which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n- the  Prior   prior  which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n- the  Misfit   misfit  which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.  To test gradient and the Hessian of the model we use forward finite differences.  model = Model(pde, prior, misfit)\n\nm0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER])\n_ = modelVerify(model, m0.vector(), 1e-12)  (yy, H xx) - (xx, H yy) =  -3.58137818086553e-13",
            "title": "6. Set up the model and test gradient and Hessian"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#7-compute-the-map-point",
            "text": "We used the globalized Newtown-CG method to compute the MAP point.  m = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[\"rel_tolerance\"] = 1e-6\nsolver.parameters[\"abs_tolerance\"] = 1e-12\nsolver.parameters[\"max_iter\"]      = 25\nsolver.parameters[\"inner_rel_tolerance\"] = 1e-15\nsolver.parameters[\"GN_iter\"] = 5\nsolver.parameters[\"globalization\"] = \"LS\"\nsolver.parameters[\"LS\"][\"c_armijo\"] = 1e-4\n\n\nx = solver.solve([None, m, None])\n\nif solver.converged:\n    print( \"\\nConverged in \", solver.it, \" iterations.\")\nelse:\n    print( \"\\nNot Converged\")\n\nprint( \"Termination reason: \", solver.termination_reasons[solver.reason] )\nprint( \"Final gradient norm: \", solver.final_grad_norm )\nprint( \"Final cost: \", solver.final_cost )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\")\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\")\nplt.show()  It  cg_it cost            misfit          reg             (g,dm)          ||g||L2        alpha          tolcg         \n  1   1    2.296924e+03    2.296798e+03    1.268357e-01   -2.687891e+03   3.512398e+04   1.000000e+00   5.000000e-01\n  2   2    7.562150e+02    7.533038e+02    2.911243e+00   -3.123660e+03   1.815603e+04   1.000000e+00   5.000000e-01\n  3   3    2.724829e+02    2.658439e+02    6.639027e+00   -9.803242e+02   6.759090e+03   1.000000e+00   4.386744e-01\n  4   2    2.310692e+02    2.237770e+02    7.292206e+00   -8.391690e+01   3.449868e+03   1.000000e+00   3.134003e-01\n  5   8    1.753607e+02    1.636311e+02    1.172959e+01   -1.160805e+02   1.959111e+03   1.000000e+00   2.361716e-01\n  6   2    1.735413e+02    1.617893e+02    1.175202e+01   -3.649454e+00   1.252445e+03   1.000000e+00   1.888328e-01\n  7  14    1.612523e+02    1.416618e+02    1.959055e+01   -2.462072e+01   8.785564e+02   1.000000e+00   1.581550e-01\n  8  11    1.607960e+02    1.409823e+02    1.981368e+01   -9.165649e-01   2.482466e+02   1.000000e+00   8.406976e-02\n  9  17    1.607155e+02    1.400093e+02    2.070616e+01   -1.612168e-01   1.142870e+02   1.000000e+00   5.704224e-02\n 10  21    1.607148e+02    1.400375e+02    2.067724e+01   -1.453391e-03   1.004748e+01   1.000000e+00   1.691323e-02\n 11  31    1.607148e+02    1.400344e+02    2.068038e+01   -2.926008e-06   4.809418e-01   1.000000e+00   3.700364e-03\n\nConverged in  11  iterations.\nTermination reason:  Norm of the gradient less than tolerance\nFinal gradient norm:  0.0009161960275146831\nFinal cost:  160.714767866629",
            "title": "7. Compute the MAP point"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#8-compute-the-low-rank-gaussian-approximation-of-the-posterior",
            "text": "We used the  double pass  algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve    \\Hmisfit {\\bf v}_i = \\lambda_i \\prcov^{-1} {\\bf v}_i.    The Figure shows the largest  k  generalized eigenvectors of the Hessian misfit.\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line ( y=1 ).\nThe effective rank is independent of the mesh size.  model.setPointForHessianEvaluations(x, gauss_newton_approx=False)\nHmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], misfit_only=True)\nk = 50\np = 20\nprint( \"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) )\n\nOmega = MultiVector(x[PARAMETER], k+p)\nparRandom.normal(1., Omega)\nlmbda, V = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nposterior = GaussianLRPosterior(prior, lmbda, V)\nposterior.mean = x[PARAMETER]\n\nplt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15])  Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.",
            "title": "8. Compute the low rank Gaussian approximation of the posterior"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#9-prior-and-posterior-pointwise-variance-fields",
            "text": "compute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200)\n    print( \"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr) )\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200)\n\nobjs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n        dl.Function(Vh[PARAMETER], post_pw_variance)]\nmytitles = [\"Prior variance\", \"Posterior variance\"]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()  Posterior trace 1.260892e-01; Prior trace 3.949821e-01; Correction trace 2.688929e-01",
            "title": "9. Prior and posterior pointwise variance fields"
        },
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#10-generate-samples-from-prior-and-posterior",
            "text": "nsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\"noise\")\ns_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\")\ns_post = dl.Function(Vh[PARAMETER], name=\"sample_post\")\n\npr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\nps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    parRandom.normal(1., noise)\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n    plt.show()       Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. \nAll Rights reserved. \nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "10. Generate samples from Prior and Posterior"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/",
            "text": "\\def\\D{\\mathcal{D}}\n\\def\\ipar{m}\n\\def\\R{\\mathbb{R}}\n\\def\\del{\\partial}\n\\def\\vec{\\bf}\n\\def\\priorm{\\mu_0}\n\\def\\C{\\mathcal{C}}\n\\def\\Acal{\\mathcal{A}}\n\\def\\postm{\\mu_{\\rm{post}}}\n\\def\\iparpost{\\ipar_\\text{post}}\n\\def\\obs{\\vec{d}} \n\\def\\yobs{\\obs^{\\text{obs}}}\n\\def\\obsop{\\mathcal{B}}\n\\def\\dd{\\vec{\\bar{d}}}\n\\def\\iFF{\\mathcal{F}}\n\\def\\iFFadj{\\mathcal{F}^*}\n\\def\\ncov{\\Gamma_{\\mathrm{noise}}}\n\n\n\n\n\nBayesian initial condition inversion in an advection-diffusion problem\n\n\nIn this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.\n\n\nThe Bayesian inverse problem:\n\n\nFollowing the Bayesian framework, we utilize \na Gaussian prior measure \n\\priorm = \\mathcal{N}(\\ipar_0,\\C_0)\n,\nwith \n\\C_0=\\Acal^{-2}\n where \n\\Acal\n is an elliptic differential operator as \ndescribed in the PoissonBayesian example, and use an additive\nGaussian noise model. Therefore, the solution of the Bayesian inverse\nproblem is the posterior measure, \n\\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post})\n with\n\n\\iparpost\n and \n\\C_\\text{post}\n.\n\n\n\n\nThe posterior mean \n\\iparpost\n is characterized as the minimizer of\n\n\n\n\n\n\n\n\\begin{aligned}\n& \\mathcal{J}(\\ipar) :=\n  \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs  \\right\\|^2_{\\ncov^{-1}}\n  + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)},\n\\end{aligned}\n\n\n\n\n\nwhich can also be interpreted as the regularized functional to be\nminimized in deterministic inversion. The observation operator \n\\mathcal{B}\n extracts the values of the forward solution \nu\n on a set of\nlocations \n\\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D\n at\ntimes \n\\{t_1, \\ldots, t_N\\} \\subset [0, T]\n.\n\n\n\n\nThe posterior covariance \n\\C_{\\text{post}}\n is the inverse of the Hessian of \n\\mathcal{J}(\\ipar)\n, i.e.,\n\n\n\n\n\n\n\n\\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.\n\n\n\n\n\nThe forward problem:\n\n\nThe PDE in the parameter-to-observable map \n\\iFF\n models diffusive transport\nin a domain \n\\D \\subset \\R^d\n (\nd \\in \\{2, 3\\}\n):\n\n\n\n\n\n\\begin{split}\nu_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0     & \\quad \\text{in } \\D\\times(0,T),\\\\\n                                 u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\\n                \\kappa \\nabla u\\cdot \\vec{n} &= 0     & \\quad \\text{on } \\partial\\D \\times (0,T).\n\\end{split}\n\n\n\n\n\nHere, \n\\kappa > 0\n is the diffusion coefficient and \nT > 0\n is the final\ntime. The velocity field\n\n\\vec{v}\n is computed by solving the following steady-state\nNavier-Stokes equation with the side walls driving the flow:\n\n\n\n\n\n\\begin{aligned}\n- \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\\n\\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\\n\\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D.\n\\end{aligned}\n\n\n\n\n\nHere, \nq\n is pressure, \n\\text{Re}\n is the Reynolds number. The Dirichlet boundary data\n\n\\vec{g} \\in \\R^d\n is given by \n\n\\vec{g} = \\vec{e}_2\n on the left wall of the domain, \n\n\\vec{g}=-\\vec{e}_2\n on the right wall,  and \n\\vec{g} = \\vec{0}\n everywhere else.\n\n\nThe adjoint problem:\n\n\n\n\n\n\\begin{aligned}\n-p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p  &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\\n                                      p(\\cdot, T) &= 0             &       &\\text{ in } \\D,\\\\ \n(\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n}            &= 0             &       &\\text{ on } \\partial\\D\\times (0,T).\n\\end{aligned}\n\n\n\n\n\n1. Load modules\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" )\nfrom model_ad_diff import TimeDependentAD\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\n\n\n\n2. Construct the velocity field\n\n\ndef v_boundary(x,on_boundary):\n    return on_boundary\n\ndef q_boundary(x,on_boundary):\n    return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS\n\ndef computeVelocityField(mesh):\n    Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2)\n    Wh = dl.FunctionSpace(mesh, 'Lagrange', 1)\n    if dlversion() <= (1,6,0):\n        XW = dl.MixedFunctionSpace([Xh, Wh])\n    else:\n        mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()])\n        XW = dl.FunctionSpace(mesh, mixed_element)\n\n    Re = 1e2\n\n    g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1)\n    bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary)\n    bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise')\n    bcs = [bc1, bc2]\n\n    vq = dl.Function(XW)\n    (v,q) = dl.split(vq)\n    (v_test, q_test) = dl.TestFunctions (XW)\n\n    def strain(v):\n        return dl.sym(dl.nabla_grad(v))\n\n    F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test)\n           - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx\n\n    dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\":\n                                         {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}})\n\n    plt.figure(figsize=(15,5))\n    vh = dl.project(v,Xh)\n    qh = dl.project(q,Wh)\n    nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\")\n    nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\")\n    plt.show()\n\n    return v\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nmesh = dl.refine( dl.Mesh(\"ad_20.xml\") )\nwind_velocity = computeVelocityField(mesh)\nVh = dl.FunctionSpace(mesh, \"Lagrange\", 1)\nprint( \"Number of dofs: {0}\".format( Vh.dim() ) )\n\n\n\n\n\n\nNumber of dofs: 2023\n\n\n\n4. Set up model (prior, true/proposed initial condition)\n\n\ngamma = 1\ndelta = 8\nprior = BiLaplacianPrior(Vh, gamma, delta)\n\nprior.mean = dl.interpolate(dl.Constant(0.5), Vh).vector()\ntrue_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))', degree=5), Vh).vector()\nproblem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior)\n\nobjs = [dl.Function(Vh,true_initial_condition),\n        dl.Function(Vh,prior.mean)]\nmytitles = [\"True Initial Condition\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\n\n\n\n\n\n5. Generate the synthetic observations\n\n\nrel_noise = 0.001\nutrue = problem.generate_vector(STATE)\nx = [utrue, true_initial_condition, None]\nproblem.solveFwd(x[STATE], x, 1e-9)\nMAX = utrue.norm(\"linf\", \"linf\")\nnoise_std_dev = rel_noise * MAX\nproblem.ud.copy(utrue)\nparRandom.normal_perturb(noise_std_dev, problem.ud)\nproblem.noise_variance = noise_std_dev*noise_std_dev\n\nnb.show_solution(Vh, true_initial_condition, utrue, \"Solution\")\n\n\n\n\n\n\n6. Test the gradient and the Hessian of the cost (negative log posterior)\n\n\nm0 = true_initial_condition.copy()\n_ = modelVerify(problem, m0, 1e-12, is_quadratic=True)\n\n\n\n\n(yy, H xx) - (xx, H yy) =  7.230028659845437e-14\n\n\n\n\n\n7. Evaluate the gradient\n\n\n[u,m,p] = problem.generate_vector()\nproblem.solveFwd(u, [u,m,p], 1e-12)\nproblem.solveAdj(p, [u,m,p], 1e-12)\nmg = problem.generate_vector(PARAMETER)\ngrad_norm = problem.evalGradientParameter([u,m,p], mg)\n\nprint( \"(g,g) = \", grad_norm)\n\n\n\n\n(g,g) =  1662959298505.7039\n\n\n\n8. The Gaussian posterior\n\n\nH = ReducedHessian(problem, 1e-12, misfit_only=True) \n\nk = 80\np = 20\nprint( \"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) )\nOmega = MultiVector(x[PARAMETER], k+p)\nparRandom.normal(1., Omega)\nlmbda, V = singlePassG(H, prior.R, prior.Rsolver, Omega, k)\n\n\nposterior = GaussianLRPosterior( prior, lmbda, V )\n\nplt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh, V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60])\n\n\n\n\nSingle Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.\n\n\n\n\n\n\n\n9. Compute the MAP point\n\n\nH.misfit_only = False\n\nsolver = CGSolverSteihaug()\nsolver.set_operator(H)\nsolver.set_preconditioner( posterior.Hlr )\nsolver.parameters[\"print_level\"] = 1\nsolver.parameters[\"rel_tolerance\"] = 1e-6\nsolver.solve(m, -mg)\nproblem.solveFwd(u, [u,m,p], 1e-12)\n\ntotal_cost, reg_cost, misfit_cost = problem.cost([u,m,p])\nprint( \"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost) )\n\nposterior.mean = m\n\nplt.figure(figsize=(7.5,5))\nnb.plot(dl.Function(Vh, m), mytitle=\"Initial Condition\")\nplt.show()\n\nnb.show_solution(Vh, m, u, \"Solution\")\n\n\n\n\n Iterartion :  0  (B r, r) =  30084.235543720704\n Iteration :  1  (B r, r) =  0.1401740221905336\n Iteration :  2  (B r, r) =  3.467479314573311e-05\n Iteration :  3  (B r, r) =  3.909772756568604e-10\nRelative/Absolute residual less than tol\nConverged in  3  iterations with final norm  1.9773145315221358e-05\nTotal cost 84.2423; Reg Cost 68.9031; Misfit 15.3392\n\n\n\n\n\n\n\n10. Prior and posterior pointwise variance fields\n\n\ncompute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200)\n    print( \"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr) )\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300)\n\nobjs = [dl.Function(Vh, pr_pw_variance),\n        dl.Function(Vh, post_pw_variance)]\nmytitles = [\"Prior Variance\", \"Posterior Variance\"]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()\n\n\n\n\nPosterior trace 0.000603033; Prior trace 0.0285673; Correction trace 0.0279643\n\n\n\n\n\n11. Draw samples from the prior and posterior distributions\n\n\nnsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\"noise\")\ns_prior = dl.Function(Vh, name=\"sample_prior\")\ns_post = dl.Function(Vh, name=\"sample_post\")\n\npr_max =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min()\nps_max =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    parRandom.normal(1., noise)\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\n\nAll Rights reserved.\n\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "Advection-Diffusion Bayesian"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#bayesian-initial-condition-inversion-in-an-advection-diffusion-problem",
            "text": "In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.",
            "title": "Bayesian initial condition inversion in an advection-diffusion problem"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#the-bayesian-inverse-problem",
            "text": "Following the Bayesian framework, we utilize \na Gaussian prior measure  \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) ,\nwith  \\C_0=\\Acal^{-2}  where  \\Acal  is an elliptic differential operator as \ndescribed in the PoissonBayesian example, and use an additive\nGaussian noise model. Therefore, the solution of the Bayesian inverse\nproblem is the posterior measure,  \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post})  with \\iparpost  and  \\C_\\text{post} .   The posterior mean  \\iparpost  is characterized as the minimizer of    \n\\begin{aligned}\n& \\mathcal{J}(\\ipar) :=\n  \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs  \\right\\|^2_{\\ncov^{-1}}\n  + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)},\n\\end{aligned}   which can also be interpreted as the regularized functional to be\nminimized in deterministic inversion. The observation operator  \\mathcal{B}  extracts the values of the forward solution  u  on a set of\nlocations  \\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D  at\ntimes  \\{t_1, \\ldots, t_N\\} \\subset [0, T] .   The posterior covariance  \\C_{\\text{post}}  is the inverse of the Hessian of  \\mathcal{J}(\\ipar) , i.e.,    \n\\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.",
            "title": "The Bayesian inverse problem:"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#the-forward-problem",
            "text": "The PDE in the parameter-to-observable map  \\iFF  models diffusive transport\nin a domain  \\D \\subset \\R^d  ( d \\in \\{2, 3\\} ):   \n\\begin{split}\nu_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0     & \\quad \\text{in } \\D\\times(0,T),\\\\\n                                 u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\\n                \\kappa \\nabla u\\cdot \\vec{n} &= 0     & \\quad \\text{on } \\partial\\D \\times (0,T).\n\\end{split}   Here,  \\kappa > 0  is the diffusion coefficient and  T > 0  is the final\ntime. The velocity field \\vec{v}  is computed by solving the following steady-state\nNavier-Stokes equation with the side walls driving the flow:   \n\\begin{aligned}\n- \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\\n\\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\\n\\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D.\n\\end{aligned}   Here,  q  is pressure,  \\text{Re}  is the Reynolds number. The Dirichlet boundary data \\vec{g} \\in \\R^d  is given by  \\vec{g} = \\vec{e}_2  on the left wall of the domain,  \\vec{g}=-\\vec{e}_2  on the right wall,  and  \\vec{g} = \\vec{0}  everywhere else.",
            "title": "The forward problem:"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#the-adjoint-problem",
            "text": "\\begin{aligned}\n-p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p  &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\\n                                      p(\\cdot, T) &= 0             &       &\\text{ in } \\D,\\\\ \n(\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n}            &= 0             &       &\\text{ on } \\partial\\D\\times (0,T).\n\\end{aligned}",
            "title": "The adjoint problem:"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#1-load-modules",
            "text": "from __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" )\nfrom model_ad_diff import TimeDependentAD\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#2-construct-the-velocity-field",
            "text": "def v_boundary(x,on_boundary):\n    return on_boundary\n\ndef q_boundary(x,on_boundary):\n    return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS\n\ndef computeVelocityField(mesh):\n    Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2)\n    Wh = dl.FunctionSpace(mesh, 'Lagrange', 1)\n    if dlversion() <= (1,6,0):\n        XW = dl.MixedFunctionSpace([Xh, Wh])\n    else:\n        mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()])\n        XW = dl.FunctionSpace(mesh, mixed_element)\n\n    Re = 1e2\n\n    g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1)\n    bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary)\n    bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise')\n    bcs = [bc1, bc2]\n\n    vq = dl.Function(XW)\n    (v,q) = dl.split(vq)\n    (v_test, q_test) = dl.TestFunctions (XW)\n\n    def strain(v):\n        return dl.sym(dl.nabla_grad(v))\n\n    F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test)\n           - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx\n\n    dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\":\n                                         {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}})\n\n    plt.figure(figsize=(15,5))\n    vh = dl.project(v,Xh)\n    qh = dl.project(q,Wh)\n    nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\")\n    nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\")\n    plt.show()\n\n    return v",
            "title": "2. Construct the velocity field"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#3-set-up-the-mesh-and-finite-element-spaces",
            "text": "mesh = dl.refine( dl.Mesh(\"ad_20.xml\") )\nwind_velocity = computeVelocityField(mesh)\nVh = dl.FunctionSpace(mesh, \"Lagrange\", 1)\nprint( \"Number of dofs: {0}\".format( Vh.dim() ) )   Number of dofs: 2023",
            "title": "3. Set up the mesh and finite element spaces"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#4-set-up-model-prior-trueproposed-initial-condition",
            "text": "gamma = 1\ndelta = 8\nprior = BiLaplacianPrior(Vh, gamma, delta)\n\nprior.mean = dl.interpolate(dl.Constant(0.5), Vh).vector()\ntrue_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))', degree=5), Vh).vector()\nproblem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior)\n\nobjs = [dl.Function(Vh,true_initial_condition),\n        dl.Function(Vh,prior.mean)]\nmytitles = [\"True Initial Condition\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()",
            "title": "4. Set up model (prior, true/proposed initial condition)"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#5-generate-the-synthetic-observations",
            "text": "rel_noise = 0.001\nutrue = problem.generate_vector(STATE)\nx = [utrue, true_initial_condition, None]\nproblem.solveFwd(x[STATE], x, 1e-9)\nMAX = utrue.norm(\"linf\", \"linf\")\nnoise_std_dev = rel_noise * MAX\nproblem.ud.copy(utrue)\nparRandom.normal_perturb(noise_std_dev, problem.ud)\nproblem.noise_variance = noise_std_dev*noise_std_dev\n\nnb.show_solution(Vh, true_initial_condition, utrue, \"Solution\")",
            "title": "5. Generate the synthetic observations"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#6-test-the-gradient-and-the-hessian-of-the-cost-negative-log-posterior",
            "text": "m0 = true_initial_condition.copy()\n_ = modelVerify(problem, m0, 1e-12, is_quadratic=True)  (yy, H xx) - (xx, H yy) =  7.230028659845437e-14",
            "title": "6. Test the gradient and the Hessian of the cost (negative log posterior)"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#7-evaluate-the-gradient",
            "text": "[u,m,p] = problem.generate_vector()\nproblem.solveFwd(u, [u,m,p], 1e-12)\nproblem.solveAdj(p, [u,m,p], 1e-12)\nmg = problem.generate_vector(PARAMETER)\ngrad_norm = problem.evalGradientParameter([u,m,p], mg)\n\nprint( \"(g,g) = \", grad_norm)  (g,g) =  1662959298505.7039",
            "title": "7. Evaluate the gradient"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#8-the-gaussian-posterior",
            "text": "H = ReducedHessian(problem, 1e-12, misfit_only=True) \n\nk = 80\np = 20\nprint( \"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p) )\nOmega = MultiVector(x[PARAMETER], k+p)\nparRandom.normal(1., Omega)\nlmbda, V = singlePassG(H, prior.R, prior.Rsolver, Omega, k)\n\n\nposterior = GaussianLRPosterior( prior, lmbda, V )\n\nplt.plot(range(0,k), lmbda, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh, V, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60])  Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.",
            "title": "8. The Gaussian posterior"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#9-compute-the-map-point",
            "text": "H.misfit_only = False\n\nsolver = CGSolverSteihaug()\nsolver.set_operator(H)\nsolver.set_preconditioner( posterior.Hlr )\nsolver.parameters[\"print_level\"] = 1\nsolver.parameters[\"rel_tolerance\"] = 1e-6\nsolver.solve(m, -mg)\nproblem.solveFwd(u, [u,m,p], 1e-12)\n\ntotal_cost, reg_cost, misfit_cost = problem.cost([u,m,p])\nprint( \"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost) )\n\nposterior.mean = m\n\nplt.figure(figsize=(7.5,5))\nnb.plot(dl.Function(Vh, m), mytitle=\"Initial Condition\")\nplt.show()\n\nnb.show_solution(Vh, m, u, \"Solution\")   Iterartion :  0  (B r, r) =  30084.235543720704\n Iteration :  1  (B r, r) =  0.1401740221905336\n Iteration :  2  (B r, r) =  3.467479314573311e-05\n Iteration :  3  (B r, r) =  3.909772756568604e-10\nRelative/Absolute residual less than tol\nConverged in  3  iterations with final norm  1.9773145315221358e-05\nTotal cost 84.2423; Reg Cost 68.9031; Misfit 15.3392",
            "title": "9. Compute the MAP point"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#10-prior-and-posterior-pointwise-variance-fields",
            "text": "compute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200)\n    print( \"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr) )\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300)\n\nobjs = [dl.Function(Vh, pr_pw_variance),\n        dl.Function(Vh, post_pw_variance)]\nmytitles = [\"Prior Variance\", \"Posterior Variance\"]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()  Posterior trace 0.000603033; Prior trace 0.0285673; Correction trace 0.0279643",
            "title": "10. Prior and posterior pointwise variance fields"
        },
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#11-draw-samples-from-the-prior-and-posterior-distributions",
            "text": "nsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\"noise\")\ns_prior = dl.Function(Vh, name=\"sample_prior\")\ns_post = dl.Function(Vh, name=\"sample_post\")\n\npr_max =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min()\nps_max =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    parRandom.normal(1., noise)\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n    plt.show()       Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. \nAll Rights reserved. \nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "11. Draw samples from the prior and posterior distributions"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/",
            "text": "Spectrum of the preconditioned Hessian misfit operator\n\n\nThe linear source inversion problem\n\n\nWe consider the following linear source inversion problem.\nFind the state \nu \\in H^1_{\\Gamma_D}(\\Omega)\n and the source (\nparameter\n) \nm \\in H^1(\\Omega)\n that solves\n\n\n\n\n\n\\begin{aligned}\n{} & \\min_m \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|m-m_0|^2 + \\gamma|\\nabla (m - m_0)|^2 \\right] dx & {}\\\\\n{\\rm s.t.} & {} &{} \\\\\n{} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = m & {\\rm in} \\; \\Omega\\\\\n{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n\\end{aligned}\n\n\n\n\n\nHere:\n\n\n\n\n\n\n\n\nu_d\n is a \nn_{\\rm obs}\n finite dimensional vector that denotes noisy observations of the state \nu\n in \nn_{\\rm obs}\n locations \n\\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}}\n. Specifically, \nu_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i\n, where \n\\eta_i\n are i.i.d. \n\\mathcal{N}(0, \\sigma^2)\n.\n\n\n\n\n\n\n\n\nB: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}}\n is the linear operator that evaluates the state \nu\n at the observation locations \n\\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}}\n.\n\n\n\n\n\n\n\n\n\\delta\n and \n\\gamma\n are the parameters of the regularization penalizing the \nL^2(\\Omega)\n and \nH^1(\\Omega)\n norm of \nm-m_0\n, respectively.\n\n\n\n\n\n\n\n\nk\n, \n{\\bf v}\n, \nc\n are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively.\n\n\n\n\n\n\n\n\n\\Gamma_D \\subset \\partial \\Omega\n, \n\\Gamma_N \\subset \\partial \\Omega\n represents the subdomain of \n\\partial\\Omega\n where we impose Dirichlet or Neumann boundary conditions, respectively.\n\n\n\n\n\n\n1. Load modules\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\n\n\n\n2. The linear source inversion problem\n\n\ndef pde_varf(u,m,p):\n    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n           + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\\n           + c*u*p*dl.dx \\\n           - m*p*dl.dx\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and x[1] < dl.DOLFIN_EPS\n\ndef solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True):\n    myRandom = Random()\n    mesh = dl.UnitSquareMesh(nx, ny)\n    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n\n    Vh = [Vh1, Vh1, Vh1]\n    if verbose:\n        print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )\n\n\n    u_bdr = dl.Constant(0.0)\n    u_bdr0 = dl.Constant(0.0)\n    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n    bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\n    mtrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector()\n    m0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector()\n\n    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n    if verbose:\n        print( \"Number of observation points: {0}\".format(targets.shape[0]) )\n\n    misfit = PointwiseStateObservation(Vh[STATE], targets)\n\n    reg = LaplacianPrior(Vh[PARAMETER], gamma, delta)\n\n    #Generate synthetic observations\n    utrue = pde.generate_state()\n    x = [utrue, mtrue, None]\n    pde.solveFwd(x[STATE], x, 1e-9)\n    misfit.B.mult(x[STATE], misfit.d)\n    MAX = misfit.d.norm(\"linf\")\n    noise_std_dev = rel_noise * MAX\n    myRandom.normal_perturb(noise_std_dev, misfit.d)\n    misfit.noise_variance = noise_std_dev*noise_std_dev\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], mtrue), mytitle = \"True source\", subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132)\n        nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133)\n        plt.show()\n\n    model = Model(pde, reg, misfit)\n    u = model.generate_vector(STATE)\n    m = m0.copy()\n    p = model.generate_vector(ADJOINT)\n    x = [u,m,p]\n    mg = model.generate_vector(PARAMETER)\n    model.solveFwd(u, x)\n    model.solveAdj(p, x)\n    model.evalGradientParameter(x, mg)\n    model.setPointForHessianEvaluations(x, gauss_newton_approx=False)\n\n    H = ReducedHessian(model, 1e-12)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(H)\n    solver.set_preconditioner( reg.Rsolver )\n    solver.parameters[\"print_level\"] = -1\n    solver.parameters[\"rel_tolerance\"] = 1e-9\n    solver.solve(m, -mg)\n\n    if solver.converged:\n        if verbose:\n            print( \"CG converged in \", solver.iter, \" iterations.\" )\n    else:\n        print( \"CG did not converged.\" )\n        raise\n\n    model.solveFwd(u, x, 1e-12)\n\n    total_cost, reg_cost, misfit_cost = model.cost(x)\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], m), mytitle = \"Reconstructed source\", subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132)\n        nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133)\n        plt.show()\n\n    H.misfit_only = True\n    k_evec = 80\n    p_evec = 5\n    if verbose:\n        print( \"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec) )\n    Omega = MultiVector(x[PARAMETER], k_evec+p_evec)\n    myRandom.normal(1., Omega)\n    lmbda, V = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec)\n\n    if verbose:\n        plt.figure()\n        nb.plot_eigenvalues(lmbda, mytitle=\"Generalized Eigenvalues\")\n        nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15])\n        plt.show()\n\n    return lmbda, V, Vh[PARAMETER], solver.iter\n\n\n\n\n\n3. Solution of the source inversion problem\n\n\nndim = 2\nnx = 32\nny = 32\n\nntargets = 300\nnp.random.seed(seed=1)\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nrel_noise = 0.01\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nlmbda, V, Vm, nit = solve(nx,ny, targets, rel_noise, gamma, delta)\n\n\n\n\nNumber of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089\nNumber of observation points: 300\n\n\n\n\n\nCG converged in  67  iterations.\n\n\n\n\n\nDouble Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.\n\n\n\n\n\n\n\n4. Mesh independence of the spectrum of the preconditioned Hessian misfit\n\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nn = [16,32,64]\nlmbda1, V1, Vm1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False)\nlmbda2, V2, Vm2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False)\nlmbda3, V3, Vm3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False)\n\nprint( \"Number of Iterations: \", niter1, niter2, niter3 )\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131)\nnb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132)\nnb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Vm1, V1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5])\nnb.plot_eigenvectors(Vm2, V2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5])\nnb.plot_eigenvectors(Vm3, V3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  61 67 72\n\n\n\n\n\n\n\n\n\n\n\n5. Dependence on the noise level\n\n\nWe solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization.\n\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nrel_noise = [1e-3,1e-2,1e-1]\nlmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False)\nlmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False)\nlmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False)\n\nprint( \"Number of Iterations: \", niter1, niter2, niter3 )\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131)\nnb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132)\nnb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Vm1, V1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5])\nnb.plot_eigenvectors(Vm2, V2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5])\nnb.plot_eigenvectors(Vm3, V3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  164 67 23\n\n\n\n\n\n\n\n\n\n\n\n6. Dependence on the PDE coefficients\n\n\nAssume a constant reaction term \nc = 1\n, and we consider different values for the diffusivity coefficient \nk\n.\n\n\nThe smaller the value of \nk\n the slower the decay in the spectrum.\n\n\nrel_noise = 0.01\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(1.0)\n\nlmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.1)\nlmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.01)\nlmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\n\nprint( \"Number of Iterations: \", niter1, niter2, niter3 )\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131)\nnb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132)\nnb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133)\n\nnb.plot_eigenvectors(Vm1, V1, mytitle=\"k=1. Eigen\", which=[0,1,5])\nnb.plot_eigenvectors(Vm2, V2, mytitle=\"k=0.1 Eigen\", which=[0,1,5])\nnb.plot_eigenvectors(Vm3, V3, mytitle=\"k=0.01 Eigen\", which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  79 143 244\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\n\nAll Rights reserved.\n\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "Hessian Spectrum"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/#spectrum-of-the-preconditioned-hessian-misfit-operator",
            "text": "",
            "title": "Spectrum of the preconditioned Hessian misfit operator"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/#the-linear-source-inversion-problem",
            "text": "We consider the following linear source inversion problem.\nFind the state  u \\in H^1_{\\Gamma_D}(\\Omega)  and the source ( parameter )  m \\in H^1(\\Omega)  that solves   \n\\begin{aligned}\n{} & \\min_m \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|m-m_0|^2 + \\gamma|\\nabla (m - m_0)|^2 \\right] dx & {}\\\\\n{\\rm s.t.} & {} &{} \\\\\n{} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = m & {\\rm in} \\; \\Omega\\\\\n{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n\\end{aligned}   Here:     u_d  is a  n_{\\rm obs}  finite dimensional vector that denotes noisy observations of the state  u  in  n_{\\rm obs}  locations  \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . Specifically,  u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where  \\eta_i  are i.i.d.  \\mathcal{N}(0, \\sigma^2) .     B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}}  is the linear operator that evaluates the state  u  at the observation locations  \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} .     \\delta  and  \\gamma  are the parameters of the regularization penalizing the  L^2(\\Omega)  and  H^1(\\Omega)  norm of  m-m_0 , respectively.     k ,  {\\bf v} ,  c  are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively.     \\Gamma_D \\subset \\partial \\Omega ,  \\Gamma_N \\subset \\partial \\Omega  represents the subdomain of  \\partial\\Omega  where we impose Dirichlet or Neumann boundary conditions, respectively.",
            "title": "The linear source inversion problem"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/#1-load-modules",
            "text": "from __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/#2-the-linear-source-inversion-problem",
            "text": "def pde_varf(u,m,p):\n    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n           + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\\n           + c*u*p*dl.dx \\\n           - m*p*dl.dx\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and x[1] < dl.DOLFIN_EPS\n\ndef solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True):\n    myRandom = Random()\n    mesh = dl.UnitSquareMesh(nx, ny)\n    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n\n    Vh = [Vh1, Vh1, Vh1]\n    if verbose:\n        print( \"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )\n\n\n    u_bdr = dl.Constant(0.0)\n    u_bdr0 = dl.Constant(0.0)\n    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n    bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\n    mtrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector()\n    m0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector()\n\n    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n    if verbose:\n        print( \"Number of observation points: {0}\".format(targets.shape[0]) )\n\n    misfit = PointwiseStateObservation(Vh[STATE], targets)\n\n    reg = LaplacianPrior(Vh[PARAMETER], gamma, delta)\n\n    #Generate synthetic observations\n    utrue = pde.generate_state()\n    x = [utrue, mtrue, None]\n    pde.solveFwd(x[STATE], x, 1e-9)\n    misfit.B.mult(x[STATE], misfit.d)\n    MAX = misfit.d.norm(\"linf\")\n    noise_std_dev = rel_noise * MAX\n    myRandom.normal_perturb(noise_std_dev, misfit.d)\n    misfit.noise_variance = noise_std_dev*noise_std_dev\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], mtrue), mytitle = \"True source\", subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132)\n        nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133)\n        plt.show()\n\n    model = Model(pde, reg, misfit)\n    u = model.generate_vector(STATE)\n    m = m0.copy()\n    p = model.generate_vector(ADJOINT)\n    x = [u,m,p]\n    mg = model.generate_vector(PARAMETER)\n    model.solveFwd(u, x)\n    model.solveAdj(p, x)\n    model.evalGradientParameter(x, mg)\n    model.setPointForHessianEvaluations(x, gauss_newton_approx=False)\n\n    H = ReducedHessian(model, 1e-12)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(H)\n    solver.set_preconditioner( reg.Rsolver )\n    solver.parameters[\"print_level\"] = -1\n    solver.parameters[\"rel_tolerance\"] = 1e-9\n    solver.solve(m, -mg)\n\n    if solver.converged:\n        if verbose:\n            print( \"CG converged in \", solver.iter, \" iterations.\" )\n    else:\n        print( \"CG did not converged.\" )\n        raise\n\n    model.solveFwd(u, x, 1e-12)\n\n    total_cost, reg_cost, misfit_cost = model.cost(x)\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], m), mytitle = \"Reconstructed source\", subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132)\n        nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133)\n        plt.show()\n\n    H.misfit_only = True\n    k_evec = 80\n    p_evec = 5\n    if verbose:\n        print( \"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec) )\n    Omega = MultiVector(x[PARAMETER], k_evec+p_evec)\n    myRandom.normal(1., Omega)\n    lmbda, V = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec)\n\n    if verbose:\n        plt.figure()\n        nb.plot_eigenvalues(lmbda, mytitle=\"Generalized Eigenvalues\")\n        nb.plot_eigenvectors(Vh[PARAMETER], V, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15])\n        plt.show()\n\n    return lmbda, V, Vh[PARAMETER], solver.iter",
            "title": "2. The linear source inversion problem"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/#3-solution-of-the-source-inversion-problem",
            "text": "ndim = 2\nnx = 32\nny = 32\n\nntargets = 300\nnp.random.seed(seed=1)\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nrel_noise = 0.01\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nlmbda, V, Vm, nit = solve(nx,ny, targets, rel_noise, gamma, delta)  Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089\nNumber of observation points: 300   CG converged in  67  iterations.   Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.",
            "title": "3. Solution of the source inversion problem"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/#4-mesh-independence-of-the-spectrum-of-the-preconditioned-hessian-misfit",
            "text": "gamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nn = [16,32,64]\nlmbda1, V1, Vm1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False)\nlmbda2, V2, Vm2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False)\nlmbda3, V3, Vm3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False)\n\nprint( \"Number of Iterations: \", niter1, niter2, niter3 )\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131)\nnb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132)\nnb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Vm1, V1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5])\nnb.plot_eigenvectors(Vm2, V2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5])\nnb.plot_eigenvectors(Vm3, V3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5])\n\nplt.show()  Number of Iterations:  61 67 72",
            "title": "4. Mesh independence of the spectrum of the preconditioned Hessian misfit"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/#5-dependence-on-the-noise-level",
            "text": "We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization.  gamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nrel_noise = [1e-3,1e-2,1e-1]\nlmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False)\nlmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False)\nlmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False)\n\nprint( \"Number of Iterations: \", niter1, niter2, niter3 )\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131)\nnb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132)\nnb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Vm1, V1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5])\nnb.plot_eigenvectors(Vm2, V2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5])\nnb.plot_eigenvectors(Vm3, V3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5])\n\nplt.show()  Number of Iterations:  164 67 23",
            "title": "5. Dependence on the noise level"
        },
        {
            "location": "/tutorials/5_HessianSpectrum/#6-dependence-on-the-pde-coefficients",
            "text": "Assume a constant reaction term  c = 1 , and we consider different values for the diffusivity coefficient  k .  The smaller the value of  k  the slower the decay in the spectrum.  rel_noise = 0.01\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(1.0)\n\nlmbda1, V1, Vm1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.1)\nlmbda2, V2, Vm2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.01)\nlmbda3, V3, Vm3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\n\nprint( \"Number of Iterations: \", niter1, niter2, niter3 )\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(lmbda1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131)\nnb.plot_eigenvalues(lmbda2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132)\nnb.plot_eigenvalues(lmbda3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133)\n\nnb.plot_eigenvectors(Vm1, V1, mytitle=\"k=1. Eigen\", which=[0,1,5])\nnb.plot_eigenvectors(Vm2, V2, mytitle=\"k=0.1 Eigen\", which=[0,1,5])\nnb.plot_eigenvectors(Vm3, V3, mytitle=\"k=0.01 Eigen\", which=[0,1,5])\n\nplt.show()  Number of Iterations:  79 143 244      Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced. \nAll Rights reserved. \nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "6. Dependence on the PDE coefficients"
        },
        {
            "location": "/tutorial_v1.6.0/",
            "text": "Tutorial\n\n\nThese tutorials are the best place to learn about the basic features and the algorithms in \nhIPPYlib\n.\n\n\n\n\nFEniCS101\n notebook illustrates the use of FEniCS for the solution of a linear boundary value problem.\n\n\nPoisson Deterministic\n notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting.\n\n\nSubsurface Bayesian\n notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting.\n\n\nAdvection-Diffusion Bayesian\n notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting.\n\n\nHessian Spectrum\n notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.\n\n\n\n\nThe interactive ipython notebooks are located in the \ntutorial\n folder of the \nhIPPYlib\n release.\n\n\nTo run the notebooks follow these instructions.\n\n\n\n\nOpen a FEniCS terminal and type\n\n\n\n\n$ cd tutorial\n$ jupyter notebook\n\n\n\n\n\n\nA new tab will open in your web-brower showing the notebooks.\n\n\nClick on the notebook you would like to use.\n\n\nTo run all the code in the notebook simply click on Cell --> Run All.\n\n\n\n\nFor more information on installing ipython and using notebooks see \nhere\n.",
            "title": "README"
        },
        {
            "location": "/tutorial_v1.6.0/#tutorial",
            "text": "These tutorials are the best place to learn about the basic features and the algorithms in  hIPPYlib .   FEniCS101  notebook illustrates the use of FEniCS for the solution of a linear boundary value problem.  Poisson Deterministic  notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting.  Subsurface Bayesian  notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting.  Advection-Diffusion Bayesian  notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting.  Hessian Spectrum  notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.   The interactive ipython notebooks are located in the  tutorial  folder of the  hIPPYlib  release.  To run the notebooks follow these instructions.   Open a FEniCS terminal and type   $ cd tutorial\n$ jupyter notebook   A new tab will open in your web-brower showing the notebooks.  Click on the notebook you would like to use.  To run all the code in the notebook simply click on Cell --> Run All.   For more information on installing ipython and using notebooks see  here .",
            "title": "Tutorial"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/",
            "text": "FEniCS101 Tutorial\n\n\nIn this tutorial we consider the boundary value problem (BVP)\n\n\n\n\n\\begin{eqnarray*}\n- \\nabla \\cdot (k \\nabla u) = f &      \\text{ in } \\Omega,\\\\\nu = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\\nk \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom},\n\\end{eqnarray*}\n\n\n\n\nwhere \n\\Omega = (0,1) \\times (0,1)\n, \n\\Gamma_D\n and and \n\\Gamma_N\n are the union of\nthe left and right, and top and bottom boundaries of \n\\Omega\n,\nrespectively.\n\n\nHere\n\n\\begin{eqnarray*}\nk(x,y) = 1  & \\text{ on } \\Omega\\\\\nf(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\\nu_0(x,y)      = 0 & \\text{ on } \\Gamma_D, \\\\\n\\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right.\n& \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array}\n\\end{eqnarray*}\n\n\n\n\nThe exact solution is\n\n u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right). \n\n\n\n\nWeak formulation\n\n\nLet us define the Hilbert spaces \nV_{u_0}, V_0 \\in \\Omega\n as\n\n V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\},\n\n\n V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}.\n\n\n\n\nTo obtain the weak formulation, we multiply the PDE by an arbitrary function \nv \\in V_0\n and integrate over the domain \n\\Omega\n leading to\n\n\n\n\n -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. \n\n\n\n\nThen, integration by parts the non-conforming term gives\n\n\n\n\n \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. \n\n\n\n\nFinally by recalling that \n v = 0 \n on \n\\Gamma_D\n and that \nk \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma \n on \n\\Gamma_N\n, we find the weak formulation:\n\n\nFind * \nu \\in V_{u_0}\n \nsuch that*\n\n \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0. \n\n\n\n\n1. Load modules\n\n\nTo start we load the following modules:\n\n\n\n\n\n\ndolfin: the python/C++ interface to FEniCS\n\n\n\n\n\n\nmath\n: the python module for mathematical functions\n\n\n\n\n\n\nnumpy\n: a python package for linear algebra\n\n\n\n\n\n\nmatplotlib\n: a python package used for plotting the results\n\n\n\n\n\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom dolfin import *\n\nimport math\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\n\n\n\n2. Define the mesh and the finite element space\n\n\nWe construct a triangulation (mesh) \n\\mathcal{T}_h\n of the computational domain \n\\Omega := [0, 1]^2\n with \nn\n elements in each direction.\n\n\nOn the mesh \n\\mathcal{T}_h\n, we then define the finite element space \nV_h \\subset H^1(\\Omega)\n consisting of globally continuous piecewise polinomials functions. The \ndegree\n variable defines the polinomial degree.\n\n\nn = 16\ndegree = 1\nmesh = UnitSquareMesh(n, n)\nnb.plot(mesh)\n\nVh  = FunctionSpace(mesh, 'Lagrange', degree)\nprint(\"dim(Vh) = \", Vh.dim())\n\n\n\n\ndim(Vh) =  289\n\n\n\n\n\n3. Define boundary labels\n\n\nTo partition the boundary of \n\\Omega\n in the subdomains \n\\Gamma_{\\rm top}\n, \n\\Gamma_{\\rm bottom}\n, \n\\Gamma_{\\rm left}\n, \n\\Gamma_{\\rm right}\n we assign a unique label \nboundary_parts\n to each of part of \n\\partial \\Omega\n.\n\n\nclass TopBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1] - 1) < DOLFIN_EPS\n\nclass BottomBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1]) < DOLFIN_EPS\n\nclass LeftBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0]) < DOLFIN_EPS\n\nclass RightBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0] - 1) < DOLFIN_EPS\n\nboundary_parts = FacetFunction(\"size_t\", mesh)\nboundary_parts.set_all(0)\n\nGamma_top = TopBoundary()\nGamma_top.mark(boundary_parts, 1)\nGamma_bottom = BottomBoundary()\nGamma_bottom.mark(boundary_parts, 2)\nGamma_left = LeftBoundary()\nGamma_left.mark(boundary_parts, 3)\nGamma_right = RightBoundary()\nGamma_right.mark(boundary_parts, 4)\n\n\n\n\n4. Define the coefficients of the PDE and the boundary conditions\n\n\nWe first define the coefficients of the PDE using the \nConstant\n and \nExpression\n classes. \nConstant\n is used to define coefficients that do not depend on the space coordinates, \nExpression\n is used to define coefficients that are a known function of the space coordinates \nx[0]\n (x-axis direction) and \nx[1]\n (y-axis direction).\n\n\nIn the finite element method community, Dirichlet boundary conditions are also known as \nessential\n boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class \nDirichletBC\n to indicate this type of condition.\n\n\nOn the other hand, Newman boundary conditions are also known as \nnatural\n boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure \nds[i]\n to integrate over the portion of the boundary marked with label \ni\n.\n\n\nu_L = Constant(0.)\nu_R = Constant(0.)\n\nsigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5)\nsigma_top    = Constant(0.)\n\nf = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5)\n\nbcs = [DirichletBC(Vh, u_L, boundary_parts, 3),\n       DirichletBC(Vh, u_R, boundary_parts, 4)]\n\nds = Measure(\"ds\", subdomain_data=boundary_parts)\n\n\n\n\n5. Define and solve the variational problem\n\n\nWe also define two special types of functions: the \nTrialFunction\n \nu\n and the \nTestFunction\n \nv\n. These special types of function are used by \nFEniCS\n to generate the finite element vectors and matrices which stem from the weak formulation of the PDE.\n\n\nMore specifically, by denoting by \n\\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)}\n the finite element basis for the space \nV_h\n, a function \nu_h \\in V_h\n can be written as\n\n u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), \n\nwhere \n{\\rm u}_i\n represents the coefficients in the finite element expansion of \nu_h\n.\n\n\nWe then define\n\n\n\n\n\n\nthe bilinear form \na(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx \n;\n\n\n\n\n\n\nthe linear form \nL(v_h) = \\int_\\Omega f v_h dx + + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds \n.\n\n\n\n\n\n\nWe can then solve the variational problem\n\n\nFind \nu_h \\in V_h\n\n\n such that\n\n\n a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h \n\n\n\n\nusing directly the built-in \nsolve\n method in FEniCS.\n\n\nNOTE:\n As an alternative one can also assemble the finite element matrix \nA\n and the right hand side \nb\n that stems from the discretization of \na\n and \nL\n, and then solve the linear system\n\n A {\\rm u} = {\\rm b}, \n\nwhere\n\n\n\n\n\n\n\n\n{\\rm u}\n is the vector collecting the coefficient of the finite element expasion of \nu_h\n,\n\n\n\n\n\n\nthe entries of the matrix A are such that \nA_{ij} = a(\\phi_j, \\phi_i)\n,\n\n\n\n\n\n\nthe entries of the right hand side b are such that \nb_i = L(\\phi_i)\n.\n\n\n\n\n\n\nu = TrialFunction(Vh)\nv = TestFunction(Vh)\na = inner(nabla_grad(u), nabla_grad(v))*dx\nL = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n\nuh = Function(Vh)\n\n#solve(a == L, uh, bcs=bcs)\nA, b = assemble_system(a,L, bcs=bcs)\nsolve(A, uh.vector(), b, \"cg\")\n\nnb.plot(uh)\n\n\n\n\n\n\n6. Compute the discretization error\n\n\nFor this problem, the exact solution is known.\nWe can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution \nu_h\n and the exact solution \nu_{\\rm ex}\n)\n\n \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, \n \nand\n\n \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. \n\n\n\n\nu_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5)\ngrad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5)\n\nerr_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\nerr_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\nerr_H1 = sqrt( err_L2**2 + err_grad**2)\n\nprint(\"|| u_h - u_e ||_L2 = \", err_L2)\nprint(\"|| u_h - u_e ||_H1 = \", err_H1)\n\n\n\n\n|| u_h - u_e ||_L2 =  0.00880525372208\n|| u_h - u_e ||_H1 =  0.396718952514\n\n\n\n7. Convergence of the finite element method\n\n\nWe now verify numerically a well-known convergence result for the finite element method.\n\n\nLet denote with \ns\n the polynomial degree of the finite element space, and assume that the solution \nu_{\\rm ex}\n is at least in \nH^{s+1}(\\Omega)\n. Then we have\n\n \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. \n\n\n\n\nIn the code below, the function \ncompute(n, degree)\n solves the PDE using a mesh with \nn\n elements in each direction and finite element spaces of polinomial order \ndegree\n.\n\n\nThe figure below shows the discretization errors in the \nH^1\n and \nL^2\n as a function of the mesh size \nh\n (\nh = \\frac{1}{n}\n) for piecewise linear (P1, \ns=1\n) and piecewise quadratic (P2, \ns=2\n) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular:\n\n\n\n\n\n\nfor piecewise linear finite element P1 we observe first order convergence in the \nH^1\n-norm and second order convergence in the \nL^2\n-norm;\n\n\n\n\n\n\nfor piecewise quadratic finite element P2 we observe second order convergence in the \nH^1\n-norm and third order convergence in the \nL^2\n-norm.\n\n\n\n\n\n\ndef compute(n, degree):\n    mesh = UnitSquareMesh(n, n)\n    Vh  = FunctionSpace(mesh, 'Lagrange', degree)\n    boundary_parts = FacetFunction(\"size_t\", mesh)\n    boundary_parts.set_all(0)\n\n    Gamma_top = TopBoundary()\n    Gamma_top.mark(boundary_parts, 1)\n    Gamma_bottom = BottomBoundary()\n    Gamma_bottom.mark(boundary_parts, 2)\n    Gamma_left = LeftBoundary()\n    Gamma_left.mark(boundary_parts, 3)\n    Gamma_right = RightBoundary()\n    Gamma_right.mark(boundary_parts, 4)\n\n    bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)]\n    ds = Measure(\"ds\", subdomain_data=boundary_parts)\n\n    u = TrialFunction(Vh)\n    v = TestFunction(Vh)\n    a = inner(nabla_grad(u), nabla_grad(v))*dx\n    L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n    uh = Function(Vh)\n    solve(a == L, uh, bcs=bcs)\n    err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\n    err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\n    err_H1 = sqrt( err_L2**2 + err_grad**2)\n\n    return err_L2, err_H1\n\nnref = 5\nn = 8*np.power(2,np.arange(0,nref))\nh = 1./n\n\nerr_L2_P1 = np.zeros(nref)\nerr_H1_P1 = np.zeros(nref)\nerr_L2_P2 = np.zeros(nref)\nerr_H1_P2 = np.zeros(nref)\n\nfor i in range(nref):\n    err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1)\n    err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nplt.loglog(h, err_H1_P1, '-or', label=\"H1 error\")\nplt.loglog(h, err_L2_P1, '-*b', label=\"L2 error\")\nplt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label=\"First Order\")\nplt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label=\"Second Order\")\nplt.xlabel(\"Mesh size h\")\nplt.ylabel(\"Error\")\nplt.title(\"P1 Finite Element\")\nplt.legend(loc = 'lower right')\n\n\nplt.subplot(122)\nplt.loglog(h, err_H1_P2, '-or', label=\"H1 error\")\nplt.loglog(h, err_L2_P2, '-*b', label=\"L2 error\")\nplt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label=\"Second Order\")\nplt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label=\"Third Order\")\nplt.xlabel(\"Mesh size h\")\nplt.ylabel(\"Error\")\nplt.title(\"P2 Finite Element\")\nplt.legend(loc='lower right')\n\nplt.show()\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "FEniCS101"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#fenics101-tutorial",
            "text": "In this tutorial we consider the boundary value problem (BVP)   \\begin{eqnarray*}\n- \\nabla \\cdot (k \\nabla u) = f &      \\text{ in } \\Omega,\\\\\nu = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\\nk \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom},\n\\end{eqnarray*}   where  \\Omega = (0,1) \\times (0,1) ,  \\Gamma_D  and and  \\Gamma_N  are the union of\nthe left and right, and top and bottom boundaries of  \\Omega ,\nrespectively.  Here \\begin{eqnarray*}\nk(x,y) = 1  & \\text{ on } \\Omega\\\\\nf(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\\nu_0(x,y)      = 0 & \\text{ on } \\Gamma_D, \\\\\n\\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right.\n& \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array}\n\\end{eqnarray*}   The exact solution is  u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right).",
            "title": "FEniCS101 Tutorial"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#weak-formulation",
            "text": "Let us define the Hilbert spaces  V_{u_0}, V_0 \\in \\Omega  as  V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\},   V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}.   To obtain the weak formulation, we multiply the PDE by an arbitrary function  v \\in V_0  and integrate over the domain  \\Omega  leading to    -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0.    Then, integration by parts the non-conforming term gives    \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0.    Finally by recalling that   v = 0   on  \\Gamma_D  and that  k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma   on  \\Gamma_N , we find the weak formulation:  Find *  u \\in V_{u_0}   such that*  \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0.",
            "title": "Weak formulation"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#1-load-modules",
            "text": "To start we load the following modules:    dolfin: the python/C++ interface to FEniCS    math : the python module for mathematical functions    numpy : a python package for linear algebra    matplotlib : a python package used for plotting the results    from __future__ import absolute_import, division, print_function\n\nfrom dolfin import *\n\nimport math\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#2-define-the-mesh-and-the-finite-element-space",
            "text": "We construct a triangulation (mesh)  \\mathcal{T}_h  of the computational domain  \\Omega := [0, 1]^2  with  n  elements in each direction.  On the mesh  \\mathcal{T}_h , we then define the finite element space  V_h \\subset H^1(\\Omega)  consisting of globally continuous piecewise polinomials functions. The  degree  variable defines the polinomial degree.  n = 16\ndegree = 1\nmesh = UnitSquareMesh(n, n)\nnb.plot(mesh)\n\nVh  = FunctionSpace(mesh, 'Lagrange', degree)\nprint(\"dim(Vh) = \", Vh.dim())  dim(Vh) =  289",
            "title": "2. Define the mesh and the finite element space"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#3-define-boundary-labels",
            "text": "To partition the boundary of  \\Omega  in the subdomains  \\Gamma_{\\rm top} ,  \\Gamma_{\\rm bottom} ,  \\Gamma_{\\rm left} ,  \\Gamma_{\\rm right}  we assign a unique label  boundary_parts  to each of part of  \\partial \\Omega .  class TopBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1] - 1) < DOLFIN_EPS\n\nclass BottomBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1]) < DOLFIN_EPS\n\nclass LeftBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0]) < DOLFIN_EPS\n\nclass RightBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0] - 1) < DOLFIN_EPS\n\nboundary_parts = FacetFunction(\"size_t\", mesh)\nboundary_parts.set_all(0)\n\nGamma_top = TopBoundary()\nGamma_top.mark(boundary_parts, 1)\nGamma_bottom = BottomBoundary()\nGamma_bottom.mark(boundary_parts, 2)\nGamma_left = LeftBoundary()\nGamma_left.mark(boundary_parts, 3)\nGamma_right = RightBoundary()\nGamma_right.mark(boundary_parts, 4)",
            "title": "3. Define boundary labels"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#4-define-the-coefficients-of-the-pde-and-the-boundary-conditions",
            "text": "We first define the coefficients of the PDE using the  Constant  and  Expression  classes.  Constant  is used to define coefficients that do not depend on the space coordinates,  Expression  is used to define coefficients that are a known function of the space coordinates  x[0]  (x-axis direction) and  x[1]  (y-axis direction).  In the finite element method community, Dirichlet boundary conditions are also known as  essential  boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class  DirichletBC  to indicate this type of condition.  On the other hand, Newman boundary conditions are also known as  natural  boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure  ds[i]  to integrate over the portion of the boundary marked with label  i .  u_L = Constant(0.)\nu_R = Constant(0.)\n\nsigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5)\nsigma_top    = Constant(0.)\n\nf = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5)\n\nbcs = [DirichletBC(Vh, u_L, boundary_parts, 3),\n       DirichletBC(Vh, u_R, boundary_parts, 4)]\n\nds = Measure(\"ds\", subdomain_data=boundary_parts)",
            "title": "4. Define the coefficients of the PDE and the boundary conditions"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#5-define-and-solve-the-variational-problem",
            "text": "We also define two special types of functions: the  TrialFunction   u  and the  TestFunction   v . These special types of function are used by  FEniCS  to generate the finite element vectors and matrices which stem from the weak formulation of the PDE.  More specifically, by denoting by  \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)}  the finite element basis for the space  V_h , a function  u_h \\in V_h  can be written as  u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x),  \nwhere  {\\rm u}_i  represents the coefficients in the finite element expansion of  u_h .  We then define    the bilinear form  a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx  ;    the linear form  L(v_h) = \\int_\\Omega f v_h dx + + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds  .    We can then solve the variational problem  Find  u_h \\in V_h   such that   a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h    using directly the built-in  solve  method in FEniCS.  NOTE:  As an alternative one can also assemble the finite element matrix  A  and the right hand side  b  that stems from the discretization of  a  and  L , and then solve the linear system  A {\\rm u} = {\\rm b},  \nwhere     {\\rm u}  is the vector collecting the coefficient of the finite element expasion of  u_h ,    the entries of the matrix A are such that  A_{ij} = a(\\phi_j, \\phi_i) ,    the entries of the right hand side b are such that  b_i = L(\\phi_i) .    u = TrialFunction(Vh)\nv = TestFunction(Vh)\na = inner(nabla_grad(u), nabla_grad(v))*dx\nL = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n\nuh = Function(Vh)\n\n#solve(a == L, uh, bcs=bcs)\nA, b = assemble_system(a,L, bcs=bcs)\nsolve(A, uh.vector(), b, \"cg\")\n\nnb.plot(uh)",
            "title": "5. Define and solve the variational problem"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#6-compute-the-discretization-error",
            "text": "For this problem, the exact solution is known.\nWe can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution  u_h  and the exact solution  u_{\\rm ex} )  \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx },   \nand  \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}.    u_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5)\ngrad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5)\n\nerr_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\nerr_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\nerr_H1 = sqrt( err_L2**2 + err_grad**2)\n\nprint(\"|| u_h - u_e ||_L2 = \", err_L2)\nprint(\"|| u_h - u_e ||_H1 = \", err_H1)  || u_h - u_e ||_L2 =  0.00880525372208\n|| u_h - u_e ||_H1 =  0.396718952514",
            "title": "6. Compute the discretization error"
        },
        {
            "location": "/tutorials_v1.6.0/1_FEniCS101/#7-convergence-of-the-finite-element-method",
            "text": "We now verify numerically a well-known convergence result for the finite element method.  Let denote with  s  the polynomial degree of the finite element space, and assume that the solution  u_{\\rm ex}  is at least in  H^{s+1}(\\Omega) . Then we have  \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}.    In the code below, the function  compute(n, degree)  solves the PDE using a mesh with  n  elements in each direction and finite element spaces of polinomial order  degree .  The figure below shows the discretization errors in the  H^1  and  L^2  as a function of the mesh size  h  ( h = \\frac{1}{n} ) for piecewise linear (P1,  s=1 ) and piecewise quadratic (P2,  s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular:    for piecewise linear finite element P1 we observe first order convergence in the  H^1 -norm and second order convergence in the  L^2 -norm;    for piecewise quadratic finite element P2 we observe second order convergence in the  H^1 -norm and third order convergence in the  L^2 -norm.    def compute(n, degree):\n    mesh = UnitSquareMesh(n, n)\n    Vh  = FunctionSpace(mesh, 'Lagrange', degree)\n    boundary_parts = FacetFunction(\"size_t\", mesh)\n    boundary_parts.set_all(0)\n\n    Gamma_top = TopBoundary()\n    Gamma_top.mark(boundary_parts, 1)\n    Gamma_bottom = BottomBoundary()\n    Gamma_bottom.mark(boundary_parts, 2)\n    Gamma_left = LeftBoundary()\n    Gamma_left.mark(boundary_parts, 3)\n    Gamma_right = RightBoundary()\n    Gamma_right.mark(boundary_parts, 4)\n\n    bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)]\n    ds = Measure(\"ds\", subdomain_data=boundary_parts)\n\n    u = TrialFunction(Vh)\n    v = TestFunction(Vh)\n    a = inner(nabla_grad(u), nabla_grad(v))*dx\n    L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n    uh = Function(Vh)\n    solve(a == L, uh, bcs=bcs)\n    err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\n    err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\n    err_H1 = sqrt( err_L2**2 + err_grad**2)\n\n    return err_L2, err_H1\n\nnref = 5\nn = 8*np.power(2,np.arange(0,nref))\nh = 1./n\n\nerr_L2_P1 = np.zeros(nref)\nerr_H1_P1 = np.zeros(nref)\nerr_L2_P2 = np.zeros(nref)\nerr_H1_P2 = np.zeros(nref)\n\nfor i in range(nref):\n    err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1)\n    err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nplt.loglog(h, err_H1_P1, '-or', label=\"H1 error\")\nplt.loglog(h, err_L2_P1, '-*b', label=\"L2 error\")\nplt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g', label=\"First Order\")\nplt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k', label=\"Second Order\")\nplt.xlabel(\"Mesh size h\")\nplt.ylabel(\"Error\")\nplt.title(\"P1 Finite Element\")\nplt.legend(loc = 'lower right')\n\n\nplt.subplot(122)\nplt.loglog(h, err_H1_P2, '-or', label=\"H1 error\")\nplt.loglog(h, err_L2_P2, '-*b', label=\"L2 error\")\nplt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g', label=\"Second Order\")\nplt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k', label=\"Third Order\")\nplt.xlabel(\"Mesh size h\")\nplt.ylabel(\"Error\")\nplt.title(\"P2 Finite Element\")\nplt.legend(loc='lower right')\n\nplt.show()   Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "7. Convergence of the finite element method"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/",
            "text": "Coefficient field inversion in an elliptic partial differential equation\n\n\nWe consider the estimation of a coefficient in an elliptic partial\ndifferential equation as a model problem. Depending on the\ninterpretation of the unknowns and the type of measurements, this\nmodel problem arises, for instance, in inversion for groundwater flow\nor heat conductivity.  It can also be interpreted as finding a\nmembrane with a certain spatially varying stiffness. Let\n\n\\Omega\\subset\\mathbb{R}^n\n, \nn\\in\\{1,2,3\\}\n be an open, bounded\ndomain and consider the following problem:\n\n\n\n\n\n\\min_{a} J(a):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx,\n\n\n\n\n\nwhere \nu\n is the solution of\n\n\n\n\n\n\\begin{split}\n\\quad -\\nabla\\cdot(\\exp(a)\\nabla u) &= f \\text{ in }\\Omega,\\\\\nu &= 0 \\text{ on }\\partial\\Omega.\n\\end{split}\n\n\n\n\n\nHere \na\\in U_{ad}:=\\{a\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\}\n the unknown coefficient field, \nu_d\n denotes (possibly noisy) data, \nf\\in H^{-1}(\\Omega)\n a given force, and \n\\gamma\\ge 0\n the regularization parameter.\n\n\nThe variational (or weak) form of the state equation:\n\n\nFind \nu\\in H_0^1(\\Omega)\n such that \n(\\exp(a)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega),\n\nwhere \nH_0^1(\\Omega)\n is the space of functions vanishing on \n\\partial\\Omega\n with square integrable derivatives. Here, \n(\\cdot\\,,\\cdot)\n denotes the \nL^2\n-inner product, i.e, for scalar functions \nu,v\n defined on \n\\Omega\n we denote \n(u,v) := \\int_\\Omega u(x) v(x) \\,dx\n.\n\n\nOptimality System:\n\n\nThe Lagrangian functional \n\\mathscr{L}:H^1(\\Omega)\\times H_0^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R}\n, which we use as a tool to derive the optimality system, is given by\n\n\n\n\n\n\\mathscr{L}(a,u,p):= \\frac{1}{2}(u-u_d,u-u_d) +\n\\frac{\\gamma}{2}(\\nabla a, \\nabla a) +  (\\exp(a)\\nabla u,\\nabla p) - (f,p).\n\n\n\n\n\nThe Lagrange multiplier theory shows that, at a solution all variations of the Lagrangian functional with respect to all variables must vanish. These variations of \n\\mathscr{L}\n with respect to \n(p,u,a)\n in directions \n(\\tilde{u}, \\tilde{p}, \\tilde{a})\n are given by\n\n\n\n\n\n  \\begin{alignat}{2}\n    \\mathscr{L}_p(a,u,p)(\\tilde{p})  &= (\\exp(a)\\nabla u, \\nabla \\tilde{p}) -\n    (f,\\tilde{p}) &&= 0,\\\\\n     \\mathscr{L}_u(a,u,p)(\\tilde{u}) &= (\\exp(a)\\nabla p, \\nabla \\tilde{u}) +\n     (u-u_d,\\tilde{u}) && = 0,\\\\\n     \\mathscr{L}_a(a,u,p)(\\tilde{a})  &= \\gamma(\\nabla a, \\nabla \\tilde{a}) +\n     (\\tilde{a}\\exp(a)\\nabla u, \\nabla p) &&= 0,\n  \\end{alignat}\n\n\n\n\n\nwhere the variations \n(\\tilde{u}, \\tilde{p}, \\tilde{a})\n are taken from the same spaces as \n(u,p,a)\n. \n\n\nThe gradient of the cost functional \n\\mathcal{J}(a)\n therefore is\n\n\n\n\n\n    \\mathcal{G}(a)(\\tilde a) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +\n     (\\tilde{a}\\exp(a)\\nabla u, \\nabla \\tilde{p}).\n\n\n\n\n\nInexact Newton-CG:\n\n\nNewton's method requires second-order variational derivatives of the Lagrangian . Written in abstract form, it computes an update direction \n(\\hat a_k, \\hat u_k,\\hat p_k)\n from the following Newton step for the Lagrangian functional:\n\n\n\n\n\n\\mathscr{L}''(a_k, u_k, p_k)\\left[(\\tilde\n  a, \\tilde u, \\tilde p),(\\hat a_k, \\hat u_k, \\hat p_k)\\right] =\n-\\mathscr{L}'(a_k,u_k,p_k)(\\tilde a, \\tilde u, \\tilde p),\n\n\n\n\n\nfor all variations \n(\\tilde a, \\tilde u, \\tilde p)\n, where \n\\mathscr{L}'\n and \n\\mathscr{L}''\n denote the first and\nsecond variations of the Lagrangian. For the elliptic parameter inversion problem, this Newton step (written in variatonal form) is as follows: Find \n(\\hat u_k, \\hat a_k,\\hat p_k)\n as the solution of the linear system\n\n\n\n\n\n  \\begin{array}{llll}\n    (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla\n    \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u,\n    \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla\n    p_k, \\nabla \\tilde u)\\\\\n    (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma\n    (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a\n     \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde\n      a  \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\\n    (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla\n      \\tilde p) & &= - (\\exp(a_k) \\nabla u_k,\n    \\nabla \\tilde p) + (f, \\tilde p),\n  \\end{array}\n\n\n\n\n\nfor all \n(\\tilde u, \\tilde a, \\tilde p)\n.\n\n\nDiscrete Newton system:\n\n\n\n\n\n\\def\\tu{\\tilde u}\n\\def\\btu{\\bf \\tilde u}\n\\def\\ta{\\tilde a}\n\\def\\bta{\\bf \\tilde a}\n\\def\\tp{\\tilde p}\n\\def\\btp{\\bf \\tilde p}\n\\def\\hu{\\hat u}\n\\def\\bhu{\\bf \\hat u}\n\\def\\ha{\\hat a}\n\\def\\bha{\\bf \\hat a}\n\\def\\hp{\\hat p}\n\\def\\bhp{\\bf \\hat p}\n\n\nThe discretized Newton step: denote the vectors corresponding to the discretization of the functions \n\\ha_k,\\hu_k, \\hp_k\n by \n\\bf \\bha_k, \\bhu_k\n and \n\\bhp_k\n. Then, the discretization of the above system is given by the following symmetric linear system:\n\n\n\n\n\n  \\begin{bmatrix}\n    \\bf W_{\\scriptsize\\mbox{uu}} & \\bf W_{\\scriptsize\\mbox{ua}} & \\bf A^T \\\\\n    \\bf W_{\\scriptsize\\mbox{au}} & \\bf R + \\bf R_{\\scriptsize\\mbox{aa}}& \\bf C^T \\\\\n    \\bf A & \\bf C & 0\n\\end{bmatrix}\n\\left[\n  \\begin{array}{c}\n    \\bhu_k \\\\\n    \\bha_k \\\\\n    \\bhp_k\n  \\end{array} \\right] =\n-\\left[\n  \\begin{array}{ccc}\n    \\bf{g}_u\\\\\n    \\bf{g}_a\\\\\n    \\bf{g}_p\n\\end{array}\n  \\right],\n\n\n\n\n\nwhere \n\\bf W_{\\scriptsize \\mbox{uu}}\n, \n\\bf W_{\\scriptsize\\mbox{ua}}\n, \n\\bf W_{\\scriptsize\\mbox{au}}\n, and \n\\bf R\n are the components of the Hessian matrix of the Lagrangian, \n\\bf A\n and \n\\bf C\n are the Jacobian of the state equation with respect to the state and the control variables, respectively and \n\\bf g_u\n, \n\\bf g_a\n, and \n\\bf g_p\n are the discrete gradients of the Lagrangian with respect to \n\\bf u \n, \n\\bf a\n and \n\\bf p\n, respectively.\n\n\nReduced Hessian apply:\n\n\nTo eliminate the incremental state and adjoint variables, \n\\bhu_k\n and \n\\bhp_k\n, from the first and last equations we use\n\n\n\n\n\n\\begin{align}\n\\bhu_k &= -\\bf A^{-1} \\bf C \\, \\bha_k,\\\\\n\\bhp_k &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu_k +\n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha_k).\n\\end{align}\n\n\n\n\n\nThis results in the following reduced linear system for the Newton step\n\n\n\n\n\n  \\bf H \\, \\bha_k = -\\bf{g}_a,\n\n\n\n\n\nwith the reduced Hessian \n\\bf H\n applied to a vector \n\\bha\n given by\n\n\n\n\n\n  \\bf H \\bha = \\underbrace{(\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})}_{\\text{Hessian of the regularization}} \\bha +\n    \\underbrace{(\\bf C^{T}\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}}\n    \\bf A^{-1} \\bf C - \\bf W_{\\scriptsize\\mbox{ua}}) -\n    \\bf W_{\\scriptsize\\mbox{au}} \\bf A^{-1}\n    \\bf C)}_{\\text{Hessian of the data misfit}}\\;\\bha.\n\n\n\n\n\nGoals:\n\n\nBy the end of this notebook, you should be able to:\n\n\n\n\nsolve the forward and adjoint Poisson equations\n\n\nunderstand the inverse method framework\n\n\nvisualise and understand the results\n\n\nmodify the problem and code\n\n\n\n\nMathematical tools used:\n\n\n\n\nFinite element method\n\n\nDerivation of gradiant and Hessian via the adjoint method\n\n\ninexact Newton-CG\n\n\nArmijo line search\n\n\n\n\nList of software used:\n\n\n\n\nFEniCS\n, a parallel finite element element library for the discretization of partial differential equations\n\n\nPETSc\n, for scalable and efficient linear algebra operations and solvers\n\n\nMatplotlib\n, a python package used for plotting the results\n\n\nNumpy\n, a python package for linear algebra\n\n\n\n\nSet up\n\n\nImport dependencies\n\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom dolfin import *\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\nnp.random.seed(seed=1)\n\n\n\n\nModel set up:\n\n\nAs in the introduction, the first thing we need to do is set up the numerical model.  In this cell, we set the mesh, the finite element functions \nu, p, g\n corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization.\n\n\n# create mesh and define function spaces\nnx = 64\nny = 64\nmesh = UnitSquareMesh(nx, ny)\nVa = FunctionSpace(mesh, 'Lagrange', 1)\nVu = FunctionSpace(mesh, 'Lagrange', 2)\n\n# The true and inverted parameter\natrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5),Va)\na = interpolate(Expression(\"log(2.0)\", degree=1),Va)\n\n# define function for state and adjoint\nu = Function(Vu)\np = Function(Vu)\n\n# define Trial and Test Functions\nu_trial, p_trial, a_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Va)\nu_test, p_test, a_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Va)\n\n# initialize input functions\nf = Constant(\"1.0\")\nu0 = Constant(\"0.0\")\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on')\nnb.plot(atrue,subplot_loc=122, mytitle=\"True parameter field\")\nplt.show()\n\n\n\n\n\n\n# set up dirichlet boundary conditions\ndef boundary(x,on_boundary):\n    return on_boundary\n\nbc_state = DirichletBC(Vu, u0, boundary)\nbc_adj = DirichletBC(Vu, Constant(0.), boundary)\n\n\n\n\nSet up synthetic observations:\n\n\n\n\nPropose a coefficient field \na_{\\text true}\n shown above\n\n\n\n\nThe weak form of the pde: \n    Find \nu\\in H_0^1(\\Omega)\n such that \n\\underbrace{(\\exp(a_{\\text true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega)\n.\n\n\n\n\n\n\nPerturb the solution: \nu = u + \\eta\n, where \n\\eta \\sim \\mathcal{N}(0, \\sigma)\n\n\n\n\n\n\n\n\n# noise level\nnoise_level = 0.05\n\n# weak form for setting up the synthetic observations\na_goal = inner(exp(atrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_goal = f * u_test * dx\n\n# solve the forward/state problem to generate synthetic observations\ngoal_A, goal_b = assemble_system(a_goal, L_goal, bc_state)\n\nutrue = Function(Vu)\nsolve(goal_A, utrue.vector(), goal_b)\n\nud = Function(Vu)\nud.assign(utrue)\n\n# perturb state solution and create synthetic measurements ud\n# ud = u + ||u||/SNR * random.normal\nMAX = ud.vector().norm(\"linf\")\nnoise = Vector()\ngoal_A.init_vector(noise,1)\nnoise.set_local( noise_level * MAX * np.random.normal(0, 1, Vu.dim()))\nbc_adj.apply(noise)\n\nud.vector().axpy(1., noise)\n\n# plot\nnb.multi1_plot([utrue, ud], [\"State solution with atrue\", \"Synthetic observations\"])\nplt.show()\n\n\n\n\n\n\nThe cost function evaluation:\n\n\n\n\n\nJ(a):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text misfit} + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx}_{\\text reg}\n\n\n\n\n\nIn the code below, \nW\n and \nR\n are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively.\n\n\n# regularization parameter\ngamma = 1e-8\n\n# weak for for setting up the misfit and regularization compoment of the cost\nW_equ   = inner(u_trial, u_test) * dx\nR_equ   = gamma * inner(nabla_grad(a_trial), nabla_grad(a_test)) * dx\n\nW = assemble(W_equ)\nR = assemble(R_equ)\n\n# refine cost function\ndef cost(u, ud, a, W, R):\n    diff = u.vector() - ud.vector()\n    reg = 0.5 * a.vector().inner(R*a.vector() ) \n    misfit = 0.5 * diff.inner(W * diff)\n    return [reg + misfit, misfit, reg]\n\n\n\n\nSetting up the state equations, right hand side for the adjoint and the necessary matrices:\n\n\n\n\n\n  \\begin{array}{llll}\n    (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla\n    \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u,\n    \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla\n    p_k, \\nabla \\tilde u)\\\\\n    (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma\n    (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a\n     \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde\n      a  \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\\n    (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla\n      \\tilde p) & &= - (\\exp(a_k) \\nabla u_k,\n    \\nabla \\tilde p) + (f, \\tilde p),\n  \\end{array}\n\n\n\n\n\n# weak form for setting up the state equation\na_state = inner(exp(a) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_state = f * u_test * dx\n\n# weak form for setting up the adjoint equation\na_adj = inner(exp(a) * nabla_grad(p_trial), nabla_grad(p_test)) * dx\nL_adj = -inner(u - ud, p_test) * dx\n\n# weak form for setting up matrices\nWua_equ = inner(exp(a) * a_trial * nabla_grad(p_test), nabla_grad(p)) * dx\nC_equ   = inner(exp(a) * a_trial * nabla_grad(u), nabla_grad(u_test)) * dx\nRaa_equ = inner(exp(a) * a_trial * a_test *  nabla_grad(u),  nabla_grad(p)) * dx\n\nM_equ   = inner(a_trial, a_test) * dx\n\n# assemble matrix M\nM = assemble(M_equ)\n\n\n\n\nInitial guess\n\n\nWe solve the state equation and compute the cost functional for the initial guess of the parameter \na_ini\n\n\n# solve state equation\nstate_A, state_b = assemble_system (a_state, L_state, bc_state)\nsolve (state_A, u.vector(), state_b)\n\n# evaluate cost\n[cost_old, misfit_old, reg_old] = cost(u, ud, a, W, R)\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(a,subplot_loc=121, mytitle=\"a_ini\", vmin=atrue.vector().min(), vmax=atrue.vector().max())\nnb.plot(u,subplot_loc=122, mytitle=\"u(a_ini)\")\nplt.show()\n\n\n\n\n\n\nThe reduced Hessian apply to a vector v:\n\n\nHere we describe how to apply the reduced Hessian operator to a vector v. For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined.\n\n\nFor this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm.\n\n\nThe Hessian apply reads:\n\n\n\\begin{align}\n\\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu +\n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha) & \\text{adjoint}\\\\\n\\bf H \\bf v &= (\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})\\bf v + \\bf C^T \\bhp + \\bf W_{\\scriptsize\\mbox{au}} \\bhu.\n\\end{align}\n\n\n\n\n\nThe Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha\n, \n\\bf R_{\\scriptsize\\mbox{aa}}\\bf v\n, and \n\\bf W_{\\scriptsize\\mbox{au}} \\bhu\n:\n\n\n\\begin{align}\n\\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bf A^{-T} \\bf W_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\\n\\bf H_{\\rm GN} \\bf v &= \\bf R \\bf v + \\bf C^T \\bhp.\n\\end{align}\n\n\n\n\n\n# Class HessianOperator to perform Hessian apply to a vector\nclass HessianOperator():\n    cgiter = 0\n    def __init__(self, R, Raa, C, A, adj_A, W, Wua, use_gaussnewton=False):\n        self.R = R\n        self.Raa = Raa\n        self.C = C\n        self.A = A\n        self.adj_A = adj_A\n        self.W = W\n        self.Wua = Wua\n        self.use_gaussnewton = use_gaussnewton\n\n        # incremental state\n        self.du = Vector()\n        self.A.init_vector(self.du,0)\n\n        #incremental adjoint\n        self.dp = Vector()\n        self.adj_A.init_vector(self.dp,0)\n\n        # auxiliary vectors\n        self.CT_dp = Vector()\n        self.C.init_vector(self.CT_dp, 1)\n        self.Wua_du = Vector()\n        self.Wua.init_vector(self.Wua_du, 1)\n\n    def init_vector(self, v, dim):\n        self.R.init_vector(v,dim)\n\n    # Hessian performed on x, output as generic vector y\n    def mult(self, v, y):\n        self.cgiter += 1\n        y.zero()\n        if self.use_gaussnewton:\n            self.mult_GaussNewton(v,y)\n        else:\n            self.mult_Newton(v,y)\n\n    # define (Gauss-Newton) Hessian apply H * v\n    def mult_GaussNewton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = - (self.W * self.du)\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        # Reg/Prior term\n        self.R.mult(v,y)\n\n        # Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1, self.CT_dp)\n\n    # define (Newton) Hessian apply H * v\n    def mult_Newton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = -(self.W * self.du) -  self.Wua * v\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        #Reg/Prior term\n        self.R.mult(v,y)\n        y.axpy(1., Raa*v)\n\n        #Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1., self.CT_dp)\n        self.Wua.transpmult(self.du, self.Wua_du)\n        y.axpy(1., self.Wua_du)\n\n\n\n\nThe inexact Newton-CG optimization with Armijo line search:\n\n\nWe solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search.\n\n\nThe stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \n\\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau\n).\n\n\nFirst, we compute the gradient by solving the state and adjoint equation for the current parameter \na\n, and then substituing the current state \nu\n, parameter \na\n and adjoint \np\n variables in the weak form expression of the gradient:\n\n (g, \\tilde{a}) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +(\\tilde{a}\\nabla u, \\nabla p).\n\n\n\n\nThen, we compute the Newton direction \n\\delta a\n by iteratively solving \n{\\bf H} {\\delta a} = - {\\bf g}\n.\nThe Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug  (to avoid negative curvature) criteria.\n\n\nFinally, the Armijo line search uses backtracking to find \n\\alpha\n such that a sufficient reduction in the cost functional is achieved.\nMore specifically, we use backtracking to find \n\\alpha\n such that:\n\nJ( a + \\alpha \\delta a ) \\leq J(a) + \\alpha c_{\\rm armijo} (\\delta a,g). \n\n\n\n\n# define parameters for the optimization\ntol = 1e-8\nc = 1e-4\nmaxiter = 12\nplot_on = False\n\n# initialize iter counters\niter = 1\ntotal_cg_iter = 0\nconverged = False\n\n# initializations\ng, a_delta = Vector(), Vector()\nR.init_vector(a_delta,0)\nR.init_vector(g,0)\n\na_prev = Function(Va)\n\nprint(\"Nit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\")\n\nwhile iter <  maxiter and not converged:\n\n    # assemble matrix C\n    C =  assemble(C_equ)\n\n    # solve the adoint problem\n    adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj)\n    solve(adjoint_A, p.vector(), adjoint_RHS)\n\n    # assemble W_ua and R\n    Wua = assemble (Wua_equ)\n    Raa = assemble (Raa_equ)\n\n    # evaluate the  gradient\n    CT_p = Vector()\n    C.init_vector(CT_p,1)\n    C.transpmult(p.vector(), CT_p)\n    MG = CT_p + R * a.vector()\n    solve(M, g, MG)\n\n    # calculate the norm of the gradient\n    grad2 = g.inner(MG)\n    gradnorm = sqrt(grad2)\n\n    # set the CG tolerance (use Eisenstat\u2013Walker termination criterion)\n    if iter == 1:\n        gradnorm_ini = gradnorm\n    tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini))\n\n    # define the Hessian apply operator (with preconditioner)\n    Hess_Apply = HessianOperator(R, Raa, C, state_A, adjoint_A, W, Wua, use_gaussnewton=(iter<6) )\n    P = R + gamma * M\n    Psolver = PETScKrylovSolver(\"cg\", amg_method())\n    Psolver.set_operator(P)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(Hess_Apply)\n    solver.set_preconditioner(Psolver)\n    solver.parameters[\"rel_tolerance\"] = tolcg\n    solver.parameters[\"zero_initial_guess\"] = True\n    solver.parameters[\"print_level\"] = -1\n\n    # solve the Newton system H a_delta = - MG\n    solver.solve(a_delta, -MG)\n    total_cg_iter += Hess_Apply.cgiter\n\n    # linesearch\n    alpha = 1\n    descent = 0\n    no_backtrack = 0\n    a_prev.assign(a)\n    while descent == 0 and no_backtrack < 10:\n        a.vector().axpy(alpha, a_delta )\n\n        # solve the state/forward problem\n        state_A, state_b = assemble_system(a_state, L_state, bc_state)\n        solve(state_A, u.vector(), state_b)\n\n        # evaluate cost\n        [cost_new, misfit_new, reg_new] = cost(u, ud, a, W, R)\n\n        # check if Armijo conditions are satisfied\n        if cost_new < cost_old + alpha * c * MG.inner(a_delta):\n            cost_old = cost_new\n            descent = 1\n        else:\n            no_backtrack += 1\n            alpha *= 0.5\n            a.assign(a_prev)  # reset a\n\n    # calculate sqrt(-G * D)\n    graddir = sqrt(- MG.inner(a_delta) )\n\n    sp = \"\"\n    print(\"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\\n        (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\\n         graddir, sp, gradnorm, sp, alpha, sp, tolcg) )\n\n    if plot_on:\n        nb.multi1_plot([a,u,p], [\"a\",\"u\",\"p\"], same_colorbar=False)\n        plt.show()\n\n    # check for convergence\n    if gradnorm < tol and iter > 1:\n        converged = True\n        print(\"Newton's method converged in \",iter,\"  iterations\")\n        print(\"Total number of CG iterations: \", total_cg_iter)\n\n    iter += 1\n\nif not converged:\n    print(\"Newton's method did not converge in \", maxiter, \" iterations\")\n\n\n\n\nNit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\n 1     1     1.12708e-05   1.12708e-05   1.33979e-11   1.56540e-02   3.79427e-04    1.00   5.000e-01\n 2     1     7.79732e-07   7.79695e-07   3.67737e-11   4.68278e-03   5.35002e-05    1.00   3.755e-01\n 3     1     3.10620e-07   3.10571e-07   4.91259e-11   9.71633e-04   7.13895e-06    1.00   1.372e-01\n 4     5     1.92183e-07   1.62405e-07   2.97780e-08   4.51694e-04   1.00276e-06    1.00   5.141e-02\n 5     1     1.86913e-07   1.57119e-07   2.97941e-08   1.02668e-04   6.12750e-07    1.00   4.019e-02\n 6    12     1.80408e-07   1.37719e-07   4.26890e-08   1.15975e-04   2.24111e-07    1.00   2.430e-02\n 7     5     1.80331e-07   1.38935e-07   4.13963e-08   1.23223e-05   4.17399e-08    1.00   1.049e-02\n 8    15     1.80330e-07   1.39056e-07   4.12734e-08   1.74451e-06   3.43216e-09    1.00   3.008e-03\nNewton's method converged in  8   iterations\nTotal number of CG iterations:  41\n\n\n\nnb.multi1_plot([atrue, a], [\"atrue\", \"a\"])\nnb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False)\nplt.show()\n\n\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "Poisson Deterministic"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#coefficient-field-inversion-in-an-elliptic-partial-differential-equation",
            "text": "We consider the estimation of a coefficient in an elliptic partial\ndifferential equation as a model problem. Depending on the\ninterpretation of the unknowns and the type of measurements, this\nmodel problem arises, for instance, in inversion for groundwater flow\nor heat conductivity.  It can also be interpreted as finding a\nmembrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n ,  n\\in\\{1,2,3\\}  be an open, bounded\ndomain and consider the following problem:   \n\\min_{a} J(a):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx,   where  u  is the solution of   \n\\begin{split}\n\\quad -\\nabla\\cdot(\\exp(a)\\nabla u) &= f \\text{ in }\\Omega,\\\\\nu &= 0 \\text{ on }\\partial\\Omega.\n\\end{split}   Here  a\\in U_{ad}:=\\{a\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\}  the unknown coefficient field,  u_d  denotes (possibly noisy) data,  f\\in H^{-1}(\\Omega)  a given force, and  \\gamma\\ge 0  the regularization parameter.",
            "title": "Coefficient field inversion in an elliptic partial differential equation"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#the-variational-or-weak-form-of-the-state-equation",
            "text": "Find  u\\in H_0^1(\\Omega)  such that  (\\exp(a)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega), \nwhere  H_0^1(\\Omega)  is the space of functions vanishing on  \\partial\\Omega  with square integrable derivatives. Here,  (\\cdot\\,,\\cdot)  denotes the  L^2 -inner product, i.e, for scalar functions  u,v  defined on  \\Omega  we denote  (u,v) := \\int_\\Omega u(x) v(x) \\,dx .",
            "title": "The variational (or weak) form of the state equation:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#optimality-system",
            "text": "The Lagrangian functional  \\mathscr{L}:H^1(\\Omega)\\times H_0^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R} , which we use as a tool to derive the optimality system, is given by   \n\\mathscr{L}(a,u,p):= \\frac{1}{2}(u-u_d,u-u_d) +\n\\frac{\\gamma}{2}(\\nabla a, \\nabla a) +  (\\exp(a)\\nabla u,\\nabla p) - (f,p).   The Lagrange multiplier theory shows that, at a solution all variations of the Lagrangian functional with respect to all variables must vanish. These variations of  \\mathscr{L}  with respect to  (p,u,a)  in directions  (\\tilde{u}, \\tilde{p}, \\tilde{a})  are given by   \n  \\begin{alignat}{2}\n    \\mathscr{L}_p(a,u,p)(\\tilde{p})  &= (\\exp(a)\\nabla u, \\nabla \\tilde{p}) -\n    (f,\\tilde{p}) &&= 0,\\\\\n     \\mathscr{L}_u(a,u,p)(\\tilde{u}) &= (\\exp(a)\\nabla p, \\nabla \\tilde{u}) +\n     (u-u_d,\\tilde{u}) && = 0,\\\\\n     \\mathscr{L}_a(a,u,p)(\\tilde{a})  &= \\gamma(\\nabla a, \\nabla \\tilde{a}) +\n     (\\tilde{a}\\exp(a)\\nabla u, \\nabla p) &&= 0,\n  \\end{alignat}   where the variations  (\\tilde{u}, \\tilde{p}, \\tilde{a})  are taken from the same spaces as  (u,p,a) .   The gradient of the cost functional  \\mathcal{J}(a)  therefore is   \n    \\mathcal{G}(a)(\\tilde a) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +\n     (\\tilde{a}\\exp(a)\\nabla u, \\nabla \\tilde{p}).",
            "title": "Optimality System:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#inexact-newton-cg",
            "text": "Newton's method requires second-order variational derivatives of the Lagrangian . Written in abstract form, it computes an update direction  (\\hat a_k, \\hat u_k,\\hat p_k)  from the following Newton step for the Lagrangian functional:   \n\\mathscr{L}''(a_k, u_k, p_k)\\left[(\\tilde\n  a, \\tilde u, \\tilde p),(\\hat a_k, \\hat u_k, \\hat p_k)\\right] =\n-\\mathscr{L}'(a_k,u_k,p_k)(\\tilde a, \\tilde u, \\tilde p),   for all variations  (\\tilde a, \\tilde u, \\tilde p) , where  \\mathscr{L}'  and  \\mathscr{L}''  denote the first and\nsecond variations of the Lagrangian. For the elliptic parameter inversion problem, this Newton step (written in variatonal form) is as follows: Find  (\\hat u_k, \\hat a_k,\\hat p_k)  as the solution of the linear system   \n  \\begin{array}{llll}\n    (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla\n    \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u,\n    \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla\n    p_k, \\nabla \\tilde u)\\\\\n    (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma\n    (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a\n     \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde\n      a  \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\\n    (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla\n      \\tilde p) & &= - (\\exp(a_k) \\nabla u_k,\n    \\nabla \\tilde p) + (f, \\tilde p),\n  \\end{array}   for all  (\\tilde u, \\tilde a, \\tilde p) .",
            "title": "Inexact Newton-CG:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#discrete-newton-system",
            "text": "\\def\\tu{\\tilde u}\n\\def\\btu{\\bf \\tilde u}\n\\def\\ta{\\tilde a}\n\\def\\bta{\\bf \\tilde a}\n\\def\\tp{\\tilde p}\n\\def\\btp{\\bf \\tilde p}\n\\def\\hu{\\hat u}\n\\def\\bhu{\\bf \\hat u}\n\\def\\ha{\\hat a}\n\\def\\bha{\\bf \\hat a}\n\\def\\hp{\\hat p}\n\\def\\bhp{\\bf \\hat p} \nThe discretized Newton step: denote the vectors corresponding to the discretization of the functions  \\ha_k,\\hu_k, \\hp_k  by  \\bf \\bha_k, \\bhu_k  and  \\bhp_k . Then, the discretization of the above system is given by the following symmetric linear system:   \n  \\begin{bmatrix}\n    \\bf W_{\\scriptsize\\mbox{uu}} & \\bf W_{\\scriptsize\\mbox{ua}} & \\bf A^T \\\\\n    \\bf W_{\\scriptsize\\mbox{au}} & \\bf R + \\bf R_{\\scriptsize\\mbox{aa}}& \\bf C^T \\\\\n    \\bf A & \\bf C & 0\n\\end{bmatrix}\n\\left[\n  \\begin{array}{c}\n    \\bhu_k \\\\\n    \\bha_k \\\\\n    \\bhp_k\n  \\end{array} \\right] =\n-\\left[\n  \\begin{array}{ccc}\n    \\bf{g}_u\\\\\n    \\bf{g}_a\\\\\n    \\bf{g}_p\n\\end{array}\n  \\right],   where  \\bf W_{\\scriptsize \\mbox{uu}} ,  \\bf W_{\\scriptsize\\mbox{ua}} ,  \\bf W_{\\scriptsize\\mbox{au}} , and  \\bf R  are the components of the Hessian matrix of the Lagrangian,  \\bf A  and  \\bf C  are the Jacobian of the state equation with respect to the state and the control variables, respectively and  \\bf g_u ,  \\bf g_a , and  \\bf g_p  are the discrete gradients of the Lagrangian with respect to  \\bf u  ,  \\bf a  and  \\bf p , respectively.",
            "title": "Discrete Newton system:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#reduced-hessian-apply",
            "text": "To eliminate the incremental state and adjoint variables,  \\bhu_k  and  \\bhp_k , from the first and last equations we use   \n\\begin{align}\n\\bhu_k &= -\\bf A^{-1} \\bf C \\, \\bha_k,\\\\\n\\bhp_k &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu_k +\n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha_k).\n\\end{align}   This results in the following reduced linear system for the Newton step   \n  \\bf H \\, \\bha_k = -\\bf{g}_a,   with the reduced Hessian  \\bf H  applied to a vector  \\bha  given by   \n  \\bf H \\bha = \\underbrace{(\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})}_{\\text{Hessian of the regularization}} \\bha +\n    \\underbrace{(\\bf C^{T}\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}}\n    \\bf A^{-1} \\bf C - \\bf W_{\\scriptsize\\mbox{ua}}) -\n    \\bf W_{\\scriptsize\\mbox{au}} \\bf A^{-1}\n    \\bf C)}_{\\text{Hessian of the data misfit}}\\;\\bha.",
            "title": "Reduced Hessian apply:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#goals",
            "text": "By the end of this notebook, you should be able to:   solve the forward and adjoint Poisson equations  understand the inverse method framework  visualise and understand the results  modify the problem and code",
            "title": "Goals:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#mathematical-tools-used",
            "text": "Finite element method  Derivation of gradiant and Hessian via the adjoint method  inexact Newton-CG  Armijo line search",
            "title": "Mathematical tools used:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#list-of-software-used",
            "text": "FEniCS , a parallel finite element element library for the discretization of partial differential equations  PETSc , for scalable and efficient linear algebra operations and solvers  Matplotlib , a python package used for plotting the results  Numpy , a python package for linear algebra",
            "title": "List of software used:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#set-up",
            "text": "",
            "title": "Set up"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#import-dependencies",
            "text": "from __future__ import absolute_import, division, print_function\n\nfrom dolfin import *\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\nnp.random.seed(seed=1)",
            "title": "Import dependencies"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#model-set-up",
            "text": "As in the introduction, the first thing we need to do is set up the numerical model.  In this cell, we set the mesh, the finite element functions  u, p, g  corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization.  # create mesh and define function spaces\nnx = 64\nny = 64\nmesh = UnitSquareMesh(nx, ny)\nVa = FunctionSpace(mesh, 'Lagrange', 1)\nVu = FunctionSpace(mesh, 'Lagrange', 2)\n\n# The true and inverted parameter\natrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) > 0.2))', degree=5),Va)\na = interpolate(Expression(\"log(2.0)\", degree=1),Va)\n\n# define function for state and adjoint\nu = Function(Vu)\np = Function(Vu)\n\n# define Trial and Test Functions\nu_trial, p_trial, a_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Va)\nu_test, p_test, a_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Va)\n\n# initialize input functions\nf = Constant(\"1.0\")\nu0 = Constant(\"0.0\")\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(mesh,subplot_loc=121, mytitle=\"Mesh\", show_axis='on')\nnb.plot(atrue,subplot_loc=122, mytitle=\"True parameter field\")\nplt.show()   # set up dirichlet boundary conditions\ndef boundary(x,on_boundary):\n    return on_boundary\n\nbc_state = DirichletBC(Vu, u0, boundary)\nbc_adj = DirichletBC(Vu, Constant(0.), boundary)",
            "title": "Model set up:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#set-up-synthetic-observations",
            "text": "Propose a coefficient field  a_{\\text true}  shown above   The weak form of the pde: \n    Find  u\\in H_0^1(\\Omega)  such that  \\underbrace{(\\exp(a_{\\text true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) .    Perturb the solution:  u = u + \\eta , where  \\eta \\sim \\mathcal{N}(0, \\sigma)     # noise level\nnoise_level = 0.05\n\n# weak form for setting up the synthetic observations\na_goal = inner(exp(atrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_goal = f * u_test * dx\n\n# solve the forward/state problem to generate synthetic observations\ngoal_A, goal_b = assemble_system(a_goal, L_goal, bc_state)\n\nutrue = Function(Vu)\nsolve(goal_A, utrue.vector(), goal_b)\n\nud = Function(Vu)\nud.assign(utrue)\n\n# perturb state solution and create synthetic measurements ud\n# ud = u + ||u||/SNR * random.normal\nMAX = ud.vector().norm(\"linf\")\nnoise = Vector()\ngoal_A.init_vector(noise,1)\nnoise.set_local( noise_level * MAX * np.random.normal(0, 1, Vu.dim()))\nbc_adj.apply(noise)\n\nud.vector().axpy(1., noise)\n\n# plot\nnb.multi1_plot([utrue, ud], [\"State solution with atrue\", \"Synthetic observations\"])\nplt.show()",
            "title": "Set up synthetic observations:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#the-cost-function-evaluation",
            "text": "J(a):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text misfit} + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx}_{\\text reg}   In the code below,  W  and  R  are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively.  # regularization parameter\ngamma = 1e-8\n\n# weak for for setting up the misfit and regularization compoment of the cost\nW_equ   = inner(u_trial, u_test) * dx\nR_equ   = gamma * inner(nabla_grad(a_trial), nabla_grad(a_test)) * dx\n\nW = assemble(W_equ)\nR = assemble(R_equ)\n\n# refine cost function\ndef cost(u, ud, a, W, R):\n    diff = u.vector() - ud.vector()\n    reg = 0.5 * a.vector().inner(R*a.vector() ) \n    misfit = 0.5 * diff.inner(W * diff)\n    return [reg + misfit, misfit, reg]",
            "title": "The cost function evaluation:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#setting-up-the-state-equations-right-hand-side-for-the-adjoint-and-the-necessary-matrices",
            "text": "\\begin{array}{llll}\n    (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla\n    \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u,\n    \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla\n    p_k, \\nabla \\tilde u)\\\\\n    (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma\n    (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a\n     \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde\n      a  \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\\n    (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla\n      \\tilde p) & &= - (\\exp(a_k) \\nabla u_k,\n    \\nabla \\tilde p) + (f, \\tilde p),\n  \\end{array}   # weak form for setting up the state equation\na_state = inner(exp(a) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_state = f * u_test * dx\n\n# weak form for setting up the adjoint equation\na_adj = inner(exp(a) * nabla_grad(p_trial), nabla_grad(p_test)) * dx\nL_adj = -inner(u - ud, p_test) * dx\n\n# weak form for setting up matrices\nWua_equ = inner(exp(a) * a_trial * nabla_grad(p_test), nabla_grad(p)) * dx\nC_equ   = inner(exp(a) * a_trial * nabla_grad(u), nabla_grad(u_test)) * dx\nRaa_equ = inner(exp(a) * a_trial * a_test *  nabla_grad(u),  nabla_grad(p)) * dx\n\nM_equ   = inner(a_trial, a_test) * dx\n\n# assemble matrix M\nM = assemble(M_equ)",
            "title": "Setting up the state equations, right hand side for the adjoint and the necessary matrices:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#initial-guess",
            "text": "We solve the state equation and compute the cost functional for the initial guess of the parameter  a_ini  # solve state equation\nstate_A, state_b = assemble_system (a_state, L_state, bc_state)\nsolve (state_A, u.vector(), state_b)\n\n# evaluate cost\n[cost_old, misfit_old, reg_old] = cost(u, ud, a, W, R)\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(a,subplot_loc=121, mytitle=\"a_ini\", vmin=atrue.vector().min(), vmax=atrue.vector().max())\nnb.plot(u,subplot_loc=122, mytitle=\"u(a_ini)\")\nplt.show()",
            "title": "Initial guess"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#the-reduced-hessian-apply-to-a-vector-v",
            "text": "Here we describe how to apply the reduced Hessian operator to a vector v. For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined.  For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm.  The Hessian apply reads: \n\\begin{align}\n\\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu +\n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha) & \\text{adjoint}\\\\\n\\bf H \\bf v &= (\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})\\bf v + \\bf C^T \\bhp + \\bf W_{\\scriptsize\\mbox{au}} \\bhu.\n\\end{align}   The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators  \\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha ,  \\bf R_{\\scriptsize\\mbox{aa}}\\bf v , and  \\bf W_{\\scriptsize\\mbox{au}} \\bhu : \n\\begin{align}\n\\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bf A^{-T} \\bf W_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\\n\\bf H_{\\rm GN} \\bf v &= \\bf R \\bf v + \\bf C^T \\bhp.\n\\end{align}   # Class HessianOperator to perform Hessian apply to a vector\nclass HessianOperator():\n    cgiter = 0\n    def __init__(self, R, Raa, C, A, adj_A, W, Wua, use_gaussnewton=False):\n        self.R = R\n        self.Raa = Raa\n        self.C = C\n        self.A = A\n        self.adj_A = adj_A\n        self.W = W\n        self.Wua = Wua\n        self.use_gaussnewton = use_gaussnewton\n\n        # incremental state\n        self.du = Vector()\n        self.A.init_vector(self.du,0)\n\n        #incremental adjoint\n        self.dp = Vector()\n        self.adj_A.init_vector(self.dp,0)\n\n        # auxiliary vectors\n        self.CT_dp = Vector()\n        self.C.init_vector(self.CT_dp, 1)\n        self.Wua_du = Vector()\n        self.Wua.init_vector(self.Wua_du, 1)\n\n    def init_vector(self, v, dim):\n        self.R.init_vector(v,dim)\n\n    # Hessian performed on x, output as generic vector y\n    def mult(self, v, y):\n        self.cgiter += 1\n        y.zero()\n        if self.use_gaussnewton:\n            self.mult_GaussNewton(v,y)\n        else:\n            self.mult_Newton(v,y)\n\n    # define (Gauss-Newton) Hessian apply H * v\n    def mult_GaussNewton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = - (self.W * self.du)\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        # Reg/Prior term\n        self.R.mult(v,y)\n\n        # Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1, self.CT_dp)\n\n    # define (Newton) Hessian apply H * v\n    def mult_Newton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = -(self.W * self.du) -  self.Wua * v\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        #Reg/Prior term\n        self.R.mult(v,y)\n        y.axpy(1., Raa*v)\n\n        #Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1., self.CT_dp)\n        self.Wua.transpmult(self.du, self.Wua_du)\n        y.axpy(1., self.Wua_du)",
            "title": "The reduced Hessian apply to a vector v:"
        },
        {
            "location": "/tutorials_v1.6.0/2_PoissonDeterministic/#the-inexact-newton-cg-optimization-with-armijo-line-search",
            "text": "We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search.  The stopping criterion is based on a relative reduction of the norm of the gradient (i.e.  \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ).  First, we compute the gradient by solving the state and adjoint equation for the current parameter  a , and then substituing the current state  u , parameter  a  and adjoint  p  variables in the weak form expression of the gradient:  (g, \\tilde{a}) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +(\\tilde{a}\\nabla u, \\nabla p).   Then, we compute the Newton direction  \\delta a  by iteratively solving  {\\bf H} {\\delta a} = - {\\bf g} .\nThe Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug  (to avoid negative curvature) criteria.  Finally, the Armijo line search uses backtracking to find  \\alpha  such that a sufficient reduction in the cost functional is achieved.\nMore specifically, we use backtracking to find  \\alpha  such that: J( a + \\alpha \\delta a ) \\leq J(a) + \\alpha c_{\\rm armijo} (\\delta a,g).    # define parameters for the optimization\ntol = 1e-8\nc = 1e-4\nmaxiter = 12\nplot_on = False\n\n# initialize iter counters\niter = 1\ntotal_cg_iter = 0\nconverged = False\n\n# initializations\ng, a_delta = Vector(), Vector()\nR.init_vector(a_delta,0)\nR.init_vector(g,0)\n\na_prev = Function(Va)\n\nprint(\"Nit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\")\n\nwhile iter <  maxiter and not converged:\n\n    # assemble matrix C\n    C =  assemble(C_equ)\n\n    # solve the adoint problem\n    adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj)\n    solve(adjoint_A, p.vector(), adjoint_RHS)\n\n    # assemble W_ua and R\n    Wua = assemble (Wua_equ)\n    Raa = assemble (Raa_equ)\n\n    # evaluate the  gradient\n    CT_p = Vector()\n    C.init_vector(CT_p,1)\n    C.transpmult(p.vector(), CT_p)\n    MG = CT_p + R * a.vector()\n    solve(M, g, MG)\n\n    # calculate the norm of the gradient\n    grad2 = g.inner(MG)\n    gradnorm = sqrt(grad2)\n\n    # set the CG tolerance (use Eisenstat\u2013Walker termination criterion)\n    if iter == 1:\n        gradnorm_ini = gradnorm\n    tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini))\n\n    # define the Hessian apply operator (with preconditioner)\n    Hess_Apply = HessianOperator(R, Raa, C, state_A, adjoint_A, W, Wua, use_gaussnewton=(iter<6) )\n    P = R + gamma * M\n    Psolver = PETScKrylovSolver(\"cg\", amg_method())\n    Psolver.set_operator(P)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(Hess_Apply)\n    solver.set_preconditioner(Psolver)\n    solver.parameters[\"rel_tolerance\"] = tolcg\n    solver.parameters[\"zero_initial_guess\"] = True\n    solver.parameters[\"print_level\"] = -1\n\n    # solve the Newton system H a_delta = - MG\n    solver.solve(a_delta, -MG)\n    total_cg_iter += Hess_Apply.cgiter\n\n    # linesearch\n    alpha = 1\n    descent = 0\n    no_backtrack = 0\n    a_prev.assign(a)\n    while descent == 0 and no_backtrack < 10:\n        a.vector().axpy(alpha, a_delta )\n\n        # solve the state/forward problem\n        state_A, state_b = assemble_system(a_state, L_state, bc_state)\n        solve(state_A, u.vector(), state_b)\n\n        # evaluate cost\n        [cost_new, misfit_new, reg_new] = cost(u, ud, a, W, R)\n\n        # check if Armijo conditions are satisfied\n        if cost_new < cost_old + alpha * c * MG.inner(a_delta):\n            cost_old = cost_new\n            descent = 1\n        else:\n            no_backtrack += 1\n            alpha *= 0.5\n            a.assign(a_prev)  # reset a\n\n    # calculate sqrt(-G * D)\n    graddir = sqrt(- MG.inner(a_delta) )\n\n    sp = \"\"\n    print(\"%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\" % \\\n        (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\\n         graddir, sp, gradnorm, sp, alpha, sp, tolcg) )\n\n    if plot_on:\n        nb.multi1_plot([a,u,p], [\"a\",\"u\",\"p\"], same_colorbar=False)\n        plt.show()\n\n    # check for convergence\n    if gradnorm < tol and iter > 1:\n        converged = True\n        print(\"Newton's method converged in \",iter,\"  iterations\")\n        print(\"Total number of CG iterations: \", total_cg_iter)\n\n    iter += 1\n\nif not converged:\n    print(\"Newton's method did not converge in \", maxiter, \" iterations\")  Nit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\n 1     1     1.12708e-05   1.12708e-05   1.33979e-11   1.56540e-02   3.79427e-04    1.00   5.000e-01\n 2     1     7.79732e-07   7.79695e-07   3.67737e-11   4.68278e-03   5.35002e-05    1.00   3.755e-01\n 3     1     3.10620e-07   3.10571e-07   4.91259e-11   9.71633e-04   7.13895e-06    1.00   1.372e-01\n 4     5     1.92183e-07   1.62405e-07   2.97780e-08   4.51694e-04   1.00276e-06    1.00   5.141e-02\n 5     1     1.86913e-07   1.57119e-07   2.97941e-08   1.02668e-04   6.12750e-07    1.00   4.019e-02\n 6    12     1.80408e-07   1.37719e-07   4.26890e-08   1.15975e-04   2.24111e-07    1.00   2.430e-02\n 7     5     1.80331e-07   1.38935e-07   4.13963e-08   1.23223e-05   4.17399e-08    1.00   1.049e-02\n 8    15     1.80330e-07   1.39056e-07   4.12734e-08   1.74451e-06   3.43216e-09    1.00   3.008e-03\nNewton's method converged in  8   iterations\nTotal number of CG iterations:  41  nb.multi1_plot([atrue, a], [\"atrue\", \"a\"])\nnb.multi1_plot([u,p], [\"u\",\"p\"], same_colorbar=False)\nplt.show()    Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "The inexact Newton-CG optimization with Armijo line search:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/",
            "text": "\\def\\data{\\bf d_\\rm{obs}}\n\\def\\vec{\\bf}\n\\def\\m{\\bf m}\n\\def\\map{\\bf m_{\\text{MAP}}}\n\\def\\postcov{\\bf \\Gamma_{\\text{post}}}\n\\def\\prcov{\\bf \\Gamma_{\\text{prior}}}\n\\def\\matrix{\\bf}\n\\def\\Hmisfit{\\bf H_{\\text{misfit}}}\n\\def\\HT{\\tilde{\\bf H}_{\\text{misfit}}}\n\\def\\diag{diag}\n\\def\\Vr{\\matrix V_r}\n\\def\\Wr{\\matrix W_r}\n\\def\\Ir{\\matrix I_r}\n\\def\\Dr{\\matrix D_r}\n\\def\\H{\\matrix H}\n\n\n\n\n\nExample: Bayesian quantification of parameter uncertainty:\n\n\nEstimating the (Gaussian) posterior pdf of the coefficient parameter field in an elliptic PDE\n\n\nIn this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields our belief (expressed as a probability) that a\nmember of this candidate set is the ``true'' parameter field that\ngave rise to the observed data.\n\n\nFor simplicity, in what follows we give finite-dimensional expressions (i.e., after\ndiscretization of the parameter space) for the Bayesian\nformulation of the inverse problem.\n\n\nBayes' Theorem:\n\n\nThe posterior probability distribution combines the prior pdf\n\n\\pi_{\\text{prior}}(\\m)\n over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf\n\n\\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m)\n, which explicitly\nrepresents the probability that a given set of parameters \n\\m\n\nmight give rise to the observed data \n\\vec{d}_{\\text{obs}} \\in\n\\mathbb{R}^m\n, namely:\n\n\n\n\n\n<script type=\"math/tex; mode=display\">\\begin{align}\n\\pi_{\\text{post}}(\\m | \\data) \\propto\n\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n\\end{align}\n\n\n\n\n\n\nNote that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.\n\n\nGaussian prior and noise:\n\n\nThe prior:\n\n\nWe consider a Gaussian prior with mean \n\\vec m_{\\text prior}\n and covariance \n\\bf \\Gamma_{\\text{prior}}\n. The covariance is given by the discretization of the inverse of differential operator \n\\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2}\n, where \n\\gamma\n, \n\\delta > 0\n control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem\n\n\nThe likelihood:\n\n\n\n\n\n\\data =  \\bf{f}(\\m) + \\bf{e }, \\;\\;\\;  \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} )\n\n\n\n\n\n\n\n\n\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right)\n\n\n\n\n\nHere \n\\bf f\n is the parameter-to-observable map that takes a parameter vector \n\\m\n and maps\nit to the space observation vector \n\\data\n.\n\n\nThe posterior:\n\n\n\n\n\n\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf  \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right)\n\n\n\n\n\nThe Gaussian approximation of the posterior: \n\\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}})\n\n\n\n\nThe mean of this posterior distribution, \n\\vec{\\map}\n, is the\nparameter vector maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,\n\n\n\n\n\n\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n\\Big( \n\\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} \n+\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \n\\Big).\n\n\n\n\n\nThe posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of \n\\mathcal{J}\n at \n\\map\n, namely\n\n\n\n\n\n\\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1}\n\n\n\n\n\nThe prior-preconditioned Hessian of the data misfit:\n\n\n\n\n\n  \\HT := \\prcov^{1/2} \\Hmisfit \\prcov^{1/2}\n\n\n\n\n\nThe generalized eigenvalue problem:\n\n\n\n\n\n \\Hmisfit \\matrix{V} = \\prcov^{-1} \\matrix{V} \\matrix{\\Lambda},\n\n\n\n\n\nwhere \n\\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n}\n\ncontains the generalized eigenvalues and the columns of \n\\matrix V\\in\n\\mathbb R^{n\\times n}\n the generalized eigenvectors such that \n\n\\matrix{V}^T \\prcov^{-1} \\matrix{V} = \\matrix{I}\n.\n\n\nRandomized eigensolvers to construct the approximate spectral decomposition:\n\n\nWhen the generalized eigenvalues \n\\{\\lambda_i\\}\n decay rapidly, we can\nextract a low-rank approximation of \n\\Hmisfit\n by retaining only the \nr\n\nlargest eigenvalues and corresponding eigenvectors,\n\n\n\n\n\n \\HT = \\prcov^{-1/2} \\matrix{V}_r \\matrix{\\Lambda}_r \\matrix{V}^T_r \\prcov^{-1/2},\n\n\n\n\n\nHere, \n\\matrix{V}_r \\in \\mathbb{R}^{n\\times r}\n contains only the \nr\n\ngeneralized eigenvectors of \n\\Hmisfit\n that correspond to the \nr\n largest eigenvalues,\nwhich are assembled into the diagonal matrix \n\\matrix{\\Lambda}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r}\n.\n\n\nThe approximate posterior covariance::\n\n\nUsing the Sherman\u2013Morrison\u2013Woodbury formula, we write\n\n\n\\begin{align}\n  \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1}\n  = \\prcov^{-1}-\\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}\n\n\n\n\n\nwhere \n\\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r}\n. The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that \nr\n is\nchosen such that \n\\lambda_r\n is small relative to 1. \n\n\nTherefore we can approximate the posterior covariance as\n\n\n\n\n\n\\postcov \\approx \\prcov - \\matrix{V}_r \\matrix{D}_r\n\\matrix{V}_r^T\n\n\n\n\n\nDrawing samples from a Gaussian distribution with covariance \n\\H^{-1}\n\n\n\n\nLet \n\\bf x\n be a sample for the prior distribution, i.e. \n\\bf x \\sim \\mathcal{N}({\\bf 0}, \\prcov)\n, then, using the low rank approximation of the posterior covariance, we compute a sample \n{\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1})\n as \n\n\n  {\\bf v} = \\big\\{ \\Vr \\big[ (\\matrix{\\Lambda}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1}  + \\bf I \\big\\} {\\bf x} \n\n\n\n\n\nThis tutorial shows:\n\n\n\n\ndescription of the inverse problem (the forward problem, the prior, and the misfit functional)\n\n\nconvergence of the inexact Newton-CG algorithm\n\n\nlow-rank-based approximation of the posterior covariance (built on a low-rank\napproximation of the Hessian of the data misfit) \n\n\nhow to construct the low-rank approximation of the Hessian of the data misfit\n\n\nhow to apply the inverse and square-root inverse Hessian to a vector efficiently\n\n\nsamples from the Gaussian approximation of the posterior\n\n\n\n\nGoals:\n\n\nBy the end of this notebook, you should be able to:\n\n\n\n\nunderstand the Bayesian inverse framework\n\n\nvisualise and understand the results\n\n\nmodify the problem and code\n\n\n\n\nMathematical tools used:\n\n\n\n\nFinite element method\n\n\nDerivation of gradiant and Hessian via the adjoint method\n\n\ninexact Newton-CG\n\n\nArmijo line search\n\n\nBayes' formula\n\n\nrandomized eigensolvers\n\n\n\n\nList of software used:\n\n\n\n\nFEniCS\n, a parallel finite element element library for the discretization of partial differential equations\n\n\nPETSc\n, for scalable and efficient linear algebra operations and solvers\n\n\nMatplotlib\n, A great python package that I used for plotting many of the results\n\n\nNumpy\n, A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook.\n\n\n\n\n1. Load modules\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)\n\n\n\n\n2. Generate the true parameter\n\n\nThis function generates a random field with a prescribed anysotropic covariance function.\n\n\ndef true_model(Vh, gamma, delta, anis_diff):\n    prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff )\n    noise = dl.Vector()\n    prior.init_vector(noise,\"noise\")\n    noise_size = get_local_size(noise)\n    noise.set_local( np.random.randn( noise_size ) )\n    atrue = dl.Vector()\n    prior.init_vector(atrue, 0)\n    prior.sample(noise,atrue)\n    return atrue\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nWe compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the \nstate\n and \nadjoint\n variable and P1 for the \nparameter\n.\n\n\nndim = 2\nnx = 64\nny = 64\nmesh = dl.UnitSquareMesh(nx, ny)\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh = [Vh2, Vh1, Vh2]\nprint(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n    Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )\n\n\n\n\nNumber of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641\n\n\n\n4. Set up the forward problem\n\n\nTo set up the forward problem we use the \nPDEVariationalProblem\n class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables \nVh\n\n- the pde in weak form \npde_varf\n\n- the boundary conditions \nbc\n for the forward problem and \nbc0\n for the adjoint and incremental problems.\n\n\nThe \nPDEVariationalProblem\n class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.\n\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS)\n\nu_bdr = dl.Expression(\"x[1]\", degree=1)\nu_bdr0 = dl.Constant(0.0)\nbc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Constant(0.0)\n\ndef pde_varf(u,a,p):\n    return dl.exp(a)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n\n\n\n4. Set up the prior\n\n\nTo obtain the synthetic true paramter \na_{\\rm true}\n we generate a realization of a Gaussian random field with zero average and covariance matrix \n\\mathcal{C} = \\widetilde{\\mathcal{A}}^{-2}\n, where \n\\widetilde{\\mathcal{A}}\n is a differential operator of the form\n\n \\widetilde{\\mathcal{A}} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I. \n\nHere \n\\Theta\n is an s.p.d. anisotropic tensor of the form\n\n \\Theta =\n\\begin{bmatrix}\n\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n\\end{bmatrix} \n\n\n\n\nFor the prior model, we assume that we can measure the log-permeability coefficient at \nN\n locations, and we denote with \na^1_{\\rm true}\n, \n\\ldots\n, \na^N_{\\rm true}\n such measures.\nWe also introduce the mollifier functions\n\n \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N,\n\nand we let\n\n \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M},\n\nwhere \np\n is a penalization costant (10 for this problem) and \n \\mathcal{M} = \\sum_{i=1}^N \\delta_i I\n.\n\n\nWe then compute \na_{\\rm pr}\n, the  mean  of  the  prior  measure,  as  a  regularized\nleast-squares fit of these point observations by solving\n\n\na_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle a, \\widetilde{\\mathcal{A}} a\\rangle + \\frac{p}{2}\\langle a_{\\rm true} - a, \\mathcal{M}(a_{\\rm true}- a) \\rangle.\n\n\n\n\n\nFinally the prior distribution is \n\\mathcal{N}(a_{\\rm pr}, \\mathcal{C}_{\\rm prior})\n, with \n\\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2}\n.\n\n\ngamma = .1\ndelta = .5\n\nanis_diff = dl.Expression(code_AnisTensor2D, degree=1)\nanis_diff.theta0 = 2.\nanis_diff.theta1 = .5\nanis_diff.alpha = math.pi/4\natrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff)\n\nlocations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]])\npen = 1e1\nprior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, atrue, anis_diff, pen)\n\nprint(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(\n    delta, gamma,2) )   \n\nobjs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [\"True Parameter\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\nmodel = Model(pde,prior, misfit)\n\n\n\n\nPrior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2\n\n\n\n\n\n5. Set up the misfit functional and generate synthetic observations\n\n\nTo setup the observation operator, we generate \nntargets\n random locations where to evaluate the value of the state.\n\n\nTo generate the synthetic observation, we first solve the forward problem using the true parameter \na_{\\rm true}\n. Synthetic observations are obtained by perturbing the state variable at the observation points with a random gaussian noise.\n\nrel_noise\n is the signal to noise ratio.\n\n\nntargets = 300\nrel_noise = 0.01\n\n\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nprint(\"Number of observation points: {0}\".format(ntargets))\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, atrue, None]\npde.solveFwd(x[STATE], x, 1e-9)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm(\"linf\")\nnoise_std_dev = rel_noise * MAX\nrandn_perturb(misfit.d, noise_std_dev)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax)\nnb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax)\nplt.show()\n\n\n\n\nNumber of observation points: 300\n\n\n\n\n\n6. Set up the model and test gradient and Hessian\n\n\nThe model is defined by three component:\n- the \nPDEVariationalProblem\n \npde\n which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n- the \nPrior\n \nprior\n which provides methods to apply the regularization (\nprecision\n) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n- the \nMisfit\n \nmisfit\n which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.\n\n\nTo test gradient and the Hessian of the model we use forward finite differences.\n\n\nmodel = Model(pde, prior, misfit)\n\na0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER])\nmodelVerify(model, a0.vector(), 1e-12)\n\n\n\n\n(yy, H xx) - (xx, H yy) =  7.57910091653e-15\n\n\n\n\n\n7. Compute the MAP point\n\n\nWe used the globalized Newtown-CG method to compute the MAP point.\n\n\na0 = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[\"rel_tolerance\"] = 1e-9\nsolver.parameters[\"abs_tolerance\"] = 1e-12\nsolver.parameters[\"max_iter\"]      = 25\nsolver.parameters[\"inner_rel_tolerance\"] = 1e-15\nsolver.parameters[\"c_armijo\"] = 1e-4\nsolver.parameters[\"GN_iter\"] = 5\n\nx = solver.solve(a0)\n\nif solver.converged:\n    print(\"\\nConverged in \", solver.it, \" iterations.\")\nelse:\n    print(\"\\nNot Converged\")\n\nprint(\"Termination reason: \", solver.termination_reasons[solver.reason])\nprint(\"Final gradient norm: \", solver.final_grad_norm)\nprint(\"Final cost: \", solver.final_cost)\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\")\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\")\nplt.show()\n\n\n\n\nIt  cg_it cost            misfit          reg             (g,da)          ||g||L2        alpha          tolcg         \n  1   1    1.206547e+03    1.206233e+03    3.147385e-01   -1.570552e+04   1.042537e+05   1.000000e+00   5.000000e-01\n  2   3    3.459350e+02    3.446814e+02    1.253641e+00   -1.846772e+03   1.431429e+04   1.000000e+00   3.705435e-01\n  3   1    2.746765e+02    2.733672e+02    1.309257e+00   -1.424615e+02   1.003736e+04   1.000000e+00   3.102873e-01\n  4   7    1.691763e+02    1.647605e+02    4.415777e+00   -2.128491e+02   3.871034e+03   1.000000e+00   1.926938e-01\n  5   6    1.573210e+02    1.522926e+02    5.028367e+00   -2.345647e+01   1.821115e+03   1.000000e+00   1.321670e-01\n  6  14    1.424926e+02    1.297495e+02    1.274310e+01   -2.988285e+01   1.157426e+03   1.000000e+00   1.053661e-01\n  7   2    1.421601e+02    1.294134e+02    1.274672e+01   -6.643211e-01   7.325602e+02   1.000000e+00   8.382545e-02\n  8  22    1.407914e+02    1.253200e+02    1.547141e+01   -2.734442e+00   4.409802e+02   1.000000e+00   6.503749e-02\n  9  14    1.407826e+02    1.253781e+02    1.540453e+01   -1.744746e-02   5.057992e+01   1.000000e+00   2.202639e-02\n 10  29    1.407817e+02    1.253307e+02    1.545107e+01   -1.819642e-03   1.502434e+01   1.000000e+00   1.200472e-02\n 11  38    1.407817e+02    1.253304e+02    1.545139e+01   -2.502663e-07   1.390539e-01   1.000000e+00   1.154904e-03\n 12  54    1.407817e+02    1.253303e+02    1.545139e+01   -1.380451e-12   2.625467e-04   1.000000e+00   5.018311e-05\n\nConverged in  12  iterations.\nTermination reason:  Norm of the gradient less than tolerance\nFinal gradient norm:  5.41022518768e-09\nFinal cost:  140.781736843\n\n\n\n\n\n8. Compute the low rank Gaussian approximation of the posterior\n\n\nWe used the \ndouble pass\n algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve\n\n\n\n\n \\Hmisfit {\\bf u} = \\lambda \\prcov^{-1} {\\bf u}. \n\n\n\n\nThe Figure shows the largest \nk\n generalized eigenvectors of the Hessian misfit.\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1).\nThe effective rank is independent of the mesh size.\n\n\nmodel.setPointForHessianEvaluations(x)\nHmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], gauss_newton_approx=False, misfit_only=True)\nk = 50\np = 20\nprint(\"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p))\nOmega = np.random.randn(get_local_size(x[PARAMETER]), k+p)\nd, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nposterior = GaussianLRPosterior(prior, d, U)\nposterior.mean = x[PARAMETER]\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15])\n\n\n\n\nSingle/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.\n\n\n\n\n\n\n\n9. Prior and posterior pointwise variance fields\n\n\ncompute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200)\n    print(\"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr))\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200)\n\nobjs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n        dl.Function(Vh[PARAMETER], post_pw_variance)]\nmytitles = [\"Prior variance\", \"Posterior variance\"]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()\n\n\n\n\nPosterior trace 1.058859e-01; Prior trace 3.949847e-01; Correction trace 2.890987e-01\n\n\n\n\n\n10. Generate samples from Prior and Posterior\n\n\nnsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\"noise\")\nnoise_size = get_local_size(noise)\ns_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\")\ns_post = dl.Function(Vh[PARAMETER], name=\"sample_post\")\n\npr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\nps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121, mytitle=\"Prior sample\",     vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post,  subplot_loc=122, mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "Subsurface Bayesian"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#example-bayesian-quantification-of-parameter-uncertainty",
            "text": "",
            "title": "Example: Bayesian quantification of parameter uncertainty:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#estimating-the-gaussian-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde",
            "text": "In this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields our belief (expressed as a probability) that a\nmember of this candidate set is the ``true'' parameter field that\ngave rise to the observed data.  For simplicity, in what follows we give finite-dimensional expressions (i.e., after\ndiscretization of the parameter space) for the Bayesian\nformulation of the inverse problem.",
            "title": "Estimating the (Gaussian) posterior pdf of the coefficient parameter field in an elliptic PDE"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#bayes-theorem",
            "text": "The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m)  over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m) , which explicitly\nrepresents the probability that a given set of parameters  \\m \nmight give rise to the observed data  \\vec{d}_{\\text{obs}} \\in\n\\mathbb{R}^m , namely:   \n<script type=\"math/tex; mode=display\">\\begin{align}\n\\pi_{\\text{post}}(\\m | \\data) \\propto\n\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n\\end{align}    Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.",
            "title": "Bayes' Theorem:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#gaussian-prior-and-noise",
            "text": "",
            "title": "Gaussian prior and noise:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#the-prior",
            "text": "We consider a Gaussian prior with mean  \\vec m_{\\text prior}  and covariance  \\bf \\Gamma_{\\text{prior}} . The covariance is given by the discretization of the inverse of differential operator  \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where  \\gamma ,  \\delta > 0  control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem",
            "title": "The prior:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#the-likelihood",
            "text": "\\data =  \\bf{f}(\\m) + \\bf{e }, \\;\\;\\;  \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} )    \n\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right)   Here  \\bf f  is the parameter-to-observable map that takes a parameter vector  \\m  and maps\nit to the space observation vector  \\data .",
            "title": "The likelihood:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#the-posterior",
            "text": "\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf  \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right)",
            "title": "The posterior:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#the-gaussian-approximation-of-the-posterior-mathcalnvecmapbf-gamma_textpost",
            "text": "The mean of this posterior distribution,  \\vec{\\map} , is the\nparameter vector maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,   \n\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n\\Big( \n\\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} \n+\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \n\\Big).   The posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of  \\mathcal{J}  at  \\map , namely   \n\\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1}",
            "title": "The Gaussian approximation of the posterior: \\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}})"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#the-prior-preconditioned-hessian-of-the-data-misfit",
            "text": "\\HT := \\prcov^{1/2} \\Hmisfit \\prcov^{1/2}",
            "title": "The prior-preconditioned Hessian of the data misfit:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#the-generalized-eigenvalue-problem",
            "text": "\\Hmisfit \\matrix{V} = \\prcov^{-1} \\matrix{V} \\matrix{\\Lambda},   where  \\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} \ncontains the generalized eigenvalues and the columns of  \\matrix V\\in\n\\mathbb R^{n\\times n}  the generalized eigenvectors such that  \\matrix{V}^T \\prcov^{-1} \\matrix{V} = \\matrix{I} .",
            "title": "The generalized eigenvalue problem:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#randomized-eigensolvers-to-construct-the-approximate-spectral-decomposition",
            "text": "When the generalized eigenvalues  \\{\\lambda_i\\}  decay rapidly, we can\nextract a low-rank approximation of  \\Hmisfit  by retaining only the  r \nlargest eigenvalues and corresponding eigenvectors,   \n \\HT = \\prcov^{-1/2} \\matrix{V}_r \\matrix{\\Lambda}_r \\matrix{V}^T_r \\prcov^{-1/2},   Here,  \\matrix{V}_r \\in \\mathbb{R}^{n\\times r}  contains only the  r \ngeneralized eigenvectors of  \\Hmisfit  that correspond to the  r  largest eigenvalues,\nwhich are assembled into the diagonal matrix  \\matrix{\\Lambda}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r} .",
            "title": "Randomized eigensolvers to construct the approximate spectral decomposition:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#the-approximate-posterior-covariance",
            "text": "Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \n\\begin{align}\n  \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1}\n  = \\prcov^{-1}-\\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}   where  \\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r} . The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that  r  is\nchosen such that  \\lambda_r  is small relative to 1.   Therefore we can approximate the posterior covariance as   \n\\postcov \\approx \\prcov - \\matrix{V}_r \\matrix{D}_r\n\\matrix{V}_r^T",
            "title": "The approximate posterior covariance::"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#drawing-samples-from-a-gaussian-distribution-with-covariance-h-1",
            "text": "Let  \\bf x  be a sample for the prior distribution, i.e.  \\bf x \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample  {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1})  as  \n  {\\bf v} = \\big\\{ \\Vr \\big[ (\\matrix{\\Lambda}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1}  + \\bf I \\big\\} {\\bf x}",
            "title": "Drawing samples from a Gaussian distribution with covariance \\H^{-1}"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#this-tutorial-shows",
            "text": "description of the inverse problem (the forward problem, the prior, and the misfit functional)  convergence of the inexact Newton-CG algorithm  low-rank-based approximation of the posterior covariance (built on a low-rank\napproximation of the Hessian of the data misfit)   how to construct the low-rank approximation of the Hessian of the data misfit  how to apply the inverse and square-root inverse Hessian to a vector efficiently  samples from the Gaussian approximation of the posterior",
            "title": "This tutorial shows:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#goals",
            "text": "By the end of this notebook, you should be able to:   understand the Bayesian inverse framework  visualise and understand the results  modify the problem and code",
            "title": "Goals:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#mathematical-tools-used",
            "text": "Finite element method  Derivation of gradiant and Hessian via the adjoint method  inexact Newton-CG  Armijo line search  Bayes' formula  randomized eigensolvers",
            "title": "Mathematical tools used:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#list-of-software-used",
            "text": "FEniCS , a parallel finite element element library for the discretization of partial differential equations  PETSc , for scalable and efficient linear algebra operations and solvers  Matplotlib , A great python package that I used for plotting many of the results  Numpy , A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook.",
            "title": "List of software used:"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#1-load-modules",
            "text": "from __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#2-generate-the-true-parameter",
            "text": "This function generates a random field with a prescribed anysotropic covariance function.  def true_model(Vh, gamma, delta, anis_diff):\n    prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff )\n    noise = dl.Vector()\n    prior.init_vector(noise,\"noise\")\n    noise_size = get_local_size(noise)\n    noise.set_local( np.random.randn( noise_size ) )\n    atrue = dl.Vector()\n    prior.init_vector(atrue, 0)\n    prior.sample(noise,atrue)\n    return atrue",
            "title": "2. Generate the true parameter"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#3-set-up-the-mesh-and-finite-element-spaces",
            "text": "We compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the  state  and  adjoint  variable and P1 for the  parameter .  ndim = 2\nnx = 64\nny = 64\nmesh = dl.UnitSquareMesh(nx, ny)\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh = [Vh2, Vh1, Vh2]\nprint(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n    Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )  Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641",
            "title": "3. Set up the mesh and finite element spaces"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#4-set-up-the-forward-problem",
            "text": "To set up the forward problem we use the  PDEVariationalProblem  class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables  Vh \n- the pde in weak form  pde_varf \n- the boundary conditions  bc  for the forward problem and  bc0  for the adjoint and incremental problems.  The  PDEVariationalProblem  class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.  def u_boundary(x, on_boundary):\n    return on_boundary and ( x[1] < dl.DOLFIN_EPS or x[1] > 1.0 - dl.DOLFIN_EPS)\n\nu_bdr = dl.Expression(\"x[1]\", degree=1)\nu_bdr0 = dl.Constant(0.0)\nbc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Constant(0.0)\n\ndef pde_varf(u,a,p):\n    return dl.exp(a)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)",
            "title": "4. Set up the forward problem"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#4-set-up-the-prior",
            "text": "To obtain the synthetic true paramter  a_{\\rm true}  we generate a realization of a Gaussian random field with zero average and covariance matrix  \\mathcal{C} = \\widetilde{\\mathcal{A}}^{-2} , where  \\widetilde{\\mathcal{A}}  is a differential operator of the form  \\widetilde{\\mathcal{A}} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I.  \nHere  \\Theta  is an s.p.d. anisotropic tensor of the form  \\Theta =\n\\begin{bmatrix}\n\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n\\end{bmatrix}    For the prior model, we assume that we can measure the log-permeability coefficient at  N  locations, and we denote with  a^1_{\\rm true} ,  \\ldots ,  a^N_{\\rm true}  such measures.\nWe also introduce the mollifier functions  \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N, \nand we let  \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M}, \nwhere  p  is a penalization costant (10 for this problem) and   \\mathcal{M} = \\sum_{i=1}^N \\delta_i I .  We then compute  a_{\\rm pr} , the  mean  of  the  prior  measure,  as  a  regularized\nleast-squares fit of these point observations by solving \na_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle a, \\widetilde{\\mathcal{A}} a\\rangle + \\frac{p}{2}\\langle a_{\\rm true} - a, \\mathcal{M}(a_{\\rm true}- a) \\rangle.   Finally the prior distribution is  \\mathcal{N}(a_{\\rm pr}, \\mathcal{C}_{\\rm prior}) , with  \\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2} .  gamma = .1\ndelta = .5\n\nanis_diff = dl.Expression(code_AnisTensor2D, degree=1)\nanis_diff.theta0 = 2.\nanis_diff.theta1 = .5\nanis_diff.alpha = math.pi/4\natrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff)\n\nlocations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]])\npen = 1e1\nprior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, atrue, anis_diff, pen)\n\nprint(\"Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\".format(\n    delta, gamma,2) )   \n\nobjs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [\"True Parameter\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\nmodel = Model(pde,prior, misfit)  Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2",
            "title": "4. Set up the prior"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#5-set-up-the-misfit-functional-and-generate-synthetic-observations",
            "text": "To setup the observation operator, we generate  ntargets  random locations where to evaluate the value of the state.  To generate the synthetic observation, we first solve the forward problem using the true parameter  a_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random gaussian noise. rel_noise  is the signal to noise ratio.  ntargets = 300\nrel_noise = 0.01\n\n\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nprint(\"Number of observation points: {0}\".format(ntargets))\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, atrue, None]\npde.solveFwd(x[STATE], x, 1e-9)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm(\"linf\")\nnoise_std_dev = rel_noise * MAX\nrandn_perturb(misfit.d, noise_std_dev)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True State\", subplot_loc=121, vmin=vmin, vmax=vmax)\nnb.plot_pts(targets, misfit.d, mytitle=\"Observations\", subplot_loc=122, vmin=vmin, vmax=vmax)\nplt.show()  Number of observation points: 300",
            "title": "5. Set up the misfit functional and generate synthetic observations"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#6-set-up-the-model-and-test-gradient-and-hessian",
            "text": "The model is defined by three component:\n- the  PDEVariationalProblem   pde  which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n- the  Prior   prior  which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n- the  Misfit   misfit  which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.  To test gradient and the Hessian of the model we use forward finite differences.  model = Model(pde, prior, misfit)\n\na0 = dl.interpolate(dl.Expression(\"sin(x[0])\", degree=5), Vh[PARAMETER])\nmodelVerify(model, a0.vector(), 1e-12)  (yy, H xx) - (xx, H yy) =  7.57910091653e-15",
            "title": "6. Set up the model and test gradient and Hessian"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#7-compute-the-map-point",
            "text": "We used the globalized Newtown-CG method to compute the MAP point.  a0 = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[\"rel_tolerance\"] = 1e-9\nsolver.parameters[\"abs_tolerance\"] = 1e-12\nsolver.parameters[\"max_iter\"]      = 25\nsolver.parameters[\"inner_rel_tolerance\"] = 1e-15\nsolver.parameters[\"c_armijo\"] = 1e-4\nsolver.parameters[\"GN_iter\"] = 5\n\nx = solver.solve(a0)\n\nif solver.converged:\n    print(\"\\nConverged in \", solver.it, \" iterations.\")\nelse:\n    print(\"\\nNot Converged\")\n\nprint(\"Termination reason: \", solver.termination_reasons[solver.reason])\nprint(\"Final gradient norm: \", solver.final_grad_norm)\nprint(\"Final cost: \", solver.final_cost)\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\"State\")\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\"Parameter\")\nplt.show()  It  cg_it cost            misfit          reg             (g,da)          ||g||L2        alpha          tolcg         \n  1   1    1.206547e+03    1.206233e+03    3.147385e-01   -1.570552e+04   1.042537e+05   1.000000e+00   5.000000e-01\n  2   3    3.459350e+02    3.446814e+02    1.253641e+00   -1.846772e+03   1.431429e+04   1.000000e+00   3.705435e-01\n  3   1    2.746765e+02    2.733672e+02    1.309257e+00   -1.424615e+02   1.003736e+04   1.000000e+00   3.102873e-01\n  4   7    1.691763e+02    1.647605e+02    4.415777e+00   -2.128491e+02   3.871034e+03   1.000000e+00   1.926938e-01\n  5   6    1.573210e+02    1.522926e+02    5.028367e+00   -2.345647e+01   1.821115e+03   1.000000e+00   1.321670e-01\n  6  14    1.424926e+02    1.297495e+02    1.274310e+01   -2.988285e+01   1.157426e+03   1.000000e+00   1.053661e-01\n  7   2    1.421601e+02    1.294134e+02    1.274672e+01   -6.643211e-01   7.325602e+02   1.000000e+00   8.382545e-02\n  8  22    1.407914e+02    1.253200e+02    1.547141e+01   -2.734442e+00   4.409802e+02   1.000000e+00   6.503749e-02\n  9  14    1.407826e+02    1.253781e+02    1.540453e+01   -1.744746e-02   5.057992e+01   1.000000e+00   2.202639e-02\n 10  29    1.407817e+02    1.253307e+02    1.545107e+01   -1.819642e-03   1.502434e+01   1.000000e+00   1.200472e-02\n 11  38    1.407817e+02    1.253304e+02    1.545139e+01   -2.502663e-07   1.390539e-01   1.000000e+00   1.154904e-03\n 12  54    1.407817e+02    1.253303e+02    1.545139e+01   -1.380451e-12   2.625467e-04   1.000000e+00   5.018311e-05\n\nConverged in  12  iterations.\nTermination reason:  Norm of the gradient less than tolerance\nFinal gradient norm:  5.41022518768e-09\nFinal cost:  140.781736843",
            "title": "7. Compute the MAP point"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#8-compute-the-low-rank-gaussian-approximation-of-the-posterior",
            "text": "We used the  double pass  algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve    \\Hmisfit {\\bf u} = \\lambda \\prcov^{-1} {\\bf u}.    The Figure shows the largest  k  generalized eigenvectors of the Hessian misfit.\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1).\nThe effective rank is independent of the mesh size.  model.setPointForHessianEvaluations(x)\nHmisfit = ReducedHessian(model, solver.parameters[\"inner_rel_tolerance\"], gauss_newton_approx=False, misfit_only=True)\nk = 50\np = 20\nprint(\"Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p))\nOmega = np.random.randn(get_local_size(x[PARAMETER]), k+p)\nd, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nposterior = GaussianLRPosterior(prior, d, U)\nposterior.mean = x[PARAMETER]\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,15])  Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.",
            "title": "8. Compute the low rank Gaussian approximation of the posterior"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#9-prior-and-posterior-pointwise-variance-fields",
            "text": "compute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200)\n    print(\"Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\".format(post_tr, prior_tr, corr_tr))\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=200)\n\nobjs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n        dl.Function(Vh[PARAMETER], post_pw_variance)]\nmytitles = [\"Prior variance\", \"Posterior variance\"]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()  Posterior trace 1.058859e-01; Prior trace 3.949847e-01; Correction trace 2.890987e-01",
            "title": "9. Prior and posterior pointwise variance fields"
        },
        {
            "location": "/tutorials_v1.6.0/3_SubsurfaceBayesian/#10-generate-samples-from-prior-and-posterior",
            "text": "nsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\"noise\")\nnoise_size = get_local_size(noise)\ns_prior = dl.Function(Vh[PARAMETER], name=\"sample_prior\")\ns_post = dl.Function(Vh[PARAMETER], name=\"sample_post\")\n\npr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\nps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121, mytitle=\"Prior sample\",     vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post,  subplot_loc=122, mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n    plt.show()       Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "10. Generate samples from Prior and Posterior"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/",
            "text": "\\def\\D{\\mathcal{D}}\n\\def\\ipar{m}\n\\def\\R{\\mathbb{R}}\n\\def\\del{\\partial}\n\\def\\vec{\\bf}\n\\def\\priorm{\\mu_0}\n\\def\\C{\\mathcal{C}}\n\\def\\Acal{\\mathcal{A}}\n\\def\\postm{\\mu_{\\rm{post}}}\n\\def\\iparpost{\\ipar_\\text{post}}\n\\def\\obs{\\vec{d}} \n\\def\\yobs{\\obs^{\\text{obs}}}\n\\def\\obsop{\\mathcal{B}}\n\\def\\dd{\\vec{\\bar{d}}}\n\\def\\iFF{\\mathcal{F}}\n\\def\\iFFadj{\\mathcal{F}^*}\n\\def\\ncov{\\Gamma_{\\mathrm{noise}}}\n\n\n\n\n\nExample: Bayesian initial condition inversion in an advection-diffusion problem\n\n\nIn this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.\n\n\nThe Bayesian inverse problem:\n\n\nFollowing the Bayesian framework, we utilize \na Gaussian prior measure \n\\priorm = \\mathcal{N}(\\ipar_0,\\C_0)\n,\nwith \n\\C_0=\\Acal^{-2}\n where \n\\Acal\n is an elliptic differential operator as \ndescribed in the PoissonBayesian example, and use an additive\nGaussian noise model. Therefore, the solution of the Bayesian inverse\nproblem is the posterior measure, \n\\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post})\n with\n\n\\iparpost\n and \n\\C_\\text{post}\n.\n\n\n\n\nThe posterior mean \n\\iparpost\n is characterized as the minimizer of\n\n\n\n\n\n\n\n\\begin{aligned}\n& \\mathcal{J}(\\ipar) :=\n  \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs  \\right\\|^2_{\\ncov^{-1}}\n  + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)},\n\\end{aligned}\n\n\n\n\n\nwhich can also be interpreted as the regularized functional to be\nminimized in deterministic inversion. The observation operator \n\\mathcal{B}\n extracts the values of the forward solution \nu\n on a set of\nlocations \n\\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D\n at\ntimes \n\\{t_1, \\ldots, t_N\\} \\subset [0, T]\n.\n\n\n\n\nThe posterior covariance \n\\C_{\\text{post}}\n is the inverse of the Hessian of \n\\mathcal{J}(\\ipar)\n, i.e.,\n\n\n\n\n\n\n\n\\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.\n\n\n\n\n\nThe forward problem:\n\n\nThe PDE in the parameter-to-observable map \n\\iFF\n models diffusive transport\nin a domain \n\\D \\subset \\R^d\n (\nd \\in \\{2, 3\\}\n):\n\n\n\n\n\n\\begin{split}\nu_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0     & \\quad \\text{in } \\D\\times(0,T),\\\\\n                                 u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\\n                \\kappa \\nabla u\\cdot \\vec{n} &= 0     & \\quad \\text{on } \\partial\\D \\times (0,T).\n\\end{split}\n\n\n\n\n\nHere, \n\\kappa > 0\n is the diffusion coefficient and \nT > 0\n is the final\ntime. The velocity field\n\n\\vec{v}\n is computed by solving the following steady-state\nNavier-Stokes equation with the side walls driving the flow:\n\n\n\n\n\n\\begin{aligned}\n- \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\\n\\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\\n\\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D.\n\\end{aligned}\n\n\n\n\n\nHere, \nq\n is pressure, \n\\text{Re}\n is the Reynolds number. The Dirichlet boundary data\n\n\\vec{g} \\in \\R^d\n is given by \n\n\\vec{g} = \\vec{e}_2\n on the left wall of the domain, \n\n\\vec{g}=-\\vec{e}_2\n on the right wall,  and \n\\vec{g} = \\vec{0}\n everywhere else.\n\n\nThe adjoint problem:\n\n\n\n\n\n\\begin{aligned}\n-p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p  &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\\n                                      p(\\cdot, T) &= 0             &       &\\text{ in } \\D,\\\\ \n(\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n}            &= 0             &       &\\text{ on } \\partial\\D\\times (0,T).\n\\end{aligned}\n\n\n\n\n\n1. Load modules\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" )\nfrom model_ad_diff import TimeDependentAD\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(1)\n\n\n\n\n2. Construct the velocity field\n\n\ndef v_boundary(x,on_boundary):\n    return on_boundary\n\ndef q_boundary(x,on_boundary):\n    return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS\n\ndef computeVelocityField(mesh):\n    Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2)\n    Wh = dl.FunctionSpace(mesh, 'Lagrange', 1)\n    if dlversion() <= (1,6,0):\n        XW = dl.MixedFunctionSpace([Xh, Wh])\n    else:\n        mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()])\n        XW = dl.FunctionSpace(mesh, mixed_element)\n\n    Re = 1e2\n\n    g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1)\n    bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary)\n    bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise')\n    bcs = [bc1, bc2]\n\n    vq = dl.Function(XW)\n    (v,q) = dl.split(vq)\n    (v_test, q_test) = dl.TestFunctions (XW)\n\n    def strain(v):\n        return dl.sym(dl.nabla_grad(v))\n\n    F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test)\n           - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx\n\n    dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\":\n                                         {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}})\n\n    plt.figure(figsize=(15,5))\n    vh = dl.project(v,Xh)\n    qh = dl.project(q,Wh)\n    nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\")\n    nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\")\n    plt.show()\n\n    return v\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nmesh = dl.refine( dl.Mesh(\"ad_20.xml\") )\nwind_velocity = computeVelocityField(mesh)\nVh = dl.FunctionSpace(mesh, \"Lagrange\", 1)\nprint(\"Number of dofs: {0}\".format( Vh.dim() ) )\n\n\n\n\n\n\nNumber of dofs: 2023\n\n\n\n4. Set up model (prior, true/proposed initial condition)\n\n\n#gamma = 1\n#delta = 10\n#prior = LaplacianPrior(Vh, gamma, delta)\n\ngamma = 1\ndelta = 8\nprior = BiLaplacianPrior(Vh, gamma, delta)\n\nprior.mean = dl.interpolate(dl.Constant(0.5), Vh).vector()\ntrue_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))', degree=5), Vh).vector()\nproblem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior)\n\nobjs = [dl.Function(Vh,true_initial_condition),\n        dl.Function(Vh,prior.mean)]\nmytitles = [\"True Initial Condition\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\n\n\n\n\n\n5. Generate the synthetic observations\n\n\nrel_noise = 0.001\nutrue = problem.generate_vector(STATE)\nx = [utrue, true_initial_condition, None]\nproblem.solveFwd(x[STATE], x, 1e-9)\nMAX = utrue.norm(\"linf\", \"linf\")\nnoise_std_dev = rel_noise * MAX\nproblem.ud.copy(utrue)\nproblem.ud.randn_perturb(noise_std_dev)\nproblem.noise_variance = noise_std_dev*noise_std_dev\n\nnb.show_solution(Vh, true_initial_condition, utrue, \"Solution\")\n\n\n\n\n\n\n6. Test the gradient and the Hessian of the cost (negative log posterior)\n\n\na0 = true_initial_condition.copy()\nmodelVerify(problem, a0, 1e-12, is_quadratic=True)\n\n\n\n\n(yy, H xx) - (xx, H yy) =  -4.75341826113e-14\n\n\n\n\n\n7. Evaluate the gradient\n\n\n[u,a,p] = problem.generate_vector()\nproblem.solveFwd(u, [u,a,p], 1e-12)\nproblem.solveAdj(p, [u,a,p], 1e-12)\nmg = problem.generate_vector(PARAMETER)\ngrad_norm = problem.evalGradientParameter([u,a,p], mg)\n\nprint(\"(g,g) = \", grad_norm)\n\n\n\n\n(g,g) =  1.66716039169e+12\n\n\n\n8. The Gaussian approximation of the posterior\n\n\nH = ReducedHessian(problem, 1e-12, gauss_newton_approx=False, misfit_only=True) \n\nk = 80\np = 20\nprint(\"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p))\nOmega = np.random.randn(get_local_size(a), k+p)\nd, U = singlePassG(H, prior.R, prior.Rsolver, Omega, k)\n\n\nposterior = GaussianLRPosterior( prior, d, U )\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh, U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60])\n\n\n\n\nSingle Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.\n\n\n\n\n\n\n\n9. Compute the MAP point\n\n\nH.misfit_only = False\n\nsolver = CGSolverSteihaug()\nsolver.set_operator(H)\nsolver.set_preconditioner( posterior.Hlr )\nsolver.parameters[\"print_level\"] = 1\nsolver.parameters[\"rel_tolerance\"] = 1e-6\nsolver.solve(a, -mg)\nproblem.solveFwd(u, [u,a,p], 1e-12)\n\ntotal_cost, reg_cost, misfit_cost = problem.cost([u,a,p])\nprint(\"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost))\n\nposterior.mean = a\n\nplt.figure(figsize=(7.5,5))\nnb.plot(dl.Function(Vh, a), mytitle=\"Initial Condition\")\nplt.show()\n\nnb.show_solution(Vh, a, u, \"Solution\")\n\n\n\n\n Iterartion :  0  (B r, r) =  30140.7469425\n Iteration :  1  (B r, r) =  0.0653733954192\n Iteration :  2  (B r, r) =  6.28002367536e-06\n Iteration :  3  (B r, r) =  9.57007003125e-10\nRelative/Absolute residual less than tol\nConverged in  3  iterations with final norm  3.09355297858e-05\nTotal cost 84.2612; Reg Cost 68.8823; Misfit 15.3789\n\n\n\n\n\n\n\n10. Prior and posterior pointwise variance fields\n\n\ncompute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200)\n    print(\"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr))\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300)\n\nobjs = [dl.Function(Vh, pr_pw_variance),\n        dl.Function(Vh, post_pw_variance)]\nmytitles = [\"Prior Variance\", \"Posterior Variance\"]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()\n\n\n\n\nPosterior trace 0.000602854; Prior trace 0.0285673; Correction trace 0.0279644\n\n\n\n\n\n11. Draw samples from the prior and posterior distributions\n\n\nnsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\"noise\")\nnoise_size = get_local_size(noise)\ns_prior = dl.Function(Vh, name=\"sample_prior\")\ns_post = dl.Function(Vh, name=\"sample_post\")\n\npr_max =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min()\nps_max =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "Advection-Diffusion Bayesian"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#example-bayesian-initial-condition-inversion-in-an-advection-diffusion-problem",
            "text": "In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.",
            "title": "Example: Bayesian initial condition inversion in an advection-diffusion problem"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#the-bayesian-inverse-problem",
            "text": "Following the Bayesian framework, we utilize \na Gaussian prior measure  \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) ,\nwith  \\C_0=\\Acal^{-2}  where  \\Acal  is an elliptic differential operator as \ndescribed in the PoissonBayesian example, and use an additive\nGaussian noise model. Therefore, the solution of the Bayesian inverse\nproblem is the posterior measure,  \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post})  with \\iparpost  and  \\C_\\text{post} .   The posterior mean  \\iparpost  is characterized as the minimizer of    \n\\begin{aligned}\n& \\mathcal{J}(\\ipar) :=\n  \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs  \\right\\|^2_{\\ncov^{-1}}\n  + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)},\n\\end{aligned}   which can also be interpreted as the regularized functional to be\nminimized in deterministic inversion. The observation operator  \\mathcal{B}  extracts the values of the forward solution  u  on a set of\nlocations  \\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D  at\ntimes  \\{t_1, \\ldots, t_N\\} \\subset [0, T] .   The posterior covariance  \\C_{\\text{post}}  is the inverse of the Hessian of  \\mathcal{J}(\\ipar) , i.e.,    \n\\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.",
            "title": "The Bayesian inverse problem:"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#the-forward-problem",
            "text": "The PDE in the parameter-to-observable map  \\iFF  models diffusive transport\nin a domain  \\D \\subset \\R^d  ( d \\in \\{2, 3\\} ):   \n\\begin{split}\nu_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0     & \\quad \\text{in } \\D\\times(0,T),\\\\\n                                 u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\\n                \\kappa \\nabla u\\cdot \\vec{n} &= 0     & \\quad \\text{on } \\partial\\D \\times (0,T).\n\\end{split}   Here,  \\kappa > 0  is the diffusion coefficient and  T > 0  is the final\ntime. The velocity field \\vec{v}  is computed by solving the following steady-state\nNavier-Stokes equation with the side walls driving the flow:   \n\\begin{aligned}\n- \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\\n\\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\\n\\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D.\n\\end{aligned}   Here,  q  is pressure,  \\text{Re}  is the Reynolds number. The Dirichlet boundary data \\vec{g} \\in \\R^d  is given by  \\vec{g} = \\vec{e}_2  on the left wall of the domain,  \\vec{g}=-\\vec{e}_2  on the right wall,  and  \\vec{g} = \\vec{0}  everywhere else.",
            "title": "The forward problem:"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#the-adjoint-problem",
            "text": "\\begin{aligned}\n-p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p  &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\\n                                      p(\\cdot, T) &= 0             &       &\\text{ in } \\D,\\\\ \n(\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n}            &= 0             &       &\\text{ on } \\partial\\D\\times (0,T).\n\\end{aligned}",
            "title": "The adjoint problem:"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#1-load-modules",
            "text": "from __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"..\") + \"/applications/ad_diff/\" )\nfrom model_ad_diff import TimeDependentAD\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(1)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#2-construct-the-velocity-field",
            "text": "def v_boundary(x,on_boundary):\n    return on_boundary\n\ndef q_boundary(x,on_boundary):\n    return x[0] < dl.DOLFIN_EPS and x[1] < dl.DOLFIN_EPS\n\ndef computeVelocityField(mesh):\n    Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2)\n    Wh = dl.FunctionSpace(mesh, 'Lagrange', 1)\n    if dlversion() <= (1,6,0):\n        XW = dl.MixedFunctionSpace([Xh, Wh])\n    else:\n        mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()])\n        XW = dl.FunctionSpace(mesh, mixed_element)\n\n    Re = 1e2\n\n    g = dl.Expression(('0.0','(x[0] < 1e-14) - (x[0] > 1 - 1e-14)'), degree=1)\n    bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary)\n    bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise')\n    bcs = [bc1, bc2]\n\n    vq = dl.Function(XW)\n    (v,q) = dl.split(vq)\n    (v_test, q_test) = dl.TestFunctions (XW)\n\n    def strain(v):\n        return dl.sym(dl.nabla_grad(v))\n\n    F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test)\n           - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx\n\n    dl.solve(F == 0, vq, bcs, solver_parameters={\"newton_solver\":\n                                         {\"relative_tolerance\":1e-4, \"maximum_iterations\":100}})\n\n    plt.figure(figsize=(15,5))\n    vh = dl.project(v,Xh)\n    qh = dl.project(q,Wh)\n    nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\"Velocity\")\n    nb.plot(qh, subplot_loc=122,mytitle=\"Pressure\")\n    plt.show()\n\n    return v",
            "title": "2. Construct the velocity field"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#3-set-up-the-mesh-and-finite-element-spaces",
            "text": "mesh = dl.refine( dl.Mesh(\"ad_20.xml\") )\nwind_velocity = computeVelocityField(mesh)\nVh = dl.FunctionSpace(mesh, \"Lagrange\", 1)\nprint(\"Number of dofs: {0}\".format( Vh.dim() ) )   Number of dofs: 2023",
            "title": "3. Set up the mesh and finite element spaces"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#4-set-up-model-prior-trueproposed-initial-condition",
            "text": "#gamma = 1\n#delta = 10\n#prior = LaplacianPrior(Vh, gamma, delta)\n\ngamma = 1\ndelta = 8\nprior = BiLaplacianPrior(Vh, gamma, delta)\n\nprior.mean = dl.interpolate(dl.Constant(0.5), Vh).vector()\ntrue_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))', degree=5), Vh).vector()\nproblem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior)\n\nobjs = [dl.Function(Vh,true_initial_condition),\n        dl.Function(Vh,prior.mean)]\nmytitles = [\"True Initial Condition\", \"Prior mean\"]\nnb.multi1_plot(objs, mytitles)\nplt.show()",
            "title": "4. Set up model (prior, true/proposed initial condition)"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#5-generate-the-synthetic-observations",
            "text": "rel_noise = 0.001\nutrue = problem.generate_vector(STATE)\nx = [utrue, true_initial_condition, None]\nproblem.solveFwd(x[STATE], x, 1e-9)\nMAX = utrue.norm(\"linf\", \"linf\")\nnoise_std_dev = rel_noise * MAX\nproblem.ud.copy(utrue)\nproblem.ud.randn_perturb(noise_std_dev)\nproblem.noise_variance = noise_std_dev*noise_std_dev\n\nnb.show_solution(Vh, true_initial_condition, utrue, \"Solution\")",
            "title": "5. Generate the synthetic observations"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#6-test-the-gradient-and-the-hessian-of-the-cost-negative-log-posterior",
            "text": "a0 = true_initial_condition.copy()\nmodelVerify(problem, a0, 1e-12, is_quadratic=True)  (yy, H xx) - (xx, H yy) =  -4.75341826113e-14",
            "title": "6. Test the gradient and the Hessian of the cost (negative log posterior)"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#7-evaluate-the-gradient",
            "text": "[u,a,p] = problem.generate_vector()\nproblem.solveFwd(u, [u,a,p], 1e-12)\nproblem.solveAdj(p, [u,a,p], 1e-12)\nmg = problem.generate_vector(PARAMETER)\ngrad_norm = problem.evalGradientParameter([u,a,p], mg)\n\nprint(\"(g,g) = \", grad_norm)  (g,g) =  1.66716039169e+12",
            "title": "7. Evaluate the gradient"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#8-the-gaussian-approximation-of-the-posterior",
            "text": "H = ReducedHessian(problem, 1e-12, gauss_newton_approx=False, misfit_only=True) \n\nk = 80\np = 20\nprint(\"Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k,p))\nOmega = np.random.randn(get_local_size(a), k+p)\nd, U = singlePassG(H, prior.R, prior.Rsolver, Omega, k)\n\n\nposterior = GaussianLRPosterior( prior, d, U )\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh, U, mytitle=\"Eigenvector\", which=[0,1,2,5,10,20,30,45,60])  Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.",
            "title": "8. The Gaussian approximation of the posterior"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#9-compute-the-map-point",
            "text": "H.misfit_only = False\n\nsolver = CGSolverSteihaug()\nsolver.set_operator(H)\nsolver.set_preconditioner( posterior.Hlr )\nsolver.parameters[\"print_level\"] = 1\nsolver.parameters[\"rel_tolerance\"] = 1e-6\nsolver.solve(a, -mg)\nproblem.solveFwd(u, [u,a,p], 1e-12)\n\ntotal_cost, reg_cost, misfit_cost = problem.cost([u,a,p])\nprint(\"Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\".format(total_cost, reg_cost, misfit_cost))\n\nposterior.mean = a\n\nplt.figure(figsize=(7.5,5))\nnb.plot(dl.Function(Vh, a), mytitle=\"Initial Condition\")\nplt.show()\n\nnb.show_solution(Vh, a, u, \"Solution\")   Iterartion :  0  (B r, r) =  30140.7469425\n Iteration :  1  (B r, r) =  0.0653733954192\n Iteration :  2  (B r, r) =  6.28002367536e-06\n Iteration :  3  (B r, r) =  9.57007003125e-10\nRelative/Absolute residual less than tol\nConverged in  3  iterations with final norm  3.09355297858e-05\nTotal cost 84.2612; Reg Cost 68.8823; Misfit 15.3789",
            "title": "9. Compute the MAP point"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#10-prior-and-posterior-pointwise-variance-fields",
            "text": "compute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\"Randomized\", r=200)\n    print(\"Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\".format(post_tr, prior_tr, corr_tr))\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(method=\"Randomized\", r=300)\n\nobjs = [dl.Function(Vh, pr_pw_variance),\n        dl.Function(Vh, post_pw_variance)]\nmytitles = [\"Prior Variance\", \"Posterior Variance\"]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()  Posterior trace 0.000602854; Prior trace 0.0285673; Correction trace 0.0279644",
            "title": "10. Prior and posterior pointwise variance fields"
        },
        {
            "location": "/tutorials_v1.6.0/4_AdvectionDiffusionBayesian/#11-draw-samples-from-the-prior-and-posterior-distributions",
            "text": "nsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\"noise\")\nnoise_size = get_local_size(noise)\ns_prior = dl.Function(Vh, name=\"sample_prior\")\ns_post = dl.Function(Vh, name=\"sample_post\")\n\npr_max =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min()\nps_max =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\"Prior sample\", vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle=\"Posterior sample\", vmin=ps_min, vmax=ps_max)\n    plt.show()       Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "11. Draw samples from the prior and posterior distributions"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/",
            "text": "Spectrum of the Reduced Hessian\n\n\nThe linear source inversion problem\n\n\nWe consider the following linear source inversion problem.\nFind the state \nu \\in H^1_{\\Gamma_D}(\\Omega)\n and the source (\nparameter\n) \na \\in H^1(\\Omega)\n that solves\n\n\\begin{align*}\n{} & \\min_a \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|a-a_0|^2 + \\gamma|\\nabla (a - a_0)|^2 \\right] dx & {}\\\\\n{\\rm s.t.} & {} &{} \\\\\n{} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = a & {\\rm in} \\; \\Omega\\\\\n{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n\\end{align*}\n\n\n\n\nHere:\n\n\n\n\n\n\n\n\nu_d\n is a \nn_{\\rm obs}\n finite dimensional vector that denotes noisy observations of the state \nu\n in \nn_{\\rm obs}\n locations \n\\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}}\n. More specifically, \nu_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i\n, where \n\\eta_i\n are i.i.d. \n\\mathcal{N}(0, \\sigma^2)\n.\n\n\n\n\n\n\n\n\nB: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}}\n is the linear operator that evaluates the state \nu\n at the observation locations \n\\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}}\n.\n\n\n\n\n\n\n\n\n\\delta\n and \n\\gamma\n are the parameters of the regularization penalizing the \nL^2(\\Omega)\n and \nH^1(\\Omega)\n norm of \na-a_0\n, respectively.\n\n\n\n\n\n\n\n\nk\n, \n{\\bf v}\n, \nc\n are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively.\n\n\n\n\n\n\n\n\n\\Gamma_D \\subset \\partial \\Omega\n, \n\\Gamma_N \\subset \\partial \\Omega\n represents the subdomain of \n\\partial\\Omega\n where we impose Dirichlet or Neumann boundary conditions, respectively.\n\n\n\n\n\n\n1. Load modules\n\n\nfrom __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\n\n\n\n2. The linear source inversion problem\n\n\ndef pde_varf(u,a,p):\n    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n           + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\\n           + c*u*p*dl.dx \\\n           - a*p*dl.dx\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and x[1] < dl.DOLFIN_EPS\n\ndef solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True):\n    np.random.seed(seed=2)\n    mesh = dl.UnitSquareMesh(nx, ny)\n    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n\n    Vh = [Vh1, Vh1, Vh1]\n    if verbose:\n        print(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n            Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )\n\n\n    u_bdr = dl.Constant(0.0)\n    u_bdr0 = dl.Constant(0.0)\n    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n    bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\n    atrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector()\n    a0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector()\n\n    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n    if verbose:\n        print(\"Number of observation points: {0}\".format(targets.shape[0]))\n\n    misfit = PointwiseStateObservation(Vh[STATE], targets)\n\n    reg = LaplacianPrior(Vh[PARAMETER], gamma, delta)\n\n    #Generate synthetic observations\n    utrue = pde.generate_state()\n    x = [utrue, atrue, None]\n    pde.solveFwd(x[STATE], x, 1e-9)\n    misfit.B.mult(x[STATE], misfit.d)\n    MAX = misfit.d.norm(\"linf\")\n    noise_std_dev = rel_noise * MAX\n    randn_perturb(misfit.d, noise_std_dev)\n    misfit.noise_variance = noise_std_dev*noise_std_dev\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], atrue), mytitle = \"True source\", subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132)\n        nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133)\n        plt.show()\n\n    model = Model(pde, reg, misfit)\n    u = model.generate_vector(STATE)\n    a = a0.copy()\n    p = model.generate_vector(ADJOINT)\n    x = [u,a,p]\n    mg = model.generate_vector(PARAMETER)\n    model.solveFwd(u, x)\n    model.solveAdj(p, x)\n    model.evalGradientParameter(x, mg)\n    model.setPointForHessianEvaluations(x)\n\n    H = ReducedHessian(model, 1e-12)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(H)\n    solver.set_preconditioner( reg.Rsolver )\n    solver.parameters[\"print_level\"] = -1\n    solver.parameters[\"rel_tolerance\"] = 1e-9\n    solver.solve(a, -mg)\n\n    if solver.converged:\n        if verbose:\n            print(\"CG converged in \", solver.iter, \" iterations.\")\n    else:\n        print(\"CG did not converged.\")\n        raise\n\n    model.solveFwd(u, x, 1e-12)\n\n    total_cost, reg_cost, misfit_cost = model.cost(x)\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], a), mytitle = \"Reconstructed source\", subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132)\n        nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133)\n        plt.show()\n\n    H.misfit_only = True\n    k_evec = 80\n    p_evec = 5\n    if verbose:\n        print(\"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec))\n    Omega = np.random.randn(get_local_size(a), k_evec+p_evec)\n    d, U = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec)\n\n    if verbose:\n        plt.figure()\n        nb.plot_eigenvalues(d, mytitle=\"Generalized Eigenvalues\")\n        nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15])\n        plt.show()\n\n    return d, U, Vh[PARAMETER], solver.iter\n\n\n\n\n\n3. Solution of the source inversion problem\n\n\nndim = 2\nnx = 32\nny = 32\n\nntargets = 300\nnp.random.seed(seed=1)\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nrel_noise = 0.01\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nd, U, Va, nit = solve(nx,ny, targets, rel_noise, gamma, delta)\n\n\n\n\nNumber of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089\nNumber of observation points: 300\n\n\n\n\n\nCG converged in  74  iterations.\n\n\n\n\n\nDouble Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.\n\n\n\n\n\n\n\n4. Mesh independence of the spectrum of the preconditioned Hessian\n\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nn = [16,32,64]\nd1, U1, Va1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False)\n\nprint(\"Number of Iterations: \", niter1, niter2, niter3)\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  62 74 76\n\n\n\n\n\n\n\n\n\n\n\n5. Dependence on the noise level\n\n\nWe solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization.\n\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nrel_noise = [1e-3,1e-2,1e-1]\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False)\n\nprint(\"Number of Iterations: \", niter1, niter2, niter3)\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  161 74 21\n\n\n\n\n\n\n\n\n\n\n\n6. Dependence on the PDE coefficients\n\n\nAssume a constant reaction term \nc = 1\n, and we consider different values for the diffusivity coefficient \nk\n.\n\n\nThe smaller the value of \nk\n the slower the decay in the spectrum.\n\n\nrel_noise = 0.01\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(1.0)\n\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.1)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.01)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\n\nprint(\"Number of Iterations: \", niter1, niter2, niter3)\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\"k=1. Eigen\", which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\"k=0.1 Eigen\", which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\"k=0.01 Eigen\", which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  88 147 256\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "Hessian Spectrum"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/#spectrum-of-the-reduced-hessian",
            "text": "",
            "title": "Spectrum of the Reduced Hessian"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/#the-linear-source-inversion-problem",
            "text": "We consider the following linear source inversion problem.\nFind the state  u \\in H^1_{\\Gamma_D}(\\Omega)  and the source ( parameter )  a \\in H^1(\\Omega)  that solves \\begin{align*}\n{} & \\min_a \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|a-a_0|^2 + \\gamma|\\nabla (a - a_0)|^2 \\right] dx & {}\\\\\n{\\rm s.t.} & {} &{} \\\\\n{} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = a & {\\rm in} \\; \\Omega\\\\\n{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n\\end{align*}   Here:     u_d  is a  n_{\\rm obs}  finite dimensional vector that denotes noisy observations of the state  u  in  n_{\\rm obs}  locations  \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . More specifically,  u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where  \\eta_i  are i.i.d.  \\mathcal{N}(0, \\sigma^2) .     B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}}  is the linear operator that evaluates the state  u  at the observation locations  \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} .     \\delta  and  \\gamma  are the parameters of the regularization penalizing the  L^2(\\Omega)  and  H^1(\\Omega)  norm of  a-a_0 , respectively.     k ,  {\\bf v} ,  c  are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively.     \\Gamma_D \\subset \\partial \\Omega ,  \\Gamma_N \\subset \\partial \\Omega  represents the subdomain of  \\partial\\Omega  where we impose Dirichlet or Neumann boundary conditions, respectively.",
            "title": "The linear source inversion problem"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/#1-load-modules",
            "text": "from __future__ import absolute_import, division, print_function\n\nimport dolfin as dl\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nimport sys\nimport os\nsys.path.append( os.environ.get('HIPPYLIB_BASE_DIR', \"../\") )\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)",
            "title": "1. Load modules"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/#2-the-linear-source-inversion-problem",
            "text": "def pde_varf(u,a,p):\n    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n           + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\\n           + c*u*p*dl.dx \\\n           - a*p*dl.dx\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and x[1] < dl.DOLFIN_EPS\n\ndef solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True):\n    np.random.seed(seed=2)\n    mesh = dl.UnitSquareMesh(nx, ny)\n    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n\n    Vh = [Vh1, Vh1, Vh1]\n    if verbose:\n        print(\"Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\".format(\n            Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim()) )\n\n\n    u_bdr = dl.Constant(0.0)\n    u_bdr0 = dl.Constant(0.0)\n    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n    bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\n    atrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector()\n    a0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector()\n\n    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n    if verbose:\n        print(\"Number of observation points: {0}\".format(targets.shape[0]))\n\n    misfit = PointwiseStateObservation(Vh[STATE], targets)\n\n    reg = LaplacianPrior(Vh[PARAMETER], gamma, delta)\n\n    #Generate synthetic observations\n    utrue = pde.generate_state()\n    x = [utrue, atrue, None]\n    pde.solveFwd(x[STATE], x, 1e-9)\n    misfit.B.mult(x[STATE], misfit.d)\n    MAX = misfit.d.norm(\"linf\")\n    noise_std_dev = rel_noise * MAX\n    randn_perturb(misfit.d, noise_std_dev)\n    misfit.noise_variance = noise_std_dev*noise_std_dev\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], atrue), mytitle = \"True source\", subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\"True state\", subplot_loc=132)\n        nb.plot_pts(targets, misfit.d,mytitle=\"Observations\", subplot_loc=133)\n        plt.show()\n\n    model = Model(pde, reg, misfit)\n    u = model.generate_vector(STATE)\n    a = a0.copy()\n    p = model.generate_vector(ADJOINT)\n    x = [u,a,p]\n    mg = model.generate_vector(PARAMETER)\n    model.solveFwd(u, x)\n    model.solveAdj(p, x)\n    model.evalGradientParameter(x, mg)\n    model.setPointForHessianEvaluations(x)\n\n    H = ReducedHessian(model, 1e-12)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(H)\n    solver.set_preconditioner( reg.Rsolver )\n    solver.parameters[\"print_level\"] = -1\n    solver.parameters[\"rel_tolerance\"] = 1e-9\n    solver.solve(a, -mg)\n\n    if solver.converged:\n        if verbose:\n            print(\"CG converged in \", solver.iter, \" iterations.\")\n    else:\n        print(\"CG did not converged.\")\n        raise\n\n    model.solveFwd(u, x, 1e-12)\n\n    total_cost, reg_cost, misfit_cost = model.cost(x)\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], a), mytitle = \"Reconstructed source\", subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], u), mytitle=\"Reconstructed state\", subplot_loc=132)\n        nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\"Misfit\", subplot_loc=133)\n        plt.show()\n\n    H.misfit_only = True\n    k_evec = 80\n    p_evec = 5\n    if verbose:\n        print(\"Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\".format(k_evec,p_evec))\n    Omega = np.random.randn(get_local_size(a), k_evec+p_evec)\n    d, U = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec)\n\n    if verbose:\n        plt.figure()\n        nb.plot_eigenvalues(d, mytitle=\"Generalized Eigenvalues\")\n        nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\"Eigenvectors\", which=[0,1,2,5,10,15])\n        plt.show()\n\n    return d, U, Vh[PARAMETER], solver.iter",
            "title": "2. The linear source inversion problem"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/#3-solution-of-the-source-inversion-problem",
            "text": "ndim = 2\nnx = 32\nny = 32\n\nntargets = 300\nnp.random.seed(seed=1)\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nrel_noise = 0.01\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nd, U, Va, nit = solve(nx,ny, targets, rel_noise, gamma, delta)  Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089\nNumber of observation points: 300   CG converged in  74  iterations.   Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.",
            "title": "3. Solution of the source inversion problem"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/#4-mesh-independence-of-the-spectrum-of-the-preconditioned-hessian",
            "text": "gamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nn = [16,32,64]\nd1, U1, Va1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False)\n\nprint(\"Number of Iterations: \", niter1, niter2, niter3)\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[0],n[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[1],n[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\"Eigenvalues Mesh {0} by {1}\".format(n[2],n[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\"Mesh {0} by {1} Eigen\".format(n[0],n[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\"Mesh {0} by {1} Eigen\".format(n[1],n[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\"Mesh {0} by {1} Eigen\".format(n[2],n[2]), which=[0,1,5])\n\nplt.show()  Number of Iterations:  62 74 76",
            "title": "4. Mesh independence of the spectrum of the preconditioned Hessian"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/#5-dependence-on-the-noise-level",
            "text": "We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization.  gamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nrel_noise = [1e-3,1e-2,1e-1]\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False)\n\nprint(\"Number of Iterations: \", niter1, niter2, niter3)\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\"Eigenvalues rel_noise {0:g}\".format(rel_noise[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\"rel_noise {0:g} Eigen\".format(rel_noise[2]), which=[0,1,5])\n\nplt.show()  Number of Iterations:  161 74 21",
            "title": "5. Dependence on the noise level"
        },
        {
            "location": "/tutorials_v1.6.0/5_HessianSpectrum/#6-dependence-on-the-pde-coefficients",
            "text": "Assume a constant reaction term  c = 1 , and we consider different values for the diffusivity coefficient  k .  The smaller the value of  k  the slower the decay in the spectrum.  rel_noise = 0.01\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(1.0)\n\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.1)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.01)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\n\nprint(\"Number of Iterations: \", niter1, niter2, niter3)\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\"Eigenvalues k=1.0\", subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\"Eigenvalues k=0.1\", subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\"Eigenvalues k=0.01\", subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\"k=1. Eigen\", which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\"k=0.1 Eigen\", which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\"k=0.01 Eigen\", which=[0,1,5])\n\nplt.show()  Number of Iterations:  88 147 256      Copyright (c) 2016-2018, The University of Texas at Austin & University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.",
            "title": "6. Dependence on the PDE coefficients"
        },
        {
            "location": "/documentation/",
            "text": "Documentation\n\n\nInstallation\n\n\nhIPPYlib\n depends on \nFEniCS\n versions 1.6, 2016.1, 2016.2, 2017.1, 2017.2.\n\n\nWe recommend using \nFEniCS 2017.2\n with \nhIPPYlib\n. \nNote\n: \nFEniCS 2018.1\n is not supported by \nhIPPYlib\n.\n\n\nFEniCS\n needs to be built with the following dependecies:\n\n\n\n\nnumpy\n, \nscipy\n, \nmatplotlib\n, \nmpi4py\n\n\nPETSc\n and \npetsc4py\n (version 3.7.0 or above)\n\n\nSLEPc\n and \nslepc4py\n (version 3.7.0 or above)\n\n\nPETSc dependencies: \nparmetis\n, \nscotch\n, \nsuitesparse\n, \nsuperlu_dist\n, \nml\n, \nhypre\n\n\n(optional): \nmshr\n, \njupyter\n\n\n\n\n\n\nFor detailed installation instructions see \nhere\n.\n\n\n\n\nhiPPYlib Docker container\n\n\nA Docker image \nhIPPYlib\n, \nFEniCS\n and their dependencies preinstalled is available \nhere\n. The username is \nuser1\n and password \nBreckenridge1_g2s3\n.\n\n\nDocumentation\n\n\nThe complete API reference of \nhIPPYlib\n is available at \nreadthedocs\n.",
            "title": "Documentation"
        },
        {
            "location": "/documentation/#documentation",
            "text": "",
            "title": "Documentation"
        },
        {
            "location": "/documentation/#installation",
            "text": "hIPPYlib  depends on  FEniCS  versions 1.6, 2016.1, 2016.2, 2017.1, 2017.2.  We recommend using  FEniCS 2017.2  with  hIPPYlib .  Note :  FEniCS 2018.1  is not supported by  hIPPYlib .  FEniCS  needs to be built with the following dependecies:   numpy ,  scipy ,  matplotlib ,  mpi4py  PETSc  and  petsc4py  (version 3.7.0 or above)  SLEPc  and  slepc4py  (version 3.7.0 or above)  PETSc dependencies:  parmetis ,  scotch ,  suitesparse ,  superlu_dist ,  ml ,  hypre  (optional):  mshr ,  jupyter    For detailed installation instructions see  here .",
            "title": "Installation"
        },
        {
            "location": "/documentation/#hippylib-docker-container",
            "text": "A Docker image  hIPPYlib ,  FEniCS  and their dependencies preinstalled is available  here . The username is  user1  and password  Breckenridge1_g2s3 .",
            "title": "hiPPYlib Docker container"
        },
        {
            "location": "/documentation/#documentation_1",
            "text": "The complete API reference of  hIPPYlib  is available at  readthedocs .",
            "title": "Documentation"
        },
        {
            "location": "/download/",
            "text": "Download\n\n\nLatest release\n\n\n\n\nDownload \nhippylib-2.2.0.zip\n\n\n\n\nAll releases\n\n\n\n\n\n\n\n\nFilename\n\n\nVersion\n\n\nRelease Date\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nhippylib-2.2.0.zip\n\n\n2.2.0\n\n\nDec 2018\n\n\nEnchantments and install\n\n\n\n\n\n\nhippylib-2.1.1.zip\n\n\n2.1.1\n\n\nOct 2018\n\n\nPublication to \nJOSS\n\n\n\n\n\n\nhippylib-2.1.0.tar.gz\n\n\n2.1.0\n\n\nJuly 2018\n\n\nEnchantments & Bugfixes\n\n\n\n\n\n\nhippylib-2.0.0.tar.gz\n\n\n2.0.0\n\n\nJune 2018\n\n\nMajor release\n\n\n\n\n\n\nhippylib-1.6.0.tar.gz\n\n\n1.6.0\n\n\nMay 2018\n\n\nBugfix\n\n\n\n\n\n\nhippylib-1.5.0.tar.gz\n\n\n1.5.0\n\n\nJan 2018\n\n\nSupport for FEniCS 2017.2\n\n\n\n\n\n\nhippylib-1.4.0.tar.gz\n\n\n1.4.0\n\n\nNov 2017\n\n\nSupport for Python 3\n\n\n\n\n\n\nhippylib-1.3.0.tar.gz\n\n\n1.3.0\n\n\nJune 2017\n\n\nSupport for FEniCS 2017.1; GLPv2\n\n\n\n\n\n\nhippylib-1.2.0.tar.gz\n\n\n1.2.0\n\n\nApr 2017\n\n\nSupport for FEniCS 2016.2\n\n\n\n\n\n\nhippylib-1.1.0.tar.gz\n\n\n1.1.0\n\n\nNov 2016\n\n\nSupport for FEniCS 2016.1\n\n\n\n\n\n\nhippylib-1.0.2.tar.gz\n\n\n1.0.2\n\n\nSept 2016\n\n\nEnchantment\n\n\n\n\n\n\nhippylib-1.0.1.tar.gz\n\n\n1.0.1\n\n\nAug 2016\n\n\nBugfix\n\n\n\n\n\n\nhippylib-1.0.0.tar.gz\n\n\n1.0.0\n\n\nAug 2016\n\n\nInitial release\n\n\n\n\n\n\n\n\nhIPPYlib\n releases as of \n1.2.0\n are also archived on \nZenodo\n.\n\n\nSee the \nchangelog\n on \nreadthedocs\n for further details.",
            "title": "Download"
        },
        {
            "location": "/download/#download",
            "text": "",
            "title": "Download"
        },
        {
            "location": "/download/#latest-release",
            "text": "Download  hippylib-2.2.0.zip",
            "title": "Latest release"
        },
        {
            "location": "/download/#all-releases",
            "text": "Filename  Version  Release Date  Notes      hippylib-2.2.0.zip  2.2.0  Dec 2018  Enchantments and install    hippylib-2.1.1.zip  2.1.1  Oct 2018  Publication to  JOSS    hippylib-2.1.0.tar.gz  2.1.0  July 2018  Enchantments & Bugfixes    hippylib-2.0.0.tar.gz  2.0.0  June 2018  Major release    hippylib-1.6.0.tar.gz  1.6.0  May 2018  Bugfix    hippylib-1.5.0.tar.gz  1.5.0  Jan 2018  Support for FEniCS 2017.2    hippylib-1.4.0.tar.gz  1.4.0  Nov 2017  Support for Python 3    hippylib-1.3.0.tar.gz  1.3.0  June 2017  Support for FEniCS 2017.1; GLPv2    hippylib-1.2.0.tar.gz  1.2.0  Apr 2017  Support for FEniCS 2016.2    hippylib-1.1.0.tar.gz  1.1.0  Nov 2016  Support for FEniCS 2016.1    hippylib-1.0.2.tar.gz  1.0.2  Sept 2016  Enchantment    hippylib-1.0.1.tar.gz  1.0.1  Aug 2016  Bugfix    hippylib-1.0.0.tar.gz  1.0.0  Aug 2016  Initial release     hIPPYlib  releases as of  1.2.0  are also archived on  Zenodo .  See the  changelog  on  readthedocs  for further details.",
            "title": "All releases"
        },
        {
            "location": "/research/",
            "text": "Research\n\n\nApplications\n\n\n\n\n\n\nInversion for optical properties of biological tissues in quantitative optoacoustic tomography\n\n\n\n\n\n\nBayesian optimal experimental design for inverse problems in acoustic scattering\n\n\n\n\n\n\nInversion and control for CO\n2\n sequestration with poroelastic models\n\n\n\n\n\n\nGoal-oriented inference for reservoir models with complex features including faults\n\n\n\n\n\n\nJoint seismic-electromagnetic inversion\n\n\n\n\n\n\nInference of basal boundary conditions for ice sheet flow\n\n\n\n\n\n\nInversion for coupled ice-ocean interaction\n\n\n\n\n\n\nInversion for material properties of cardiac tissue\n\n\n\n\n\n\nInference, prediction and optimization under uncertainty for turbulent combustion\n\n\n\n\n\n\nInference of constitutive laws in mechanics of nano-scale filaments\n\n\n\n\n\n\nPublications\n\n\n\n\n\n\nE. M. Constantinescu, N. Petra, J. Bessac, C. G. Petra, \nStatistical Treatment of Inverse Problems Constrained by Differential Equations-Based Models with Stochastic Terms\n, arXiv, 2018\n\n\n\n\n\n\nP. Chen, U. Villa, O. Ghattas, \nTaylor approximation and variance reduction for PDE-constrained optimal control under uncertainty\n, ArXiv, 2018\n\n\n\n\n\n\nU. Villa, N. Petra, O. Ghattas, \nhIPPYlib: An extensible software framework for large-scale inverse problems; Part I: Deterministic inversion and linearized Bayesian inference\n, in preparation, 2018\n\n\n\n\n\n\nU. Villa, N. Petra, O. Ghattas, \nhIPPYlib: An extensible software framework for large-scale inverse problems\n, Journal of Open Source Software (JOSS), 3(30):940, 2018\n\n\n\n\n\n\nP. Chen, U. Villa, O. Ghattas, \nTaylor approximation for PDE-constrained optimization under uncertainty: Application to turbulent jet flow\n, Proceedings in Applied Mathematics and Mechanics - 89th GAMM Annual Meeting, accepted, 2018\n\n\n\n\n\n\nPh.D. Thesis\n\n\n\n\n\n\nK. A. McCormack, \nEarthquakes, groundwater and surface deformation: exploring the poroelastic response to megathrust earthquakes\n, University of Texas at Austin, 2018. Adviser M. Hesse\n\n\n\n\n\n\nB. Crestel, \nAdvanced techniques for multi-source, multi-parameter, and multi-physics inverse problems\n, University of Texas at Austin, 2017. Adviser O. Ghattas\n\n\n\n\n\n\nOral Presentations\n\n\n\n\n\n\nU. Villa, \nLearning from data through the lens of mathematical models\n, Analysis Seminar, Dept. of Mathematics & Statistics, Washington University, January 28, 2019, St Louis, MO, US\n\n\n\n\n\n\nU. Villa, \nLarge Scale Inverse Problems and Uncertainty Quantification: Computational Tools and Imaging Applications\n, Electrical & Systems Engineering Seminar, Washington University, January 24, 2019, St Louis, MO, US\n\n\n\n\n\n\nN. Petra, \nhIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems\n, Optimization Seminar, University of California, Merced, October 19, 2018, Merced, CA, US\n\n\n\n\n\n\nU. Villa, O. Ghattas, \nMaximize the Expected Information Gain in Bayesian Experimental Design Problems: A Fast Optimization Algorithm Based on Laplace Approximation and Randomized Eigensolvers\n, SIAM UQ, April 16-19, 2018, Garden Grove, CA, US\n\n\n\n\n\n\nT. O\u2019Leary-Roseberry, \nA PDE Constrained Optimization Approach to the Solution\nof the Stefan Problem\n, Texas Applied Mathematics\nand Engineering Symposium, Sept. 21-23, 2017, Austin, TX, US\n\n\n\n\n\n\nU. Villa, \nhIPPYlib:  An Extensible Software Framework for Large-Scale Deterministic and Linearized Bayesian Inverse\n, Texas Applied Mathematics\nand Engineering Symposium, Sept. 21-23, 2017, Austin, TX, US\n\n\n\n\n\n\nU. Villa, \nTaylor Approximation for PDE-constrained Optimal Control Problems under High-dimensional Uncertainty\n, SIAM Control, July 10 - 12, 2017, Pittsburgh, Pa, US\n\n\n\n\n\n\nU. Villa, \nDerivative-Informed MCMC for Bayesian Calibration of Stochastic PDE Models\n, SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US\n\n\n\n\n\n\nU. Villa, \nHessian-based sampling techniques for Bayesian inverse problems with stochastic PDE forward model\n, Applied Inverse Problems, May 29 - June 2, 2017, Hangzhou, China\n\n\n\n\n\n\nU. Villa, \nBayesian Calibration of Inadequate Stochastic PDE Models\n, SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nP. Chen, \nTaylor Approximation for PDE-Constrained Optimal Control Problems Under High-Dimensional Uncertainty: Application to a Turbulence Model\n, SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nB. Crestel, \nScalable Solvers for Joint Inversion with Several Structural Coupling Terms\n, SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nAmal Alghamdi, \nBayesian Inversion for Subsurface Properties from Poroelastic Forward Models and Surface Deformation Data\n, SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nU. Villa, \nAn Analytical Technique for Forward and Inverse Propagation of Uncertainty\n, SIAM UQ, April 5-8, 2016, Lausanne, Switzerland\n\n\n\n\n\n\nPoster Presentations\n\n\n\n\n\n\nA. O. Babaniyi, O. Ghattas, N. Petra, U. Villa, \nhIPPYlib: An Extensible Software Framework for Large-scale Inverse Problems\n, SIAM CSE, Feb 25- March 1, 2019, Spokane, Wa, US\n\n\n\n\n\n\nO. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa, \nIntegrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion\n, NSF-SI2 PI meeting, Apr. 30- May 1, 2018, Washington, DC, US\n\n\n\n\n\n\nK. Koval, G. Stadler, \nComputational Approaches for Linear Goal-Oriented Bayesian Inverse Problems\n, SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US\n\n\n\n\n\n\nJ. Chen, A. Drach, U. Villa, R. Avazmohammadi, D. Li, O. Ghattas and M. Sacks, \nIdentification of Mechanical Properties of 3D Myocardial Tissue: An Inverse Modeling and Optimal Experimental Design Problem\n, FEniCS'17, June 12-14, 2017, University of Luxembourg, Luxembourg\n\n\n\n\n\n\nT. O\u2019Leary-Roseberry, U. Villa, O. Ghattas, P. Heimbach, \nAn Adjoint Capable Solver for the Stefan Problem: a Bilevel Optimization and Level Set Approach\n, SIAM CSE, Feb. 27 - March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nO. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa, \nIntegrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion\n, NSF-SI2 PI meeting, Feb. 21-22, 2017, Arlington, VA, US\n\n\n\n\n\n\n\n\nDid \nyou\n publish an article or give an oral/poster presentation using \nhIPPYlib\n? Please let us know by creating an \nissue\n and we will reference your work on this page.",
            "title": "Research"
        },
        {
            "location": "/research/#research",
            "text": "",
            "title": "Research"
        },
        {
            "location": "/research/#applications",
            "text": "Inversion for optical properties of biological tissues in quantitative optoacoustic tomography    Bayesian optimal experimental design for inverse problems in acoustic scattering    Inversion and control for CO 2  sequestration with poroelastic models    Goal-oriented inference for reservoir models with complex features including faults    Joint seismic-electromagnetic inversion    Inference of basal boundary conditions for ice sheet flow    Inversion for coupled ice-ocean interaction    Inversion for material properties of cardiac tissue    Inference, prediction and optimization under uncertainty for turbulent combustion    Inference of constitutive laws in mechanics of nano-scale filaments",
            "title": "Applications"
        },
        {
            "location": "/research/#publications",
            "text": "E. M. Constantinescu, N. Petra, J. Bessac, C. G. Petra,  Statistical Treatment of Inverse Problems Constrained by Differential Equations-Based Models with Stochastic Terms , arXiv, 2018    P. Chen, U. Villa, O. Ghattas,  Taylor approximation and variance reduction for PDE-constrained optimal control under uncertainty , ArXiv, 2018    U. Villa, N. Petra, O. Ghattas,  hIPPYlib: An extensible software framework for large-scale inverse problems; Part I: Deterministic inversion and linearized Bayesian inference , in preparation, 2018    U. Villa, N. Petra, O. Ghattas,  hIPPYlib: An extensible software framework for large-scale inverse problems , Journal of Open Source Software (JOSS), 3(30):940, 2018    P. Chen, U. Villa, O. Ghattas,  Taylor approximation for PDE-constrained optimization under uncertainty: Application to turbulent jet flow , Proceedings in Applied Mathematics and Mechanics - 89th GAMM Annual Meeting, accepted, 2018",
            "title": "Publications"
        },
        {
            "location": "/research/#phd-thesis",
            "text": "K. A. McCormack,  Earthquakes, groundwater and surface deformation: exploring the poroelastic response to megathrust earthquakes , University of Texas at Austin, 2018. Adviser M. Hesse    B. Crestel,  Advanced techniques for multi-source, multi-parameter, and multi-physics inverse problems , University of Texas at Austin, 2017. Adviser O. Ghattas",
            "title": "Ph.D. Thesis"
        },
        {
            "location": "/research/#oral-presentations",
            "text": "U. Villa,  Learning from data through the lens of mathematical models , Analysis Seminar, Dept. of Mathematics & Statistics, Washington University, January 28, 2019, St Louis, MO, US    U. Villa,  Large Scale Inverse Problems and Uncertainty Quantification: Computational Tools and Imaging Applications , Electrical & Systems Engineering Seminar, Washington University, January 24, 2019, St Louis, MO, US    N. Petra,  hIPPYlib: An Extensible Software Framework for Large-Scale Inverse Problems , Optimization Seminar, University of California, Merced, October 19, 2018, Merced, CA, US    U. Villa, O. Ghattas,  Maximize the Expected Information Gain in Bayesian Experimental Design Problems: A Fast Optimization Algorithm Based on Laplace Approximation and Randomized Eigensolvers , SIAM UQ, April 16-19, 2018, Garden Grove, CA, US    T. O\u2019Leary-Roseberry,  A PDE Constrained Optimization Approach to the Solution\nof the Stefan Problem , Texas Applied Mathematics\nand Engineering Symposium, Sept. 21-23, 2017, Austin, TX, US    U. Villa,  hIPPYlib:  An Extensible Software Framework for Large-Scale Deterministic and Linearized Bayesian Inverse , Texas Applied Mathematics\nand Engineering Symposium, Sept. 21-23, 2017, Austin, TX, US    U. Villa,  Taylor Approximation for PDE-constrained Optimal Control Problems under High-dimensional Uncertainty , SIAM Control, July 10 - 12, 2017, Pittsburgh, Pa, US    U. Villa,  Derivative-Informed MCMC for Bayesian Calibration of Stochastic PDE Models , SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US    U. Villa,  Hessian-based sampling techniques for Bayesian inverse problems with stochastic PDE forward model , Applied Inverse Problems, May 29 - June 2, 2017, Hangzhou, China    U. Villa,  Bayesian Calibration of Inadequate Stochastic PDE Models , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US    P. Chen,  Taylor Approximation for PDE-Constrained Optimal Control Problems Under High-Dimensional Uncertainty: Application to a Turbulence Model , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US    B. Crestel,  Scalable Solvers for Joint Inversion with Several Structural Coupling Terms , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US    Amal Alghamdi,  Bayesian Inversion for Subsurface Properties from Poroelastic Forward Models and Surface Deformation Data , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US    U. Villa,  An Analytical Technique for Forward and Inverse Propagation of Uncertainty , SIAM UQ, April 5-8, 2016, Lausanne, Switzerland",
            "title": "Oral Presentations"
        },
        {
            "location": "/research/#poster-presentations",
            "text": "A. O. Babaniyi, O. Ghattas, N. Petra, U. Villa,  hIPPYlib: An Extensible Software Framework for Large-scale Inverse Problems , SIAM CSE, Feb 25- March 1, 2019, Spokane, Wa, US    O. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa,  Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SI2 PI meeting, Apr. 30- May 1, 2018, Washington, DC, US    K. Koval, G. Stadler,  Computational Approaches for Linear Goal-Oriented Bayesian Inverse Problems , SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US    J. Chen, A. Drach, U. Villa, R. Avazmohammadi, D. Li, O. Ghattas and M. Sacks,  Identification of Mechanical Properties of 3D Myocardial Tissue: An Inverse Modeling and Optimal Experimental Design Problem , FEniCS'17, June 12-14, 2017, University of Luxembourg, Luxembourg    T. O\u2019Leary-Roseberry, U. Villa, O. Ghattas, P. Heimbach,  An Adjoint Capable Solver for the Stefan Problem: a Bilevel Optimization and Level Set Approach , SIAM CSE, Feb. 27 - March 3, 2017, Atlanta, GA, US    O. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa,  Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SI2 PI meeting, Feb. 21-22, 2017, Arlington, VA, US     Did  you  publish an article or give an oral/poster presentation using  hIPPYlib ? Please let us know by creating an  issue  and we will reference your work on this page.",
            "title": "Poster Presentations"
        },
        {
            "location": "/outreach/",
            "text": "Outreach\n\n\nSchools and workshops\n\n\n\n\n\n\nO. Ghattas, Y. Marzouk, M. Parno, N. Petra, G. Stadler, U. Villa, \nInverse Problems:\nSystematic Integration of Data with Models under Uncertainty\n, 2018 Gene Golub SIAM Summer School, June 17-30, 2018. Breckenridge, Colorado, USA. See also \nannouncement flyer\n\n\n\n\n\n\nN. Petra, \nSAMSI Optimization Program Summer School\n, Research Triangle Park, NC,  August 8-12, 2016\n\n\n\n\n\n\nN. Petra and O. Ghattas, \nIDEALab: Inverse Problems and Uncertainty Quantification\n, Brown University, Providence, RD, July 6-10, 2015\n\n\n\n\n\n\nGraduate level courses\n\n\n\n\n\n\nO. Ghattas @UT Austin, \nComputational and Variational Inverse Problems\n, Fall 2017 \nlink\n\n\n\n\n\n\nA. Alexanderian @NC State, \nInverse problems\n, Fall 2016\n\n\n\n\n\n\nG. Stadler @NYU, \nAdvanced Topics in Numerical Analysis: Computational and Variational Methods for Inverse Problems\n, Spring 2016 \nlink\n\n\n\n\n\n\nN. Petra @UC Merced, \nSpecial Topics: Computational and Variational Inverse Problems\n, Fall 2015 \nlink\n\n\n\n\n\n\nO. Ghattas @UT Austin, \nComputational and Variational Inverse Problems\n, Fall 2015 \nlink",
            "title": "Outreach"
        },
        {
            "location": "/outreach/#outreach",
            "text": "",
            "title": "Outreach"
        },
        {
            "location": "/outreach/#schools-and-workshops",
            "text": "O. Ghattas, Y. Marzouk, M. Parno, N. Petra, G. Stadler, U. Villa,  Inverse Problems:\nSystematic Integration of Data with Models under Uncertainty , 2018 Gene Golub SIAM Summer School, June 17-30, 2018. Breckenridge, Colorado, USA. See also  announcement flyer    N. Petra,  SAMSI Optimization Program Summer School , Research Triangle Park, NC,  August 8-12, 2016    N. Petra and O. Ghattas,  IDEALab: Inverse Problems and Uncertainty Quantification , Brown University, Providence, RD, July 6-10, 2015",
            "title": "Schools and workshops"
        },
        {
            "location": "/outreach/#graduate-level-courses",
            "text": "O. Ghattas @UT Austin,  Computational and Variational Inverse Problems , Fall 2017  link    A. Alexanderian @NC State,  Inverse problems , Fall 2016    G. Stadler @NYU,  Advanced Topics in Numerical Analysis: Computational and Variational Methods for Inverse Problems , Spring 2016  link    N. Petra @UC Merced,  Special Topics: Computational and Variational Inverse Problems , Fall 2015  link    O. Ghattas @UT Austin,  Computational and Variational Inverse Problems , Fall 2015  link",
            "title": "Graduate level courses"
        },
        {
            "location": "/postdoc_position/",
            "text": "Openings\n\n\nOpen Postdoc Position at UC Merced\n\n\nThere is an opening for a postdoc position in Professor \nNoemi Petra\n's\nresearch group in the \nSchool of Natural Sciences\n at the \nUniversity of\nCalifornia, Merced\n.\n\n\nThe postdoctoral researcher will work under an\nNSF-funded Collaborative Research (with UC Merced, UT Austin and MIT)\nentitled: \nIntegrating Data with Complex Predictive Models under\nUncertainty: An Extensible Software Framework for Large-Scale Bayesian\nInversion\n (see \nhere\n for a general overview of the project).\n\n\nThe postdoctoral researcher will perform research in the\nfield of large-scale Bayesian inverse problems, and on the\nimplementation of new features in hIPPYlib. The postdoc will contribute to\nthe research dissemination and will also help build the user/developer\ncommunity by attending and speaking at conferences, workshops and\nsummer schools at local and international events.\n\n\nInterested candidates should contact Noemi Petra at\n\nnpetra@ucmerced.edu\n.\n\n\nApply at \nhttps://aprecruit.ucmerced.edu/apply/JPF00738\n.",
            "title": "Openings"
        },
        {
            "location": "/postdoc_position/#openings",
            "text": "",
            "title": "Openings"
        },
        {
            "location": "/postdoc_position/#open-postdoc-position-at-uc-merced",
            "text": "There is an opening for a postdoc position in Professor  Noemi Petra 's\nresearch group in the  School of Natural Sciences  at the  University of\nCalifornia, Merced .  The postdoctoral researcher will work under an\nNSF-funded Collaborative Research (with UC Merced, UT Austin and MIT)\nentitled:  Integrating Data with Complex Predictive Models under\nUncertainty: An Extensible Software Framework for Large-Scale Bayesian\nInversion  (see  here  for a general overview of the project).  The postdoctoral researcher will perform research in the\nfield of large-scale Bayesian inverse problems, and on the\nimplementation of new features in hIPPYlib. The postdoc will contribute to\nthe research dissemination and will also help build the user/developer\ncommunity by attending and speaking at conferences, workshops and\nsummer schools at local and international events.  Interested candidates should contact Noemi Petra at npetra@ucmerced.edu .  Apply at  https://aprecruit.ucmerced.edu/apply/JPF00738 .",
            "title": "Open Postdoc Position at UC Merced"
        },
        {
            "location": "/about/",
            "text": "About hIPPYlib\n\n\nAuthors\n\n\n\n\nUmberto Villa\n\n\nNoemi Petra\n\n\nOmar Ghattas\n\n\n\n\nCurrent and past contributors\n\n\nAmal Alghamdi,\n\nIlona Ambartsumyan\n,\nJoshua Chen,\n\nPeng Chen\n,\nBen Crestel,\n\nEldar Khattatov\n,\nTom O'Leary-Roseberry,\nVishwas Rao,\nSiddhant Wahal\n\n\nCopyright\n\n\n\u00a9 2016-2018 The University of Texas at Austin, University of California Merced.\n\n\nSupport\n\n\n\n\n\n\nO. Ghattas, N. Petra (PIs),  U. Villa (Co-PI), \nIntegrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion\n, NSF-SSI2, Grants No ACI-1550593, ACI-1550547 (2016-2019).\n\n\n\n\n\n\nO. Ghattas (PI); G. Biros and Y. Marzouk (Co-PIs), \nBayesian Optimal Experimental Design for Inverse Scattering\n,\nAir Force Office of Scientific Research, Computational Mathematics program, grant FA9550-12-1-81243 (2017\u20132020).\n\n\n\n\n\n\nO. Ghattas (PI), G. Biros, M. Girolami, M. Heinkenschloss, R. Moser, A. Philpott, A. Stuart, and K. Willcox (Co-PIs), \nInference, Simulation, and Optimization of Complex Systems Under Uncertainty: Theory, Algorithms, and Applications to Turbulent Combustion\n, DARPA/ARO, Grants No W911NF-15-2-0121 (2016-2017).\n\n\n\n\n\n\nO. Ghattas (PI), M. Hesse (Co-PI), \nCDS&E: A Bayesian inference\nframework for management of CO2 sequestration\n, National Science\nFoundation, Division Of Chemical, Bioengineering, Environmental, &\nTransport Systems, grant CBET-1508713 (2015-2017).\n\n\n\n\n\n\nLicense\n\n\n\n\nGNU General Public License version 2 (GPL)\n\n\nOlder Releases (1.2.0 or older): \nGNU General Public License version 3 (GPL)\n\n\n\n\n\n\n\n\nWebsite built with \nMkDocs\n, \nbootstrap\n, \nbootswatch\n, and \nMathJax\n.\nHosted on \nGitHub\n.",
            "title": "About"
        },
        {
            "location": "/about/#about-hippylib",
            "text": "",
            "title": "About hIPPYlib"
        },
        {
            "location": "/about/#authors",
            "text": "Umberto Villa  Noemi Petra  Omar Ghattas",
            "title": "Authors"
        },
        {
            "location": "/about/#current-and-past-contributors",
            "text": "Amal Alghamdi, Ilona Ambartsumyan ,\nJoshua Chen, Peng Chen ,\nBen Crestel, Eldar Khattatov ,\nTom O'Leary-Roseberry,\nVishwas Rao,\nSiddhant Wahal",
            "title": "Current and past contributors"
        },
        {
            "location": "/about/#copyright",
            "text": "\u00a9 2016-2018 The University of Texas at Austin, University of California Merced.",
            "title": "Copyright"
        },
        {
            "location": "/about/#support",
            "text": "O. Ghattas, N. Petra (PIs),  U. Villa (Co-PI),  Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SSI2, Grants No ACI-1550593, ACI-1550547 (2016-2019).    O. Ghattas (PI); G. Biros and Y. Marzouk (Co-PIs),  Bayesian Optimal Experimental Design for Inverse Scattering ,\nAir Force Office of Scientific Research, Computational Mathematics program, grant FA9550-12-1-81243 (2017\u20132020).    O. Ghattas (PI), G. Biros, M. Girolami, M. Heinkenschloss, R. Moser, A. Philpott, A. Stuart, and K. Willcox (Co-PIs),  Inference, Simulation, and Optimization of Complex Systems Under Uncertainty: Theory, Algorithms, and Applications to Turbulent Combustion , DARPA/ARO, Grants No W911NF-15-2-0121 (2016-2017).    O. Ghattas (PI), M. Hesse (Co-PI),  CDS&E: A Bayesian inference\nframework for management of CO2 sequestration , National Science\nFoundation, Division Of Chemical, Bioengineering, Environmental, &\nTransport Systems, grant CBET-1508713 (2015-2017).",
            "title": "Support"
        },
        {
            "location": "/about/#license",
            "text": "GNU General Public License version 2 (GPL)  Older Releases (1.2.0 or older):  GNU General Public License version 3 (GPL)     Website built with  MkDocs ,  bootstrap ,  bootswatch , and  MathJax .\nHosted on  GitHub .",
            "title": "License"
        }
    ]
}