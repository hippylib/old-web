{
    "docs": [
        {
            "location": "/", 
            "text": "hIPPYlib - Inverse Problem PYthon library\n\n\nhIPPYlib implements state-of-the-art \nscalable\n \nadjoint-based\n algorithms for PDE-based \ndeterministic and Bayesian inverse problems\n. It builds on \nFEniCS\n for the discretization of the PDE and on \nPETSc\n for scalable and efficient linear algebra operations and solvers.\n\n\nFeatures\n\n\n\n\nFriendly, compact, near-mathematical FEniCS notation to express the PDE and likelihood in weak form\n\n\nAutomatic generation of efficient code for the discretization of weak forms using FEniCS\n\n\nSymbolic differentiation of weak forms to generate derivatives and adjoint information\n\n\nGlobalized Inexact Newton-CG method to solve the inverse problem\n\n\nLow rank representation of the posterior covariace using randomized algorithms\n\n\n\n\nSee also our \ntutorial\n and list of related \npublications\n.\n\n\nAnnouncements\n\n\n\n\n\n\nOpen \npostdoc position\n in Professor Noemi Petra's group. Apply \nhere\n.\n\n\n\n\n\n\nJune 2018: Gene Golub SIAM Summer School in Breckenridge, Colorado. \nEarly announement flyer\n; more details coming soon.\n\n\n\n\n\n\nLatest Release\n\n\n\n\nDevelopment version\n\n\nDownload \nhippylib-1.3.0.tar.gz\n\n\nPrevious releases\n\n\n\n\nContact\n\n\nDeveloped by the \nhIPPYlib team\n at \nUT Austin\n and \nUC Merced\n.\n\n\nTo ask question and find answers see \nhere\n.\n\n\nPlease cite with \n\n\n@article{VillaPetraGhattas2016,\ntitle = \n{hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Linearized Bayesian Inversion}\n,\nauthor = {Villa, U. and Petra, N. and Ghattas, O.},\nyear = {2016},\nurl = {http://hippylib.github.io}\n}", 
            "title": "Home"
        }, 
        {
            "location": "/#hippylib-inverse-problem-python-library", 
            "text": "hIPPYlib implements state-of-the-art  scalable   adjoint-based  algorithms for PDE-based  deterministic and Bayesian inverse problems . It builds on  FEniCS  for the discretization of the PDE and on  PETSc  for scalable and efficient linear algebra operations and solvers.", 
            "title": "hIPPYlib - Inverse Problem PYthon library"
        }, 
        {
            "location": "/#features", 
            "text": "Friendly, compact, near-mathematical FEniCS notation to express the PDE and likelihood in weak form  Automatic generation of efficient code for the discretization of weak forms using FEniCS  Symbolic differentiation of weak forms to generate derivatives and adjoint information  Globalized Inexact Newton-CG method to solve the inverse problem  Low rank representation of the posterior covariace using randomized algorithms   See also our  tutorial  and list of related  publications .", 
            "title": "Features"
        }, 
        {
            "location": "/#announcements", 
            "text": "Open  postdoc position  in Professor Noemi Petra's group. Apply  here .    June 2018: Gene Golub SIAM Summer School in Breckenridge, Colorado.  Early announement flyer ; more details coming soon.", 
            "title": "Announcements"
        }, 
        {
            "location": "/#latest-release", 
            "text": "Development version  Download  hippylib-1.3.0.tar.gz  Previous releases", 
            "title": "Latest Release"
        }, 
        {
            "location": "/#contact", 
            "text": "Developed by the  hIPPYlib team  at  UT Austin  and  UC Merced .  To ask question and find answers see  here .  Please cite with   @article{VillaPetraGhattas2016,\ntitle =  {hIPPYlib: an Extensible Software Framework for Large-scale Deterministic and Linearized Bayesian Inversion} ,\nauthor = {Villa, U. and Petra, N. and Ghattas, O.},\nyear = {2016},\nurl = {http://hippylib.github.io}\n}", 
            "title": "Contact"
        }, 
        {
            "location": "/tutorial/", 
            "text": "Tutorial\n\n\nThese tutorials are the best place to learn about the basic features and the algorithms in \nhIPPYlib\n.\n\n\n\n\nFEniCS101\n notebook illustrates the use of FEniCS for the solution of a linear boundary value problem.\n\n\nPoisson Deterministic\n notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting.\n\n\nSubsurface Bayesian\n notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting.\n\n\nAdvection-Diffusion Bayesian\n notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting.\n\n\nHessian Spectrum\n notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.\n\n\n\n\nThe interactive ipython notebooks are located in the \ntutorial\n folder of the \nhIPPYlib\n release.\n\n\nTo run the notebooks follow these instructions.\n\n\n\n\nOpen a FEniCS terminal and type\n\n\n\n\n$ cd tutorial\n$ jupyter notebook\n\n\n\n\n\n\nA new tab will open in your web-brower showing the notebooks.\n\n\nClick on the notebook you would like to use.\n\n\nTo run all the code in the notebook simply click on Cell --\n Run All.\n\n\n\n\nFor more information on installing ipython and using notebooks see \nhere\n.", 
            "title": "README"
        }, 
        {
            "location": "/tutorial/#tutorial", 
            "text": "These tutorials are the best place to learn about the basic features and the algorithms in  hIPPYlib .   FEniCS101  notebook illustrates the use of FEniCS for the solution of a linear boundary value problem.  Poisson Deterministic  notebook illustrates how to compute gradient/Hessian information and solve a non-linear parameter inversion for the Poisson equation in a deterministic setting.  Subsurface Bayesian  notebook illustrates how to solve a non-linear parameter inversion for the Poisson equation in a Bayesian setting.  Advection-Diffusion Bayesian  notebook illustrates how to solve a time-dependent linear inverse problem in a Bayesian setting.  Hessian Spectrum  notebook illustrates the spectral property of the Hessian operator for a linear source inversion problem.   The interactive ipython notebooks are located in the  tutorial  folder of the  hIPPYlib  release.  To run the notebooks follow these instructions.   Open a FEniCS terminal and type   $ cd tutorial\n$ jupyter notebook   A new tab will open in your web-brower showing the notebooks.  Click on the notebook you would like to use.  To run all the code in the notebook simply click on Cell --  Run All.   For more information on installing ipython and using notebooks see  here .", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/", 
            "text": "FEniCS101 Tutorial\n\n\nIn this tutorial we consider the boundary value problem (BVP)\n\n\n\n\n\\begin{eqnarray*}\n- \\nabla \\cdot (k \\nabla u) = f &      \\text{ in } \\Omega,\\\\\nu = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\\nk \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom},\n\\end{eqnarray*}\n\n\n\n\nwhere \n\\Omega = (0,1) \\times (0,1)\n, \n\\Gamma_D\n and and \n\\Gamma_N\n are the union of\nthe left and right, and top and bottom boundaries of \n\\Omega\n,\nrespectively.\n\n\nHere\n\n\\begin{eqnarray*}\nk(x,y) = 1  & \\text{ on } \\Omega\\\\\nf(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\\nu_0(x,y)      = 0 & \\text{ on } \\Gamma_D, \\\\\n\\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right.\n& \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array}\n\\end{eqnarray*}\n\n\n\n\nThe exact solution is\n\n u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right). \n\n\n\n\nWeak formulation\n\n\nLet us define the Hilbert spaces \nV_{u_0}, V_0 \\in \\Omega\n as\n\n V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\},\n\n\n V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}.\n\n\n\n\nTo obtain the weak formulation, we multiply the PDE by an arbitrary function \nv \\in V_0\n and integrate over the domain \n\\Omega\n leading to\n\n\n\n\n -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0. \n\n\n\n\nThen, integration by parts the non-conforming term gives\n\n\n\n\n \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0. \n\n\n\n\nFinally by recalling that \n v = 0 \n on \n\\Gamma_D\n and that \nk \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma \n on \n\\Gamma_N\n, we find the weak formulation:\n\n\nFind * \nu \\in V_{u_0}\n \nsuch that*\n\n \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0. \n\n\n\n\n1. Load modules\n\n\nTo start we load the following modules:\n\n\n\n\n\n\ndolfin: the python/C++ interface to FEniCS\n\n\n\n\n\n\nmath\n: the python module for mathematical functions\n\n\n\n\n\n\nnumpy\n: a python package for linear algebra\n\n\n\n\n\n\nmatplotlib\n: a python package used for plotting the results\n\n\n\n\n\n\nfrom dolfin import *\n\nimport math\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\n\n\n\n2. Define the mesh and the finite element space\n\n\nWe construct a triangulation (mesh) \n\\mathcal{T}_h\n of the computational domain \n\\Omega := [0, 1]^2\n with \nn\n elements in each direction.\n\n\nOn the mesh \n\\mathcal{T}_h\n, we then define the finite element space \nV_h \\subset H^1(\\Omega)\n consisting of globally continuous piecewise polinomials functions. The \ndegree\n variable defines the polinomial degree.\n\n\nn = 16\ndegree = 1\nmesh = UnitSquareMesh(n, n)\nnb.plot(mesh)\n\nVh  = FunctionSpace(mesh, 'Lagrange', degree)\nprint \ndim(Vh) = \n, Vh.dim()\n\n\n\n\ndim(Vh) =  289\n\n\n\n\n\n3. Define boundary labels\n\n\nTo partition the boundary of \n\\Omega\n in the subdomains \n\\Gamma_{\\rm top}\n, \n\\Gamma_{\\rm bottom}\n, \n\\Gamma_{\\rm left}\n, \n\\Gamma_{\\rm right}\n we assign a unique label \nboundary_parts\n to each of part of \n\\partial \\Omega\n.\n\n\nclass TopBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1] - 1) \n DOLFIN_EPS\n\nclass BottomBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1]) \n DOLFIN_EPS\n\nclass LeftBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0]) \n DOLFIN_EPS\n\nclass RightBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0] - 1) \n DOLFIN_EPS\n\nboundary_parts = FacetFunction(\nsize_t\n, mesh)\nboundary_parts.set_all(0)\n\nGamma_top = TopBoundary()\nGamma_top.mark(boundary_parts, 1)\nGamma_bottom = BottomBoundary()\nGamma_bottom.mark(boundary_parts, 2)\nGamma_left = LeftBoundary()\nGamma_left.mark(boundary_parts, 3)\nGamma_right = RightBoundary()\nGamma_right.mark(boundary_parts, 4)\n\n\n\n\n4. Define the coefficients of the PDE and the boundary conditions\n\n\nWe first define the coefficients of the PDE using the \nConstant\n and \nExpression\n classes. \nConstant\n is used to define coefficients that do not depend on the space coordinates, \nExpression\n is used to define coefficients that are a known function of the space coordinates \nx[0]\n (x-axis direction) and \nx[1]\n (y-axis direction).\n\n\nIn the finite element method community, Dirichlet boundary conditions are also known as \nessential\n boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class \nDirichletBC\n to indicate this type of condition.\n\n\nOn the other hand, Newman boundary conditions are also known as \nnatural\n boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure \nds[i]\n to integrate over the portion of the boundary marked with label \ni\n.\n\n\nu_L = Constant(0.)\nu_R = Constant(0.)\n\nsigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5)\nsigma_top    = Constant(0.)\n\nf = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5)\n\nbcs = [DirichletBC(Vh, u_L, boundary_parts, 3),\n       DirichletBC(Vh, u_R, boundary_parts, 4)]\n\nds = Measure(\nds\n, subdomain_data=boundary_parts)\n\n\n\n\n5. Define and solve the variational problem\n\n\nWe also define two special types of functions: the \nTrialFunction\n \nu\n and the \nTestFunction\n \nv\n. These special types of function are used by \nFEniCS\n to generate the finite element vectors and matrices which stem from the weak formulation of the PDE.\n\n\nMore specifically, by denoting by \n\\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)}\n the finite element basis for the space \nV_h\n, a function \nu_h \\in V_h\n can be written as\n\n u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x), \n\nwhere \n{\\rm u}_i\n represents the coefficients in the finite element expansion of \nu_h\n.\n\n\nWe then define\n\n\n\n\n\n\nthe bilinear form \na(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx \n;\n\n\n\n\n\n\nthe linear form \nL(v_h) = \\int_\\Omega f v_h dx + + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds \n.\n\n\n\n\n\n\nWe can then solve the variational problem\n\n\nFind \nu_h \\in V_h\n\n\n such that\n\n\n a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h \n\n\n\n\nusing directly the built-in \nsolve\n method in FEniCS.\n\n\nNOTE:\n As an alternative one can also assemble the finite element matrix \nA\n and the right hand side \nb\n that stems from the discretization of \na\n and \nL\n, and then solve the linear system\n\n A {\\rm u} = {\\rm b}, \n\nwhere\n\n\n\n\n\n\n\n\n{\\rm u}\n is the vector collecting the coefficient of the finite element expasion of \nu_h\n,\n\n\n\n\n\n\nthe entries of the matrix A are such that \nA_{ij} = a(\\phi_j, \\phi_i)\n,\n\n\n\n\n\n\nthe entries of the right hand side b are such that \nb_i = L(\\phi_i)\n.\n\n\n\n\n\n\nu = TrialFunction(Vh)\nv = TestFunction(Vh)\na = inner(nabla_grad(u), nabla_grad(v))*dx\nL = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n\nuh = Function(Vh)\n\n#solve(a == L, uh, bcs=bcs)\nA, b = assemble_system(a,L, bcs=bcs)\nsolve(A, uh.vector(), b, \ncg\n)\n\nnb.plot(uh)\n\n\n\n\n\n\n6. Compute the discretization error\n\n\nFor this problem, the exact solution is known.\nWe can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution \nu_h\n and the exact solution \nu_{\\rm ex}\n)\n\n \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx }, \n \nand\n\n \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}. \n\n\n\n\nu_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5)\ngrad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5)\n\nerr_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\nerr_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\nerr_H1 = sqrt( err_L2**2 + err_grad**2)\n\nprint \n|| u_h - u_e ||_L2 = \n, err_L2\nprint \n|| u_h - u_e ||_H1 = \n, err_H1\n\n\n\n\n|| u_h - u_e ||_L2 =  0.00880525372208\n|| u_h - u_e ||_H1 =  0.396718952514\n\n\n\n7. Convergence of the finite element method\n\n\nWe now verify numerically a well-known convergence result for the finite element method.\n\n\nLet denote with \ns\n the polynomial degree of the finite element space, and assume that the solution \nu_{\\rm ex}\n is at least in \nH^{s+1}(\\Omega)\n. Then we have\n\n \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}. \n\n\n\n\nIn the code below, the function \ncompute(n, degree)\n solves the PDE using a mesh with \nn\n elements in each direction and finite element spaces of polinomial order \ndegree\n.\n\n\nThe figure below shows the discretization errors in the \nH^1\n and \nL^2\n as a function of the mesh size \nh\n (\nh = \\frac{1}{n}\n) for piecewise linear (P1, \ns=1\n) and piecewise quadratic (P2, \ns=2\n) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular:\n\n\n\n\n\n\nfor piecewise linear finite element P1 we observe first order convergence in the \nH^1\n-norm and second order convergence in the \nL^2\n-norm;\n\n\n\n\n\n\nfor piecewise quadratic finite element P2 we observe second order convergence in the \nH^1\n-norm and third order convergence in the \nL^2\n-norm.\n\n\n\n\n\n\ndef compute(n, degree):\n    mesh = UnitSquareMesh(n, n)\n    Vh  = FunctionSpace(mesh, 'Lagrange', degree)\n    boundary_parts = FacetFunction(\nsize_t\n, mesh)\n    boundary_parts.set_all(0)\n\n    Gamma_top = TopBoundary()\n    Gamma_top.mark(boundary_parts, 1)\n    Gamma_bottom = BottomBoundary()\n    Gamma_bottom.mark(boundary_parts, 2)\n    Gamma_left = LeftBoundary()\n    Gamma_left.mark(boundary_parts, 3)\n    Gamma_right = RightBoundary()\n    Gamma_right.mark(boundary_parts, 4)\n\n    bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)]\n    ds = Measure(\nds\n, subdomain_data=boundary_parts)\n\n    u = TrialFunction(Vh)\n    v = TestFunction(Vh)\n    a = inner(nabla_grad(u), nabla_grad(v))*dx\n    L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n    uh = Function(Vh)\n    solve(a == L, uh, bcs=bcs)\n    err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\n    err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\n    err_H1 = sqrt( err_L2**2 + err_grad**2)\n\n    return err_L2, err_H1\n\nnref = 5\nn = 8*np.power(2,np.arange(0,nref))\nh = 1./n\n\nerr_L2_P1 = np.zeros(nref)\nerr_H1_P1 = np.zeros(nref)\nerr_L2_P2 = np.zeros(nref)\nerr_H1_P2 = np.zeros(nref)\n\nfor i in range(nref):\n    err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1)\n    err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nplt.loglog(h, err_H1_P1, '-or')\nplt.loglog(h, err_L2_P1, '-*b')\nplt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g')\nplt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k')\nplt.xlabel(\nMesh size h\n)\nplt.ylabel(\nError\n)\nplt.title(\nP1 Finite Element\n)\nplt.legend([\nH1 error\n, \nL2 error\n, \nFirst Order\n, \nSecond Order\n], 'lower right')\n\n\nplt.subplot(122)\nplt.loglog(h, err_H1_P2, '-or')\nplt.loglog(h, err_L2_P2, '-*b')\nplt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g')\nplt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k')\nplt.xlabel(\nMesh size h\n)\nplt.ylabel(\nError\n)\nplt.title(\nP2 Finite Element\n)\nplt.legend([\nH1 error\n, \nL2 error\n, \nSecond Order\n, \nThird Order\n], 'lower right')\n\nplt.show()\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "FEniCS101"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#fenics101-tutorial", 
            "text": "In this tutorial we consider the boundary value problem (BVP)   \\begin{eqnarray*}\n- \\nabla \\cdot (k \\nabla u) = f &      \\text{ in } \\Omega,\\\\\nu = u_0 & \\text{ on } \\Gamma_D = \\Gamma_{\\rm left} \\bigcup \\Gamma_{\\rm right},\\\\\nk \\frac{\\partial u}{\\partial {\\bf{n}}} = \\sigma & \\text{ on } \\Gamma_N = \\Gamma_{\\rm top} \\bigcup \\Gamma_{\\rm bottom},\n\\end{eqnarray*}   where  \\Omega = (0,1) \\times (0,1) ,  \\Gamma_D  and and  \\Gamma_N  are the union of\nthe left and right, and top and bottom boundaries of  \\Omega ,\nrespectively.  Here \\begin{eqnarray*}\nk(x,y) = 1  & \\text{ on } \\Omega\\\\\nf(x,y) = \\left(4\\pi^2+\\frac{\\pi^2}{4}\\right)\\sin(2 \\pi x) \\sin\\left(\\frac{\\pi}{2} y\\right) & \\text{ on } \\Omega\\\\\nu_0(x,y)      = 0 & \\text{ on } \\Gamma_D, \\\\\n\\sigma(x) = \\left\\{ \\begin{array}{l} -\\frac{\\pi}{2}\\sin(2 \\pi x) \\\\ 0 \\end{array}\\right.\n& \\begin{array}{l} \\text{ on } \\Gamma_{\\rm bottom},\\\\ \\text{ on } \\Gamma_{\\rm top}.\\end{array}\n\\end{eqnarray*}   The exact solution is  u_e(x,y) = \\sin(2\\pi x)\\sin\\left(\\frac{\\pi}{2}y\\right).", 
            "title": "FEniCS101 Tutorial"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#weak-formulation", 
            "text": "Let us define the Hilbert spaces  V_{u_0}, V_0 \\in \\Omega  as  V_{u_0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = u_0 \\text{ on } \\Gamma_D \\right\\},   V_{0} := \\left\\{ v \\in H^1(\\Omega) \\text{ s. t. } v = 0 \\text{ on } \\Gamma_D \\right\\}.   To obtain the weak formulation, we multiply the PDE by an arbitrary function  v \\in V_0  and integrate over the domain  \\Omega  leading to    -\\int_{\\Omega} \\nabla \\cdot (k \\nabla u) v \\, dx = \\int_\\Omega f v \\, dx\\quad \\forall \\; v \\in V_0.    Then, integration by parts the non-conforming term gives    \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx - \\int_{\\partial \\Omega} k \\frac{\\partial u}{\\partial {\\bf n} } v \\, ds = \\int_\\Omega f v \\, dx \\quad \\forall \\; v \\in V_0.    Finally by recalling that   v = 0   on  \\Gamma_D  and that  k \\frac{\\partial u}{\\partial {\\bf n} } = \\sigma   on  \\Gamma_N , we find the weak formulation:  Find *  u \\in V_{u_0}   such that*  \\int_{\\Omega} k \\nabla u \\cdot \\nabla v \\, dx = \\int_\\Omega f v \\, dx + \\int_{\\Gamma_N} \\sigma v \\, ds \\quad \\forall \\; v \\in V_0.", 
            "title": "Weak formulation"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#1-load-modules", 
            "text": "To start we load the following modules:    dolfin: the python/C++ interface to FEniCS    math : the python module for mathematical functions    numpy : a python package for linear algebra    matplotlib : a python package used for plotting the results    from dolfin import *\n\nimport math\nimport numpy as np\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)", 
            "title": "1. Load modules"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#2-define-the-mesh-and-the-finite-element-space", 
            "text": "We construct a triangulation (mesh)  \\mathcal{T}_h  of the computational domain  \\Omega := [0, 1]^2  with  n  elements in each direction.  On the mesh  \\mathcal{T}_h , we then define the finite element space  V_h \\subset H^1(\\Omega)  consisting of globally continuous piecewise polinomials functions. The  degree  variable defines the polinomial degree.  n = 16\ndegree = 1\nmesh = UnitSquareMesh(n, n)\nnb.plot(mesh)\n\nVh  = FunctionSpace(mesh, 'Lagrange', degree)\nprint  dim(Vh) =  , Vh.dim()  dim(Vh) =  289", 
            "title": "2. Define the mesh and the finite element space"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#3-define-boundary-labels", 
            "text": "To partition the boundary of  \\Omega  in the subdomains  \\Gamma_{\\rm top} ,  \\Gamma_{\\rm bottom} ,  \\Gamma_{\\rm left} ,  \\Gamma_{\\rm right}  we assign a unique label  boundary_parts  to each of part of  \\partial \\Omega .  class TopBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1] - 1)   DOLFIN_EPS\n\nclass BottomBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[1])   DOLFIN_EPS\n\nclass LeftBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0])   DOLFIN_EPS\n\nclass RightBoundary(SubDomain):\n    def inside(self, x, on_boundary):\n        return on_boundary and abs(x[0] - 1)   DOLFIN_EPS\n\nboundary_parts = FacetFunction( size_t , mesh)\nboundary_parts.set_all(0)\n\nGamma_top = TopBoundary()\nGamma_top.mark(boundary_parts, 1)\nGamma_bottom = BottomBoundary()\nGamma_bottom.mark(boundary_parts, 2)\nGamma_left = LeftBoundary()\nGamma_left.mark(boundary_parts, 3)\nGamma_right = RightBoundary()\nGamma_right.mark(boundary_parts, 4)", 
            "title": "3. Define boundary labels"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#4-define-the-coefficients-of-the-pde-and-the-boundary-conditions", 
            "text": "We first define the coefficients of the PDE using the  Constant  and  Expression  classes.  Constant  is used to define coefficients that do not depend on the space coordinates,  Expression  is used to define coefficients that are a known function of the space coordinates  x[0]  (x-axis direction) and  x[1]  (y-axis direction).  In the finite element method community, Dirichlet boundary conditions are also known as  essential  boundary conditions since they are imposed directly in the definition of the finite element space. In FEniCS, we use the class  DirichletBC  to indicate this type of condition.  On the other hand, Newman boundary conditions are also known as  natural  boundary conditions since they are weakly imposed as boundary integrals in the variational formulation (weak form). In FEniCS, we create a new boundary measure  ds[i]  to integrate over the portion of the boundary marked with label  i .  u_L = Constant(0.)\nu_R = Constant(0.)\n\nsigma_bottom = Expression('-(pi/2.0)*sin(2*pi*x[0])', degree=5)\nsigma_top    = Constant(0.)\n\nf = Expression('(4.0*pi*pi+pi*pi/4.0)*(sin(2*pi*x[0])*sin((pi/2.0)*x[1]))', degree=5)\n\nbcs = [DirichletBC(Vh, u_L, boundary_parts, 3),\n       DirichletBC(Vh, u_R, boundary_parts, 4)]\n\nds = Measure( ds , subdomain_data=boundary_parts)", 
            "title": "4. Define the coefficients of the PDE and the boundary conditions"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#5-define-and-solve-the-variational-problem", 
            "text": "We also define two special types of functions: the  TrialFunction   u  and the  TestFunction   v . These special types of function are used by  FEniCS  to generate the finite element vectors and matrices which stem from the weak formulation of the PDE.  More specifically, by denoting by  \\left[{\\phi_i(x)}\\right]_{i=1}^{{\\rm dim}(V_h)}  the finite element basis for the space  V_h , a function  u_h \\in V_h  can be written as  u_h = \\sum_{i=1}^{{\\rm dim}(V_h)} {\\rm u}_i \\phi_i(x),  \nwhere  {\\rm u}_i  represents the coefficients in the finite element expansion of  u_h .  We then define    the bilinear form  a(u_h, v_h) = \\int_\\Omega \\nabla u_h \\cdot \\nabla v_h dx  ;    the linear form  L(v_h) = \\int_\\Omega f v_h dx + + \\int_{\\Gamma_{\\rm top}} \\sigma_{\\rm top} v_h ds \\int_{\\Gamma_{\\rm bottom}} \\sigma_{\\rm bottom} v_h ds  .    We can then solve the variational problem  Find  u_h \\in V_h   such that   a(u_h, v_h) = L(v_h) \\quad \\forall\\, v_h \\in V_h    using directly the built-in  solve  method in FEniCS.  NOTE:  As an alternative one can also assemble the finite element matrix  A  and the right hand side  b  that stems from the discretization of  a  and  L , and then solve the linear system  A {\\rm u} = {\\rm b},  \nwhere     {\\rm u}  is the vector collecting the coefficient of the finite element expasion of  u_h ,    the entries of the matrix A are such that  A_{ij} = a(\\phi_j, \\phi_i) ,    the entries of the right hand side b are such that  b_i = L(\\phi_i) .    u = TrialFunction(Vh)\nv = TestFunction(Vh)\na = inner(nabla_grad(u), nabla_grad(v))*dx\nL = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n\nuh = Function(Vh)\n\n#solve(a == L, uh, bcs=bcs)\nA, b = assemble_system(a,L, bcs=bcs)\nsolve(A, uh.vector(), b,  cg )\n\nnb.plot(uh)", 
            "title": "5. Define and solve the variational problem"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#6-compute-the-discretization-error", 
            "text": "For this problem, the exact solution is known.\nWe can therefore compute the following norms of the discretization error (i.e. the of the difference between the finite element solution  u_h  and the exact solution  u_{\\rm ex} )  \\| u_{\\rm ex} - u_h \\|_{L^2{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx },   \nand  \\| u_{\\rm ex} - u_h \\|_{H^1{\\Omega}} := \\sqrt{ \\int_{\\Omega} (u_{\\rm ex} - u_h)^2 \\, dx + \\int_{\\Omega} |\\nabla u_{\\rm ex} - \\nabla u_h|^2 \\, dx}.    u_e = Expression('sin(2*pi*x[0])*sin((pi/2.0)*x[1])', degree=5)\ngrad_u_e = Expression( ('2*pi*cos(2*pi*x[0])*sin((pi/2.0)*x[1])', 'pi/2.0*sin(2*pi*x[0])*cos((pi/2.0)*x[1])'), degree=5)\n\nerr_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\nerr_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\nerr_H1 = sqrt( err_L2**2 + err_grad**2)\n\nprint  || u_h - u_e ||_L2 =  , err_L2\nprint  || u_h - u_e ||_H1 =  , err_H1  || u_h - u_e ||_L2 =  0.00880525372208\n|| u_h - u_e ||_H1 =  0.396718952514", 
            "title": "6. Compute the discretization error"
        }, 
        {
            "location": "/tutorials/1_FEniCS101/#7-convergence-of-the-finite-element-method", 
            "text": "We now verify numerically a well-known convergence result for the finite element method.  Let denote with  s  the polynomial degree of the finite element space, and assume that the solution  u_{\\rm ex}  is at least in  H^{s+1}(\\Omega) . Then we have  \\| u_{\\rm ex} - u_h \\|_{H^1} \\leq C h^{s}, \\quad \\| u_{\\rm ex} - u_h \\|_{L^2} \\leq C h^{s+1}.    In the code below, the function  compute(n, degree)  solves the PDE using a mesh with  n  elements in each direction and finite element spaces of polinomial order  degree .  The figure below shows the discretization errors in the  H^1  and  L^2  as a function of the mesh size  h  ( h = \\frac{1}{n} ) for piecewise linear (P1,  s=1 ) and piecewise quadratic (P2,  s=2 ) finite elements. We observe that numerical results are consistent with the finite element convergence theory. In particular:    for piecewise linear finite element P1 we observe first order convergence in the  H^1 -norm and second order convergence in the  L^2 -norm;    for piecewise quadratic finite element P2 we observe second order convergence in the  H^1 -norm and third order convergence in the  L^2 -norm.    def compute(n, degree):\n    mesh = UnitSquareMesh(n, n)\n    Vh  = FunctionSpace(mesh, 'Lagrange', degree)\n    boundary_parts = FacetFunction( size_t , mesh)\n    boundary_parts.set_all(0)\n\n    Gamma_top = TopBoundary()\n    Gamma_top.mark(boundary_parts, 1)\n    Gamma_bottom = BottomBoundary()\n    Gamma_bottom.mark(boundary_parts, 2)\n    Gamma_left = LeftBoundary()\n    Gamma_left.mark(boundary_parts, 3)\n    Gamma_right = RightBoundary()\n    Gamma_right.mark(boundary_parts, 4)\n\n    bcs = [DirichletBC(Vh, u_L, boundary_parts, 3), DirichletBC(Vh, u_R, boundary_parts, 4)]\n    ds = Measure( ds , subdomain_data=boundary_parts)\n\n    u = TrialFunction(Vh)\n    v = TestFunction(Vh)\n    a = inner(nabla_grad(u), nabla_grad(v))*dx\n    L = f*v*dx + sigma_top*v*ds(1) + sigma_bottom*v*ds(2)\n    uh = Function(Vh)\n    solve(a == L, uh, bcs=bcs)\n    err_L2 = sqrt( assemble( (uh-u_e)**2*dx ) )\n    err_grad = sqrt( assemble( inner(nabla_grad(uh) - grad_u_e, nabla_grad(uh) - grad_u_e)*dx ) )\n    err_H1 = sqrt( err_L2**2 + err_grad**2)\n\n    return err_L2, err_H1\n\nnref = 5\nn = 8*np.power(2,np.arange(0,nref))\nh = 1./n\n\nerr_L2_P1 = np.zeros(nref)\nerr_H1_P1 = np.zeros(nref)\nerr_L2_P2 = np.zeros(nref)\nerr_H1_P2 = np.zeros(nref)\n\nfor i in range(nref):\n    err_L2_P1[i], err_H1_P1[i] = compute(n[i], 1)\n    err_L2_P2[i], err_H1_P2[i] = compute(n[i], 2)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(121)\nplt.loglog(h, err_H1_P1, '-or')\nplt.loglog(h, err_L2_P1, '-*b')\nplt.loglog(h, h*.5*err_H1_P1[0]/h[0], '--g')\nplt.loglog(h, np.power(h,2)*.5*np.power( err_L2_P1[0]/h[0], 2), '-.k')\nplt.xlabel( Mesh size h )\nplt.ylabel( Error )\nplt.title( P1 Finite Element )\nplt.legend([ H1 error ,  L2 error ,  First Order ,  Second Order ], 'lower right')\n\n\nplt.subplot(122)\nplt.loglog(h, err_H1_P2, '-or')\nplt.loglog(h, err_L2_P2, '-*b')\nplt.loglog(h, np.power(h/h[0],2)*.5*err_H1_P2[0], '--g')\nplt.loglog(h, np.power(h/h[0],3)*.5*err_L2_P2[0], '-.k')\nplt.xlabel( Mesh size h )\nplt.ylabel( Error )\nplt.title( P2 Finite Element )\nplt.legend([ H1 error ,  L2 error ,  Second Order ,  Third Order ], 'lower right')\n\nplt.show()   Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "7. Convergence of the finite element method"
        }, 
        {
            "location": "/tutorials/2_PoissonDeterministic/", 
            "text": "Coefficient field inversion in an elliptic partial differential equation\n\n\nWe consider the estimation of a coefficient in an elliptic partial\ndifferential equation as a model problem. Depending on the\ninterpretation of the unknowns and the type of measurements, this\nmodel problem arises, for instance, in inversion for groundwater flow\nor heat conductivity.  It can also be interpreted as finding a\nmembrane with a certain spatially varying stiffness. Let\n\n\\Omega\\subset\\mathbb{R}^n\n, \nn\\in\\{1,2,3\\}\n be an open, bounded\ndomain and consider the following problem:\n\n\n\n\n\n\\min_{a} J(a):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx,\n\n\n\n\n\nwhere \nu\n is the solution of\n\n\n\n\n\n\\begin{split}\n\\quad -\\nabla\\cdot(\\exp(a)\\nabla u) &= f \\text{ in }\\Omega,\\\\\nu &= 0 \\text{ on }\\partial\\Omega.\n\\end{split}\n\n\n\n\n\nHere \na\\in U_{ad}:=\\{a\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\}\n the unknown coefficient field, \nu_d\n denotes (possibly noisy) data, \nf\\in H^{-1}(\\Omega)\n a given force, and \n\\gamma\\ge 0\n the regularization parameter.\n\n\nThe variational (or weak) form of the state equation:\n\n\nFind \nu\\in H_0^1(\\Omega)\n such that \n(\\exp(a)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega),\n\nwhere \nH_0^1(\\Omega)\n is the space of functions vanishing on \n\\partial\\Omega\n with square integrable derivatives. Here, \n(\\cdot\\,,\\cdot)\n denotes the \nL^2\n-inner product, i.e, for scalar functions \nu,v\n defined on \n\\Omega\n we denote \n(u,v) := \\int_\\Omega u(x) v(x) \\,dx\n.\n\n\nOptimality System:\n\n\nThe Lagrangian functional \n\\mathscr{L}:H^1(\\Omega)\\times H_0^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R}\n, which we use as a tool to derive the optimality system, is given by\n\n\n\n\n\n\\mathscr{L}(a,u,p):= \\frac{1}{2}(u-u_d,u-u_d) +\n\\frac{\\gamma}{2}(\\nabla a, \\nabla a) +  (\\exp(a)\\nabla u,\\nabla p) - (f,p).\n\n\n\n\n\nThe Lagrange multiplier theory shows that, at a solution all variations of the Lagrangian functional with respect to all variables must vanish. These variations of \n\\mathscr{L}\n with respect to \n(p,u,a)\n in directions \n(\\tilde{u}, \\tilde{p}, \\tilde{a})\n are given by\n\n\n\n\n\n  \\begin{alignat}{2}\n    \\mathscr{L}_p(a,u,p)(\\tilde{p})  &= (\\exp(a)\\nabla u, \\nabla \\tilde{p}) -\n    (f,\\tilde{p}) &&= 0,\\\\\n     \\mathscr{L}_u(a,u,p)(\\tilde{u}) &= (\\exp(a)\\nabla p, \\nabla \\tilde{u}) +\n     (u-u_d,\\tilde{u}) && = 0,\\\\\n     \\mathscr{L}_a(a,u,p)(\\tilde{a})  &= \\gamma(\\nabla a, \\nabla \\tilde{a}) +\n     (\\tilde{a}\\exp(a)\\nabla u, \\nabla p) &&= 0,\n  \\end{alignat}\n\n\n\n\n\nwhere the variations \n(\\tilde{u}, \\tilde{p}, \\tilde{a})\n are taken from the same spaces as \n(u,p,a)\n. \n\n\nThe gradient of the cost functional \n\\mathcal{J}(a)\n therefore is\n\n\n\n\n\n    \\mathcal{G}(a)(\\tilde a) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +\n     (\\tilde{a}\\exp(a)\\nabla u, \\nabla \\tilde{p}).\n\n\n\n\n\nInexact Newton-CG:\n\n\nNewton's method requires second-order variational derivatives of the Lagrangian . Written in abstract form, it computes an update direction \n(\\hat a_k, \\hat u_k,\\hat p_k)\n from the following Newton step for the Lagrangian functional:\n\n\n\n\n\n\\mathscr{L}''(a_k, u_k, p_k)\\left[(\\tilde\n  a, \\tilde u, \\tilde p),(\\hat a_k, \\hat u_k, \\hat p_k)\\right] =\n-\\mathscr{L}'(a_k,u_k,p_k)(\\tilde a, \\tilde u, \\tilde p),\n\n\n\n\n\nfor all variations \n(\\tilde a, \\tilde u, \\tilde p)\n, where \n\\mathscr{L}'\n and \n\\mathscr{L}''\n denote the first and\nsecond variations of the Lagrangian. For the elliptic parameter inversion problem, this Newton step (written in variatonal form) is as follows: Find \n(\\hat u_k, \\hat a_k,\\hat p_k)\n as the solution of the linear system\n\n\n\n\n\n  \\begin{array}{llll}\n    (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla\n    \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u,\n    \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla\n    p_k, \\nabla \\tilde u)\\\\\n    (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma\n    (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a\n     \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde\n      a  \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\\n    (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla\n      \\tilde p) & &= - (\\exp(a_k) \\nabla u_k,\n    \\nabla \\tilde p) + (f, \\tilde p),\n  \\end{array}\n\n\n\n\n\nfor all \n(\\tilde u, \\tilde a, \\tilde p)\n.\n\n\nDiscrete Newton system:\n\n\n\n\n\n\\def\\tu{\\tilde u}\n\\def\\btu{\\bf \\tilde u}\n\\def\\ta{\\tilde a}\n\\def\\bta{\\bf \\tilde a}\n\\def\\tp{\\tilde p}\n\\def\\btp{\\bf \\tilde p}\n\\def\\hu{\\hat u}\n\\def\\bhu{\\bf \\hat u}\n\\def\\ha{\\hat a}\n\\def\\bha{\\bf \\hat a}\n\\def\\hp{\\hat p}\n\\def\\bhp{\\bf \\hat p}\n\n\nThe discretized Newton step: denote the vectors corresponding to the discretization of the functions \n\\ha_k,\\hu_k, \\hp_k\n by \n\\bf \\bha_k, \\bhu_k\n and \n\\bhp_k\n. Then, the discretization of the above system is given by the following symmetric linear system:\n\n\n\n\n\n  \\begin{bmatrix}\n    \\bf W_{\\scriptsize\\mbox{uu}} & \\bf W_{\\scriptsize\\mbox{ua}} & \\bf A^T \\\\\n    \\bf W_{\\scriptsize\\mbox{au}} & \\bf R + \\bf R_{\\scriptsize\\mbox{aa}}& \\bf C^T \\\\\n    \\bf A & \\bf C & 0\n\\end{bmatrix}\n\\left[\n  \\begin{array}{c}\n    \\bhu_k \\\\\n    \\bha_k \\\\\n    \\bhp_k\n  \\end{array} \\right] =\n-\\left[\n  \\begin{array}{ccc}\n    \\bf{g}_u\\\\\n    \\bf{g}_a\\\\\n    \\bf{g}_p\n\\end{array}\n  \\right],\n\n\n\n\n\nwhere \n\\bf W_{\\scriptsize \\mbox{uu}}\n, \n\\bf W_{\\scriptsize\\mbox{ua}}\n, \n\\bf W_{\\scriptsize\\mbox{au}}\n, and \n\\bf R\n are the components of the Hessian matrix of the Lagrangian, \n\\bf A\n and \n\\bf C\n are the Jacobian of the state equation with respect to the state and the control variables, respectively and \n\\bf g_u\n, \n\\bf g_a\n, and \n\\bf g_p\n are the discrete gradients of the Lagrangian with respect to \n\\bf u \n, \n\\bf a\n and \n\\bf p\n, respectively.\n\n\nReduced Hessian apply:\n\n\nTo eliminate the incremental state and adjoint variables, \n\\bhu_k\n and \n\\bhp_k\n, from the first and last equations we use\n\n\n\n\n\n\\begin{align}\n\\bhu_k &= -\\bf A^{-1} \\bf C \\, \\bha_k,\\\\\n\\bhp_k &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu_k +\n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha_k).\n\\end{align}\n\n\n\n\n\nThis results in the following reduced linear system for the Newton step\n\n\n\n\n\n  \\bf H \\, \\bha_k = -\\bf{g}_a,\n\n\n\n\n\nwith the reduced Hessian \n\\bf H\n applied to a vector \n\\bha\n given by\n\n\n\n\n\n  \\bf H \\bha = \\underbrace{(\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})}_{\\text{Hessian of the regularization}} \\bha +\n    \\underbrace{(\\bf C^{T}\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}}\n    \\bf A^{-1} \\bf C - \\bf W_{\\scriptsize\\mbox{ua}}) -\n    \\bf W_{\\scriptsize\\mbox{au}} \\bf A^{-1}\n    \\bf C)}_{\\text{Hessian of the data misfit}}\\;\\bha.\n\n\n\n\n\nGoals:\n\n\nBy the end of this notebook, you should be able to:\n\n\n\n\nsolve the forward and adjoint Poisson equations\n\n\nunderstand the inverse method framework\n\n\nvisualise and understand the results\n\n\nmodify the problem and code\n\n\n\n\nMathematical tools used:\n\n\n\n\nFinite element method\n\n\nDerivation of gradiant and Hessian via the adjoint method\n\n\ninexact Newton-CG\n\n\nArmijo line search\n\n\n\n\nList of software used:\n\n\n\n\nFEniCS\n, a parallel finite element element library for the discretization of partial differential equations\n\n\nPETSc\n, for scalable and efficient linear algebra operations and solvers\n\n\nMatplotlib\n, a python package used for plotting the results\n\n\nNumpy\n, a python package for linear algebra\n\n\n\n\nSet up\n\n\nImport dependencies\n\n\nfrom dolfin import *\n\nimport sys\nsys.path.append( \n../\n )\nfrom hippylib import *\n\nimport numpy as np\nimport time\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nstart = time.clock()\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\nnp.random.seed(seed=1)\n\n\n\n\nModel set up:\n\n\nAs in the introduction, the first thing we need to do is set up the numerical model.  In this cell, we set the mesh, the finite element functions \nu, p, g\n corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization.\n\n\n# create mesh and define function spaces\nnx = 64\nny = 64\nmesh = UnitSquareMesh(nx, ny)\nVa = FunctionSpace(mesh, 'Lagrange', 1)\nVu = FunctionSpace(mesh, 'Lagrange', 2)\n\n# The true and inverted parameter\natrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5) \n 0.2))', degree=5),Va)\na = interpolate(Expression(\nlog(2.0)\n, degree=1),Va)\n\n# define function for state and adjoint\nu = Function(Vu)\np = Function(Vu)\n\n# define Trial and Test Functions\nu_trial, p_trial, a_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Va)\nu_test, p_test, a_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Va)\n\n# initialize input functions\nf = Constant(\n1.0\n)\nu0 = Constant(\n0.0\n)\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(mesh,subplot_loc=121, mytitle=\nMesh\n, show_axis='on')\nnb.plot(atrue,subplot_loc=122, mytitle=\nTrue parameter field\n)\nplt.show()\n\n\n\n\n\n\n# set up dirichlet boundary conditions\ndef boundary(x,on_boundary):\n    return on_boundary\n\nbc_state = DirichletBC(Vu, u0, boundary)\nbc_adj = DirichletBC(Vu, Constant(0.), boundary)\n\n\n\n\nSet up synthetic observations:\n\n\n\n\nPropose a coefficient field \na_{\\text true}\n shown above\n\n\n\n\nThe weak form of the pde: \n    Find \nu\\in H_0^1(\\Omega)\n such that \n\\underbrace{(\\exp(a_{\\text true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega)\n.\n\n\n\n\n\n\nPerturb the solution: \nu = u + \\eta\n, where \n\\eta \\sim \\mathcal{N}(0, \\sigma)\n\n\n\n\n\n\n\n\n# noise level\nnoise_level = 0.05\n\n# weak form for setting up the synthetic observations\na_goal = inner(exp(atrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_goal = f * u_test * dx\n\n# solve the forward/state problem to generate synthetic observations\ngoal_A, goal_b = assemble_system(a_goal, L_goal, bc_state)\n\nutrue = Function(Vu)\nsolve(goal_A, utrue.vector(), goal_b)\n\nud = Function(Vu)\nud.assign(utrue)\n\n# perturb state solution and create synthetic measurements ud\n# ud = u + ||u||/SNR * random.normal\nMAX = ud.vector().norm(\nlinf\n)\nnoise = Vector()\ngoal_A.init_vector(noise,1)\nnoise.set_local( noise_level * MAX * np.random.normal(0, 1, len(ud.vector().array())) )\nbc_adj.apply(noise)\n\nud.vector().axpy(1., noise)\n\n# plot\nnb.multi1_plot([utrue, ud], [\nState solution with atrue\n, \nSynthetic observations\n])\nplt.show()\n\n\n\n\n\n\nThe cost function evaluation:\n\n\n\n\n\nJ(a):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text misfit} + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx}_{\\text reg}\n\n\n\n\n\nIn the code below, \nW\n and \nR\n are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively.\n\n\n# regularization parameter\ngamma = 1e-8\n\n# weak for for setting up the misfit and regularization compoment of the cost\nW_equ   = inner(u_trial, u_test) * dx\nR_equ   = gamma * inner(nabla_grad(a_trial), nabla_grad(a_test)) * dx\n\nW = assemble(W_equ)\nR = assemble(R_equ)\n\n# refine cost function\ndef cost(u, ud, a, W, R):\n    diff = u.vector() - ud.vector()\n    reg = 0.5 * a.vector().inner(R*a.vector() ) \n    misfit = 0.5 * diff.inner(W * diff)\n    return [reg + misfit, misfit, reg]\n\n\n\n\nSetting up the state equations, right hand side for the adjoint and the necessary matrices:\n\n\n\n\n\n  \\begin{array}{llll}\n    (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla\n    \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u,\n    \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla\n    p_k, \\nabla \\tilde u)\\\\\n    (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma\n    (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a\n     \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde\n      a  \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\\n    (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla\n      \\tilde p) & &= - (\\exp(a_k) \\nabla u_k,\n    \\nabla \\tilde p) + (f, \\tilde p),\n  \\end{array}\n\n\n\n\n\n# weak form for setting up the state equation\na_state = inner(exp(a) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_state = f * u_test * dx\n\n# weak form for setting up the adjoint equation\na_adj = inner(exp(a) * nabla_grad(p_trial), nabla_grad(p_test)) * dx\nL_adj = -inner(u - ud, p_test) * dx\n\n# weak form for setting up matrices\nWua_equ = inner(exp(a) * a_trial * nabla_grad(p_test), nabla_grad(p)) * dx\nC_equ   = inner(exp(a) * a_trial * nabla_grad(u), nabla_grad(u_test)) * dx\nRaa_equ = inner(exp(a) * a_trial * a_test *  nabla_grad(u),  nabla_grad(p)) * dx\n\nM_equ   = inner(a_trial, a_test) * dx\n\n# assemble matrix M\nM = assemble(M_equ)\n\n\n\n\nInitial guess\n\n\nWe solve the state equation and compute the cost functional for the initial guess of the parameter \na_ini\n\n\n# solve state equation\nstate_A, state_b = assemble_system (a_state, L_state, bc_state)\nsolve (state_A, u.vector(), state_b)\n\n# evaluate cost\n[cost_old, misfit_old, reg_old] = cost(u, ud, a, W, R)\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(a,subplot_loc=121, mytitle=\na_ini\n, vmin=atrue.vector().min(), vmax=atrue.vector().max())\nnb.plot(u,subplot_loc=122, mytitle=\nu(a_ini)\n)\nplt.show()\n\n\n\n\n\n\nThe reduced Hessian apply to a vector v:\n\n\nHere we describe how to apply the reduced Hessian operator to a vector v. For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined.\n\n\nFor this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm.\n\n\nThe Hessian apply reads:\n\n\n\\begin{align}\n\\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu +\n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha) & \\text{adjoint}\\\\\n\\bf H \\bf v &= (\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})\\bf v + \\bf C^T \\bhp + \\bf W_{\\scriptsize\\mbox{au}} \\bhu.\n\\end{align}\n\n\n\n\n\nThe Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators \n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha\n, \n\\bf R_{\\scriptsize\\mbox{aa}}\\bf v\n, and \n\\bf W_{\\scriptsize\\mbox{au}} \\bhu\n:\n\n\n\\begin{align}\n\\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bf A^{-T} \\bf W_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\\n\\bf H_{\\rm GN} \\bf v &= \\bf R \\bf v + \\bf C^T \\bhp.\n\\end{align}\n\n\n\n\n\n# Class HessianOperator to perform Hessian apply to a vector\nclass HessianOperator():\n    cgiter = 0\n    def __init__(self, R, Raa, C, A, adj_A, W, Wua, use_gaussnewton=False):\n        self.R = R\n        self.Raa = Raa\n        self.C = C\n        self.A = A\n        self.adj_A = adj_A\n        self.W = W\n        self.Wua = Wua\n        self.use_gaussnewton = use_gaussnewton\n\n        # incremental state\n        self.du = Vector()\n        self.A.init_vector(self.du,0)\n\n        #incremental adjoint\n        self.dp = Vector()\n        self.adj_A.init_vector(self.dp,0)\n\n        # auxiliary vectors\n        self.CT_dp = Vector()\n        self.C.init_vector(self.CT_dp, 1)\n        self.Wua_du = Vector()\n        self.Wua.init_vector(self.Wua_du, 1)\n\n    def init_vector(self, v, dim):\n        self.R.init_vector(v,dim)\n\n    # Hessian performed on x, output as generic vector y\n    def mult(self, v, y):\n        self.cgiter += 1\n        y.zero()\n        if self.use_gaussnewton:\n            self.mult_GaussNewton(v,y)\n        else:\n            self.mult_Newton(v,y)\n\n    # define (Gauss-Newton) Hessian apply H * v\n    def mult_GaussNewton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = - (self.W * self.du)\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        # Reg/Prior term\n        self.R.mult(v,y)\n\n        # Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1, self.CT_dp)\n\n    # define (Newton) Hessian apply H * v\n    def mult_Newton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = -(self.W * self.du) -  self.Wua * v\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        #Reg/Prior term\n        self.R.mult(v,y)\n        y.axpy(1., Raa*v)\n\n        #Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1., self.CT_dp)\n        self.Wua.transpmult(self.du, self.Wua_du)\n        y.axpy(1., self.Wua_du)\n\n\n\n\nThe inexact Newton-CG optimization with Armijo line search:\n\n\nWe solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search.\n\n\nThe stopping criterion is based on a relative reduction of the norm of the gradient (i.e. \n\\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau\n).\n\n\nFirst, we compute the gradient by solving the state and adjoint equation for the current parameter \na\n, and then substituing the current state \nu\n, parameter \na\n and adjoint \np\n variables in the weak form expression of the gradient:\n\n (g, \\tilde{a}) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +(\\tilde{a}\\nabla u, \\nabla p).\n\n\n\n\nThen, we compute the Newton direction \n\\delta a\n by iteratively solving \n{\\bf H} {\\delta a} = - {\\bf g}\n.\nThe Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug  (to avoid negative curvature) criteria.\n\n\nFinally, the Armijo line search uses backtracking to find \n\\alpha\n such that a sufficient reduction in the cost functional is achieved.\nMore specifically, we use backtracking to find \n\\alpha\n such that:\n\nJ( a + \\alpha \\delta a ) \\leq J(a) + \\alpha c_{\\rm armijo} (\\delta a,g). \n\n\n\n\n# define parameters for the optimization\ntol = 1e-8\nc = 1e-4\nmaxiter = 12\nplot_on = False\n\n# initialize iter counters\niter = 1\ntotal_cg_iter = 0\nconverged = False\n\n# initializations\ng, a_delta = Vector(), Vector()\nR.init_vector(a_delta,0)\nR.init_vector(g,0)\n\na_prev = Function(Va)\n\nprint \nNit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\n\n\nwhile iter \n  maxiter and not converged:\n\n    # assemble matrix C\n    C =  assemble(C_equ)\n\n    # solve the adoint problem\n    adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj)\n    solve(adjoint_A, p.vector(), adjoint_RHS)\n\n    # assemble W_ua and R\n    Wua = assemble (Wua_equ)\n    Raa = assemble (Raa_equ)\n\n    # evaluate the  gradient\n    CT_p = Vector()\n    C.init_vector(CT_p,1)\n    C.transpmult(p.vector(), CT_p)\n    MG = CT_p + R * a.vector()\n    solve(M, g, MG)\n\n    # calculate the norm of the gradient\n    grad2 = g.inner(MG)\n    gradnorm = sqrt(grad2)\n\n    # set the CG tolerance (use Eisenstat\u2013Walker termination criterion)\n    if iter == 1:\n        gradnorm_ini = gradnorm\n    tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini))\n\n    # define the Hessian apply operator (with preconditioner)\n    Hess_Apply = HessianOperator(R, Raa, C, state_A, adjoint_A, W, Wua, use_gaussnewton=(iter\n6) )\n    P = R + gamma * M\n    Psolver = PETScKrylovSolver(\ncg\n, amg_method())\n    Psolver.set_operator(P)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(Hess_Apply)\n    solver.set_preconditioner(Psolver)\n    solver.parameters[\nrel_tolerance\n] = tolcg\n    solver.parameters[\nzero_initial_guess\n] = True\n    solver.parameters[\nprint_level\n] = -1\n\n    # solve the Newton system H a_delta = - MG\n    solver.solve(a_delta, -MG)\n    total_cg_iter += Hess_Apply.cgiter\n\n    # linesearch\n    alpha = 1\n    descent = 0\n    no_backtrack = 0\n    a_prev.assign(a)\n    while descent == 0 and no_backtrack \n 10:\n        a.vector().axpy(alpha, a_delta )\n\n        # solve the state/forward problem\n        state_A, state_b = assemble_system(a_state, L_state, bc_state)\n        solve(state_A, u.vector(), state_b)\n\n        # evaluate cost\n        [cost_new, misfit_new, reg_new] = cost(u, ud, a, W, R)\n\n        # check if Armijo conditions are satisfied\n        if cost_new \n cost_old + alpha * c * MG.inner(a_delta):\n            cost_old = cost_new\n            descent = 1\n        else:\n            no_backtrack += 1\n            alpha *= 0.5\n            a.assign(a_prev)  # reset a\n\n    # calculate sqrt(-G * D)\n    graddir = sqrt(- MG.inner(a_delta) )\n\n    sp = \n\n    print \n%2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e\n % \\\n        (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\\n         graddir, sp, gradnorm, sp, alpha, sp, tolcg)\n\n    if plot_on:\n        nb.multi1_plot([a,u,p], [\na\n,\nu\n,\np\n], same_colorbar=False)\n        plt.show()\n\n    # check for convergence\n    if gradnorm \n tol and iter \n 1:\n        converged = True\n        print \nNewton's method converged in \n,iter,\n  iterations\n\n        print \nTotal number of CG iterations: \n, total_cg_iter\n\n    iter += 1\n\nif not converged:\n    print \nNewton's method did not converge in \n, maxiter, \n iterations\n\n\nprint \nTime elapsed: \n, time.clock()-start\n\n\n\n\nNit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\n 1     1     1.12708e-05   1.12708e-05   1.33979e-11   1.56540e-02   3.79427e-04    1.00   5.000e-01\n 2     1     7.79732e-07   7.79695e-07   3.67737e-11   4.68278e-03   5.35002e-05    1.00   3.755e-01\n 3     1     3.10620e-07   3.10571e-07   4.91259e-11   9.71633e-04   7.13895e-06    1.00   1.372e-01\n 4     5     1.92183e-07   1.62405e-07   2.97780e-08   4.51694e-04   1.00276e-06    1.00   5.141e-02\n 5     1     1.86913e-07   1.57119e-07   2.97941e-08   1.02668e-04   6.12750e-07    1.00   4.019e-02\n 6    12     1.80408e-07   1.37719e-07   4.26890e-08   1.15975e-04   2.24111e-07    1.00   2.430e-02\n 7     5     1.80331e-07   1.38935e-07   4.13963e-08   1.23223e-05   4.17399e-08    1.00   1.049e-02\n 8    15     1.80330e-07   1.39056e-07   4.12734e-08   1.74451e-06   3.43216e-09    1.00   3.008e-03\nNewton's method converged in  8   iterations\nTotal number of CG iterations:  41\nTime elapsed:  9.28313\n\n\n\nnb.multi1_plot([atrue, a], [\natrue\n, \na\n])\nnb.multi1_plot([u,p], [\nu\n,\np\n], same_colorbar=False)\nplt.show()\n\n\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "Poisson Deterministic"
        }, 
        {
            "location": "/tutorials/2_PoissonDeterministic/#coefficient-field-inversion-in-an-elliptic-partial-differential-equation", 
            "text": "We consider the estimation of a coefficient in an elliptic partial\ndifferential equation as a model problem. Depending on the\ninterpretation of the unknowns and the type of measurements, this\nmodel problem arises, for instance, in inversion for groundwater flow\nor heat conductivity.  It can also be interpreted as finding a\nmembrane with a certain spatially varying stiffness. Let \\Omega\\subset\\mathbb{R}^n ,  n\\in\\{1,2,3\\}  be an open, bounded\ndomain and consider the following problem:   \n\\min_{a} J(a):=\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx + \\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx,   where  u  is the solution of   \n\\begin{split}\n\\quad -\\nabla\\cdot(\\exp(a)\\nabla u) &= f \\text{ in }\\Omega,\\\\\nu &= 0 \\text{ on }\\partial\\Omega.\n\\end{split}   Here  a\\in U_{ad}:=\\{a\\in H^1(\\Omega) \\bigcap L^{\\infty}(\\Omega)\\}  the unknown coefficient field,  u_d  denotes (possibly noisy) data,  f\\in H^{-1}(\\Omega)  a given force, and  \\gamma\\ge 0  the regularization parameter.  The variational (or weak) form of the state equation:  Find  u\\in H_0^1(\\Omega)  such that  (\\exp(a)\\nabla u,\\nabla v) - (f,v) = 0, \\text{ for all } v\\in H_0^1(\\Omega), \nwhere  H_0^1(\\Omega)  is the space of functions vanishing on  \\partial\\Omega  with square integrable derivatives. Here,  (\\cdot\\,,\\cdot)  denotes the  L^2 -inner product, i.e, for scalar functions  u,v  defined on  \\Omega  we denote  (u,v) := \\int_\\Omega u(x) v(x) \\,dx .  Optimality System:  The Lagrangian functional  \\mathscr{L}:H^1(\\Omega)\\times H_0^1(\\Omega)\\times H_0^1(\\Omega)\\rightarrow \\mathbb{R} , which we use as a tool to derive the optimality system, is given by   \n\\mathscr{L}(a,u,p):= \\frac{1}{2}(u-u_d,u-u_d) +\n\\frac{\\gamma}{2}(\\nabla a, \\nabla a) +  (\\exp(a)\\nabla u,\\nabla p) - (f,p).   The Lagrange multiplier theory shows that, at a solution all variations of the Lagrangian functional with respect to all variables must vanish. These variations of  \\mathscr{L}  with respect to  (p,u,a)  in directions  (\\tilde{u}, \\tilde{p}, \\tilde{a})  are given by   \n  \\begin{alignat}{2}\n    \\mathscr{L}_p(a,u,p)(\\tilde{p})  &= (\\exp(a)\\nabla u, \\nabla \\tilde{p}) -\n    (f,\\tilde{p}) &&= 0,\\\\\n     \\mathscr{L}_u(a,u,p)(\\tilde{u}) &= (\\exp(a)\\nabla p, \\nabla \\tilde{u}) +\n     (u-u_d,\\tilde{u}) && = 0,\\\\\n     \\mathscr{L}_a(a,u,p)(\\tilde{a})  &= \\gamma(\\nabla a, \\nabla \\tilde{a}) +\n     (\\tilde{a}\\exp(a)\\nabla u, \\nabla p) &&= 0,\n  \\end{alignat}   where the variations  (\\tilde{u}, \\tilde{p}, \\tilde{a})  are taken from the same spaces as  (u,p,a) .   The gradient of the cost functional  \\mathcal{J}(a)  therefore is   \n    \\mathcal{G}(a)(\\tilde a) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +\n     (\\tilde{a}\\exp(a)\\nabla u, \\nabla \\tilde{p}).   Inexact Newton-CG:  Newton's method requires second-order variational derivatives of the Lagrangian . Written in abstract form, it computes an update direction  (\\hat a_k, \\hat u_k,\\hat p_k)  from the following Newton step for the Lagrangian functional:   \n\\mathscr{L}''(a_k, u_k, p_k)\\left[(\\tilde\n  a, \\tilde u, \\tilde p),(\\hat a_k, \\hat u_k, \\hat p_k)\\right] =\n-\\mathscr{L}'(a_k,u_k,p_k)(\\tilde a, \\tilde u, \\tilde p),   for all variations  (\\tilde a, \\tilde u, \\tilde p) , where  \\mathscr{L}'  and  \\mathscr{L}''  denote the first and\nsecond variations of the Lagrangian. For the elliptic parameter inversion problem, this Newton step (written in variatonal form) is as follows: Find  (\\hat u_k, \\hat a_k,\\hat p_k)  as the solution of the linear system   \n  \\begin{array}{llll}\n    (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla\n    \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u,\n    \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla\n    p_k, \\nabla \\tilde u)\\\\\n    (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma\n    (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a\n     \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde\n      a  \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\\n    (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla\n      \\tilde p) & &= - (\\exp(a_k) \\nabla u_k,\n    \\nabla \\tilde p) + (f, \\tilde p),\n  \\end{array}   for all  (\\tilde u, \\tilde a, \\tilde p) .  Discrete Newton system:   \n\\def\\tu{\\tilde u}\n\\def\\btu{\\bf \\tilde u}\n\\def\\ta{\\tilde a}\n\\def\\bta{\\bf \\tilde a}\n\\def\\tp{\\tilde p}\n\\def\\btp{\\bf \\tilde p}\n\\def\\hu{\\hat u}\n\\def\\bhu{\\bf \\hat u}\n\\def\\ha{\\hat a}\n\\def\\bha{\\bf \\hat a}\n\\def\\hp{\\hat p}\n\\def\\bhp{\\bf \\hat p} \nThe discretized Newton step: denote the vectors corresponding to the discretization of the functions  \\ha_k,\\hu_k, \\hp_k  by  \\bf \\bha_k, \\bhu_k  and  \\bhp_k . Then, the discretization of the above system is given by the following symmetric linear system:   \n  \\begin{bmatrix}\n    \\bf W_{\\scriptsize\\mbox{uu}} & \\bf W_{\\scriptsize\\mbox{ua}} & \\bf A^T \\\\\n    \\bf W_{\\scriptsize\\mbox{au}} & \\bf R + \\bf R_{\\scriptsize\\mbox{aa}}& \\bf C^T \\\\\n    \\bf A & \\bf C & 0\n\\end{bmatrix}\n\\left[\n  \\begin{array}{c}\n    \\bhu_k \\\\\n    \\bha_k \\\\\n    \\bhp_k\n  \\end{array} \\right] =\n-\\left[\n  \\begin{array}{ccc}\n    \\bf{g}_u\\\\\n    \\bf{g}_a\\\\\n    \\bf{g}_p\n\\end{array}\n  \\right],   where  \\bf W_{\\scriptsize \\mbox{uu}} ,  \\bf W_{\\scriptsize\\mbox{ua}} ,  \\bf W_{\\scriptsize\\mbox{au}} , and  \\bf R  are the components of the Hessian matrix of the Lagrangian,  \\bf A  and  \\bf C  are the Jacobian of the state equation with respect to the state and the control variables, respectively and  \\bf g_u ,  \\bf g_a , and  \\bf g_p  are the discrete gradients of the Lagrangian with respect to  \\bf u  ,  \\bf a  and  \\bf p , respectively.  Reduced Hessian apply:  To eliminate the incremental state and adjoint variables,  \\bhu_k  and  \\bhp_k , from the first and last equations we use   \n\\begin{align}\n\\bhu_k &= -\\bf A^{-1} \\bf C \\, \\bha_k,\\\\\n\\bhp_k &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu_k +\n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha_k).\n\\end{align}   This results in the following reduced linear system for the Newton step   \n  \\bf H \\, \\bha_k = -\\bf{g}_a,   with the reduced Hessian  \\bf H  applied to a vector  \\bha  given by   \n  \\bf H \\bha = \\underbrace{(\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})}_{\\text{Hessian of the regularization}} \\bha +\n    \\underbrace{(\\bf C^{T}\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}}\n    \\bf A^{-1} \\bf C - \\bf W_{\\scriptsize\\mbox{ua}}) -\n    \\bf W_{\\scriptsize\\mbox{au}} \\bf A^{-1}\n    \\bf C)}_{\\text{Hessian of the data misfit}}\\;\\bha.   Goals:  By the end of this notebook, you should be able to:   solve the forward and adjoint Poisson equations  understand the inverse method framework  visualise and understand the results  modify the problem and code   Mathematical tools used:   Finite element method  Derivation of gradiant and Hessian via the adjoint method  inexact Newton-CG  Armijo line search   List of software used:   FEniCS , a parallel finite element element library for the discretization of partial differential equations  PETSc , for scalable and efficient linear algebra operations and solvers  Matplotlib , a python package used for plotting the results  Numpy , a python package for linear algebra", 
            "title": "Coefficient field inversion in an elliptic partial differential equation"
        }, 
        {
            "location": "/tutorials/2_PoissonDeterministic/#set-up", 
            "text": "Import dependencies  from dolfin import *\n\nimport sys\nsys.path.append(  ../  )\nfrom hippylib import *\n\nimport numpy as np\nimport time\nimport logging\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nstart = time.clock()\n\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\nset_log_active(False)\n\nnp.random.seed(seed=1)  Model set up:  As in the introduction, the first thing we need to do is set up the numerical model.  In this cell, we set the mesh, the finite element functions  u, p, g  corresponding to state, adjoint and coefficient/gradient variables, and the corresponding test functions and the parameters for the optimization.  # create mesh and define function spaces\nnx = 64\nny = 64\nmesh = UnitSquareMesh(nx, ny)\nVa = FunctionSpace(mesh, 'Lagrange', 1)\nVu = FunctionSpace(mesh, 'Lagrange', 2)\n\n# The true and inverted parameter\natrue = interpolate(Expression('log(2 + 7*(pow(pow(x[0] - 0.5,2) + pow(x[1] - 0.5,2),0.5)   0.2))', degree=5),Va)\na = interpolate(Expression( log(2.0) , degree=1),Va)\n\n# define function for state and adjoint\nu = Function(Vu)\np = Function(Vu)\n\n# define Trial and Test Functions\nu_trial, p_trial, a_trial = TrialFunction(Vu), TrialFunction(Vu), TrialFunction(Va)\nu_test, p_test, a_test = TestFunction(Vu), TestFunction(Vu), TestFunction(Va)\n\n# initialize input functions\nf = Constant( 1.0 )\nu0 = Constant( 0.0 )\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(mesh,subplot_loc=121, mytitle= Mesh , show_axis='on')\nnb.plot(atrue,subplot_loc=122, mytitle= True parameter field )\nplt.show()   # set up dirichlet boundary conditions\ndef boundary(x,on_boundary):\n    return on_boundary\n\nbc_state = DirichletBC(Vu, u0, boundary)\nbc_adj = DirichletBC(Vu, Constant(0.), boundary)  Set up synthetic observations:   Propose a coefficient field  a_{\\text true}  shown above   The weak form of the pde: \n    Find  u\\in H_0^1(\\Omega)  such that  \\underbrace{(\\exp(a_{\\text true})\\nabla u,\\nabla v)}_{\\; := \\; a_{pde}} - \\underbrace{(f,v)}_{\\; := \\;L_{pde}} = 0, \\text{ for all } v\\in H_0^1(\\Omega) .    Perturb the solution:  u = u + \\eta , where  \\eta \\sim \\mathcal{N}(0, \\sigma)     # noise level\nnoise_level = 0.05\n\n# weak form for setting up the synthetic observations\na_goal = inner(exp(atrue) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_goal = f * u_test * dx\n\n# solve the forward/state problem to generate synthetic observations\ngoal_A, goal_b = assemble_system(a_goal, L_goal, bc_state)\n\nutrue = Function(Vu)\nsolve(goal_A, utrue.vector(), goal_b)\n\nud = Function(Vu)\nud.assign(utrue)\n\n# perturb state solution and create synthetic measurements ud\n# ud = u + ||u||/SNR * random.normal\nMAX = ud.vector().norm( linf )\nnoise = Vector()\ngoal_A.init_vector(noise,1)\nnoise.set_local( noise_level * MAX * np.random.normal(0, 1, len(ud.vector().array())) )\nbc_adj.apply(noise)\n\nud.vector().axpy(1., noise)\n\n# plot\nnb.multi1_plot([utrue, ud], [ State solution with atrue ,  Synthetic observations ])\nplt.show()   The cost function evaluation:   \nJ(a):=\\underbrace{\\frac{1}{2}\\int_\\Omega (u-u_d)^2\\, dx}_{\\text misfit} + \\underbrace{\\frac{\\gamma}{2}\\int_\\Omega|\\nabla a|^2\\,dx}_{\\text reg}   In the code below,  W  and  R  are symmetric positive definite matrices that stem from finite element discretization of the misfit and regularization component of the cost functional, respectively.  # regularization parameter\ngamma = 1e-8\n\n# weak for for setting up the misfit and regularization compoment of the cost\nW_equ   = inner(u_trial, u_test) * dx\nR_equ   = gamma * inner(nabla_grad(a_trial), nabla_grad(a_test)) * dx\n\nW = assemble(W_equ)\nR = assemble(R_equ)\n\n# refine cost function\ndef cost(u, ud, a, W, R):\n    diff = u.vector() - ud.vector()\n    reg = 0.5 * a.vector().inner(R*a.vector() ) \n    misfit = 0.5 * diff.inner(W * diff)\n    return [reg + misfit, misfit, reg]  Setting up the state equations, right hand side for the adjoint and the necessary matrices:   \n  \\begin{array}{llll}\n    (\\hat{u}_k, \\tilde u) &+ (\\hat{a}_k \\exp(a_k)\\nabla p_k, \\nabla\n    \\tilde u) &+ (\\exp(a_k) \\nabla \\tilde u,\n    \\nabla \\hat p_k) &= (u_d - u_k, \\tilde u)- (\\exp(a_k) \\nabla\n    p_k, \\nabla \\tilde u)\\\\\n    (\\tilde a \\exp(a_k) \\nabla \\hat u_k, \\nabla p_k) &+ \\gamma\n    (\\nabla \\hat a_k, \\nabla \\tilde a) + (\\tilde a \\hat a_k \\exp(a_k)\\nabla u, \\nabla p) &+ (\\tilde a\n     \\exp(a_k) \\nabla u_k, \\nabla \\hat p_k) &= - \\gamma (\\nabla a_k, \\nabla\\tilde a) - (\\tilde\n      a  \\exp(a_k) \\nabla u_k, \\nabla p_k)\\\\\n    (\\exp(a_k) \\nabla \\hat u_k, \\nabla \\tilde p) &+ (\\hat a_k \\exp(a_k) \\nabla u_k, \\nabla\n      \\tilde p) & &= - (\\exp(a_k) \\nabla u_k,\n    \\nabla \\tilde p) + (f, \\tilde p),\n  \\end{array}   # weak form for setting up the state equation\na_state = inner(exp(a) * nabla_grad(u_trial), nabla_grad(u_test)) * dx\nL_state = f * u_test * dx\n\n# weak form for setting up the adjoint equation\na_adj = inner(exp(a) * nabla_grad(p_trial), nabla_grad(p_test)) * dx\nL_adj = -inner(u - ud, p_test) * dx\n\n# weak form for setting up matrices\nWua_equ = inner(exp(a) * a_trial * nabla_grad(p_test), nabla_grad(p)) * dx\nC_equ   = inner(exp(a) * a_trial * nabla_grad(u), nabla_grad(u_test)) * dx\nRaa_equ = inner(exp(a) * a_trial * a_test *  nabla_grad(u),  nabla_grad(p)) * dx\n\nM_equ   = inner(a_trial, a_test) * dx\n\n# assemble matrix M\nM = assemble(M_equ)  Initial guess  We solve the state equation and compute the cost functional for the initial guess of the parameter  a_ini  # solve state equation\nstate_A, state_b = assemble_system (a_state, L_state, bc_state)\nsolve (state_A, u.vector(), state_b)\n\n# evaluate cost\n[cost_old, misfit_old, reg_old] = cost(u, ud, a, W, R)\n\n# plot\nplt.figure(figsize=(15,5))\nnb.plot(a,subplot_loc=121, mytitle= a_ini , vmin=atrue.vector().min(), vmax=atrue.vector().max())\nnb.plot(u,subplot_loc=122, mytitle= u(a_ini) )\nplt.show()   The reduced Hessian apply to a vector v:  Here we describe how to apply the reduced Hessian operator to a vector v. For an opportune choice of the regularization, the reduced Hessian operator evaluated in a neighborhood of the solution is positive define, whereas far from the solution the reduced Hessian may be indefinite. On the constrary, the Gauss-Newton approximation of the Hessian is always positive defined.  For this reason, it is beneficial to perform a few initial Gauss-Newton steps (5 in this particular example) to accelerate the convergence of the inexact Newton-CG algorithm.  The Hessian apply reads: \n\\begin{align}\n\\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bf A^{-T} (\\bf W_{\\scriptsize\\mbox{uu}} \\bhu +\n\\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha) & \\text{adjoint}\\\\\n\\bf H \\bf v &= (\\bf R + \\bf R_{\\scriptsize\\mbox{aa}})\\bf v + \\bf C^T \\bhp + \\bf W_{\\scriptsize\\mbox{au}} \\bhu.\n\\end{align}   The Gauss-Newton Hessian apply is obtained by dropping the second derivatives operators  \\bf W_{\\scriptsize\\mbox{ua}}\\,\\bha ,  \\bf R_{\\scriptsize\\mbox{aa}}\\bf v , and  \\bf W_{\\scriptsize\\mbox{au}} \\bhu : \n\\begin{align}\n\\bhu &= -\\bf A^{-1} \\bf C \\bf v\\, & \\text{linearized forward}\\\\\n\\bhp &= -\\bf A^{-T} \\bf W_{\\scriptsize\\mbox{uu}} \\bhu & \\text{adjoint}\\\\\n\\bf H_{\\rm GN} \\bf v &= \\bf R \\bf v + \\bf C^T \\bhp.\n\\end{align}   # Class HessianOperator to perform Hessian apply to a vector\nclass HessianOperator():\n    cgiter = 0\n    def __init__(self, R, Raa, C, A, adj_A, W, Wua, use_gaussnewton=False):\n        self.R = R\n        self.Raa = Raa\n        self.C = C\n        self.A = A\n        self.adj_A = adj_A\n        self.W = W\n        self.Wua = Wua\n        self.use_gaussnewton = use_gaussnewton\n\n        # incremental state\n        self.du = Vector()\n        self.A.init_vector(self.du,0)\n\n        #incremental adjoint\n        self.dp = Vector()\n        self.adj_A.init_vector(self.dp,0)\n\n        # auxiliary vectors\n        self.CT_dp = Vector()\n        self.C.init_vector(self.CT_dp, 1)\n        self.Wua_du = Vector()\n        self.Wua.init_vector(self.Wua_du, 1)\n\n    def init_vector(self, v, dim):\n        self.R.init_vector(v,dim)\n\n    # Hessian performed on x, output as generic vector y\n    def mult(self, v, y):\n        self.cgiter += 1\n        y.zero()\n        if self.use_gaussnewton:\n            self.mult_GaussNewton(v,y)\n        else:\n            self.mult_Newton(v,y)\n\n    # define (Gauss-Newton) Hessian apply H * v\n    def mult_GaussNewton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = - (self.W * self.du)\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        # Reg/Prior term\n        self.R.mult(v,y)\n\n        # Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1, self.CT_dp)\n\n    # define (Newton) Hessian apply H * v\n    def mult_Newton(self, v, y):\n\n        #incremental forward\n        rhs = -(self.C * v)\n        bc_adj.apply(rhs)\n        solve (self.A, self.du, rhs)\n\n        #incremental adjoint\n        rhs = -(self.W * self.du) -  self.Wua * v\n        bc_adj.apply(rhs)\n        solve (self.adj_A, self.dp, rhs)\n\n        #Reg/Prior term\n        self.R.mult(v,y)\n        y.axpy(1., Raa*v)\n\n        #Misfit term\n        self.C.transpmult(self.dp, self.CT_dp)\n        y.axpy(1., self.CT_dp)\n        self.Wua.transpmult(self.du, self.Wua_du)\n        y.axpy(1., self.Wua_du)", 
            "title": "Set up"
        }, 
        {
            "location": "/tutorials/2_PoissonDeterministic/#the-inexact-newton-cg-optimization-with-armijo-line-search", 
            "text": "We solve the constrained optimization problem using the inexact Newton-CG method with Armijo line search.  The stopping criterion is based on a relative reduction of the norm of the gradient (i.e.  \\frac{\\|g_{n}\\|}{\\|g_{0}\\|} \\leq \\tau ).  First, we compute the gradient by solving the state and adjoint equation for the current parameter  a , and then substituing the current state  u , parameter  a  and adjoint  p  variables in the weak form expression of the gradient:  (g, \\tilde{a}) = \\gamma(\\nabla a, \\nabla \\tilde{a}) +(\\tilde{a}\\nabla u, \\nabla p).   Then, we compute the Newton direction  \\delta a  by iteratively solving  {\\bf H} {\\delta a} = - {\\bf g} .\nThe Newton system is solved inexactly by early termination of conjugate gradient iterations via Eisenstat\u2013Walker (to prevent oversolving) and Steihaug  (to avoid negative curvature) criteria.  Finally, the Armijo line search uses backtracking to find  \\alpha  such that a sufficient reduction in the cost functional is achieved.\nMore specifically, we use backtracking to find  \\alpha  such that: J( a + \\alpha \\delta a ) \\leq J(a) + \\alpha c_{\\rm armijo} (\\delta a,g).    # define parameters for the optimization\ntol = 1e-8\nc = 1e-4\nmaxiter = 12\nplot_on = False\n\n# initialize iter counters\niter = 1\ntotal_cg_iter = 0\nconverged = False\n\n# initializations\ng, a_delta = Vector(), Vector()\nR.init_vector(a_delta,0)\nR.init_vector(g,0)\n\na_prev = Function(Va)\n\nprint  Nit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg \n\nwhile iter    maxiter and not converged:\n\n    # assemble matrix C\n    C =  assemble(C_equ)\n\n    # solve the adoint problem\n    adjoint_A, adjoint_RHS = assemble_system(a_adj, L_adj, bc_adj)\n    solve(adjoint_A, p.vector(), adjoint_RHS)\n\n    # assemble W_ua and R\n    Wua = assemble (Wua_equ)\n    Raa = assemble (Raa_equ)\n\n    # evaluate the  gradient\n    CT_p = Vector()\n    C.init_vector(CT_p,1)\n    C.transpmult(p.vector(), CT_p)\n    MG = CT_p + R * a.vector()\n    solve(M, g, MG)\n\n    # calculate the norm of the gradient\n    grad2 = g.inner(MG)\n    gradnorm = sqrt(grad2)\n\n    # set the CG tolerance (use Eisenstat\u2013Walker termination criterion)\n    if iter == 1:\n        gradnorm_ini = gradnorm\n    tolcg = min(0.5, sqrt(gradnorm/gradnorm_ini))\n\n    # define the Hessian apply operator (with preconditioner)\n    Hess_Apply = HessianOperator(R, Raa, C, state_A, adjoint_A, W, Wua, use_gaussnewton=(iter 6) )\n    P = R + gamma * M\n    Psolver = PETScKrylovSolver( cg , amg_method())\n    Psolver.set_operator(P)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(Hess_Apply)\n    solver.set_preconditioner(Psolver)\n    solver.parameters[ rel_tolerance ] = tolcg\n    solver.parameters[ zero_initial_guess ] = True\n    solver.parameters[ print_level ] = -1\n\n    # solve the Newton system H a_delta = - MG\n    solver.solve(a_delta, -MG)\n    total_cg_iter += Hess_Apply.cgiter\n\n    # linesearch\n    alpha = 1\n    descent = 0\n    no_backtrack = 0\n    a_prev.assign(a)\n    while descent == 0 and no_backtrack   10:\n        a.vector().axpy(alpha, a_delta )\n\n        # solve the state/forward problem\n        state_A, state_b = assemble_system(a_state, L_state, bc_state)\n        solve(state_A, u.vector(), state_b)\n\n        # evaluate cost\n        [cost_new, misfit_new, reg_new] = cost(u, ud, a, W, R)\n\n        # check if Armijo conditions are satisfied\n        if cost_new   cost_old + alpha * c * MG.inner(a_delta):\n            cost_old = cost_new\n            descent = 1\n        else:\n            no_backtrack += 1\n            alpha *= 0.5\n            a.assign(a_prev)  # reset a\n\n    # calculate sqrt(-G * D)\n    graddir = sqrt(- MG.inner(a_delta) )\n\n    sp =  \n    print  %2d %2s %2d %3s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %8.5e %1s %5.2f %1s %5.3e  % \\\n        (iter, sp, Hess_Apply.cgiter, sp, cost_new, sp, misfit_new, sp, reg_new, sp, \\\n         graddir, sp, gradnorm, sp, alpha, sp, tolcg)\n\n    if plot_on:\n        nb.multi1_plot([a,u,p], [ a , u , p ], same_colorbar=False)\n        plt.show()\n\n    # check for convergence\n    if gradnorm   tol and iter   1:\n        converged = True\n        print  Newton's method converged in  ,iter,   iterations \n        print  Total number of CG iterations:  , total_cg_iter\n\n    iter += 1\n\nif not converged:\n    print  Newton's method did not converge in  , maxiter,   iterations \n\nprint  Time elapsed:  , time.clock()-start  Nit   CGit   cost          misfit        reg           sqrt(-G*D)    ||grad||       alpha  tolcg\n 1     1     1.12708e-05   1.12708e-05   1.33979e-11   1.56540e-02   3.79427e-04    1.00   5.000e-01\n 2     1     7.79732e-07   7.79695e-07   3.67737e-11   4.68278e-03   5.35002e-05    1.00   3.755e-01\n 3     1     3.10620e-07   3.10571e-07   4.91259e-11   9.71633e-04   7.13895e-06    1.00   1.372e-01\n 4     5     1.92183e-07   1.62405e-07   2.97780e-08   4.51694e-04   1.00276e-06    1.00   5.141e-02\n 5     1     1.86913e-07   1.57119e-07   2.97941e-08   1.02668e-04   6.12750e-07    1.00   4.019e-02\n 6    12     1.80408e-07   1.37719e-07   4.26890e-08   1.15975e-04   2.24111e-07    1.00   2.430e-02\n 7     5     1.80331e-07   1.38935e-07   4.13963e-08   1.23223e-05   4.17399e-08    1.00   1.049e-02\n 8    15     1.80330e-07   1.39056e-07   4.12734e-08   1.74451e-06   3.43216e-09    1.00   3.008e-03\nNewton's method converged in  8   iterations\nTotal number of CG iterations:  41\nTime elapsed:  9.28313  nb.multi1_plot([atrue, a], [ atrue ,  a ])\nnb.multi1_plot([u,p], [ u , p ], same_colorbar=False)\nplt.show()    Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "The inexact Newton-CG optimization with Armijo line search:"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/", 
            "text": "\\def\\data{\\bf d_\\rm{obs}}\n\\def\\vec{\\bf}\n\\def\\m{\\bf m}\n\\def\\map{\\bf m_{\\text{MAP}}}\n\\def\\postcov{\\bf \\Gamma_{\\text{post}}}\n\\def\\prcov{\\bf \\Gamma_{\\text{prior}}}\n\\def\\matrix{\\bf}\n\\def\\Hmisfit{\\bf H_{\\text{misfit}}}\n\\def\\HT{\\tilde{\\bf H}_{\\text{misfit}}}\n\\def\\diag{diag}\n\\def\\Vr{\\matrix V_r}\n\\def\\Wr{\\matrix W_r}\n\\def\\Ir{\\matrix I_r}\n\\def\\Dr{\\matrix D_r}\n\\def\\H{\\matrix H}\n\n\n\n\n\nExample: Bayesian quantification of parameter uncertainty:\n\n\nEstimating the (Gaussian) posterior pdf of the coefficient parameter field in an elliptic PDE\n\n\nIn this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields our belief (expressed as a probability) that a\nmember of this candidate set is the ``true'' parameter field that\ngave rise to the observed data.\n\n\nFor simplicity, in what follows we give finite-dimensional expressions (i.e., after\ndiscretization of the parameter space) for the Bayesian\nformulation of the inverse problem.\n\n\nBayes' Theorem:\n\n\nThe posterior probability distribution combines the prior pdf\n\n\\pi_{\\text{prior}}(\\m)\n over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf\n\n\\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m)\n, which explicitly\nrepresents the probability that a given set of parameters \n\\m\n\nmight give rise to the observed data \n\\vec{d}_{\\text{obs}} \\in\n\\mathbb{R}^m\n, namely:\n\n\n\n\n\n\\begin{align}\n\\pi_{\\text{post}}(\\m | \\data) \\propto\n\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n\\end{align}\n\n\n\n\n\nNote that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.\n\n\nGaussian prior and noise:\n\n\nThe prior:\n\n\nWe consider a Gaussian prior with mean \n\\vec m_{\\text prior}\n and covariance \n\\bf \\Gamma_{\\text{prior}}\n. The covariance is given by the discretization of the inverse of differential operator \n\\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2}\n, where \n\\gamma\n, \n\\delta > 0\n control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem\n\n\nThe likelihood:\n\n\n\n\n\n\\data =  \\bf{f}(\\m) + \\bf{e }, \\;\\;\\;  \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} )\n\n\n\n\n\n\n\n\n\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right)\n\n\n\n\n\nHere \n\\bf f\n is the parameter-to-observable map that takes a parameter vector \n\\m\n and maps\nit to the space observation vector \n\\data\n.\n\n\nThe posterior:\n\n\n\n\n\n\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf  \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right)\n\n\n\n\n\nThe Gaussian approximation of the posterior: \n\\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}})\n\n\n\n\nThe mean of this posterior distribution, \n\\vec{\\map}\n, is the\nparameter vector maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,\n\n\n\n\n\n\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n\\Big( \n\\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} \n+\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \n\\Big).\n\n\n\n\n\nThe posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of \n\\mathcal{J}\n at \n\\map\n, namely\n\n\n\n\n\n\\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1}\n\n\n\n\n\nThe prior-preconditioned Hessian of the data misfit:\n\n\n\n\n\n  \\HT := \\prcov^{1/2} \\Hmisfit \\prcov^{1/2}\n\n\n\n\n\nThe generalized eigenvalue problem:\n\n\n\n\n\n \\Hmisfit \\matrix{V} = \\prcov^{-1} \\matrix{V} \\matrix{\\Lambda},\n\n\n\n\n\nwhere \n\\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n}\n\ncontains the generalized eigenvalues and the columns of \n\\matrix V\\in\n\\mathbb R^{n\\times n}\n the generalized eigenvectors such that \n\n\\matrix{V}^T \\prcov^{-1} \\matrix{V} = \\matrix{I}\n.\n\n\nRandomized eigensolvers to construct the approximate spectral decomposition:\n\n\nWhen the generalized eigenvalues \n\\{\\lambda_i\\}\n decay rapidly, we can\nextract a low-rank approximation of \n\\Hmisfit\n by retaining only the \nr\n\nlargest eigenvalues and corresponding eigenvectors,\n\n\n\n\n\n \\HT = \\prcov^{-1/2} \\matrix{V}_r \\matrix{\\Lambda}_r \\matrix{V}^T_r \\prcov^{-1/2},\n\n\n\n\n\nHere, \n\\matrix{V}_r \\in \\mathbb{R}^{n\\times r}\n contains only the \nr\n\ngeneralized eigenvectors of \n\\Hmisfit\n that correspond to the \nr\n largest eigenvalues,\nwhich are assembled into the diagonal matrix \n\\matrix{\\Lambda}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r}\n.\n\n\nThe approximate posterior covariance::\n\n\nUsing the Sherman\u2013Morrison\u2013Woodbury formula, we write\n\n\n\\begin{align}\n  \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1}\n  = \\prcov^{-1}-\\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}\n\n\n\n\n\nwhere \n\\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r}\n. The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that \nr\n is\nchosen such that \n\\lambda_r\n is small relative to 1. \n\n\nTherefore we can approximate the posterior covariance as\n\n\n\n\n\n\\postcov \\approx \\prcov - \\matrix{V}_r \\matrix{D}_r\n\\matrix{V}_r^T\n\n\n\n\n\nDrawing samples from a Gaussian distribution with covariance \n\\H^{-1}\n\n\n\n\nLet \n\\bf x\n be a sample for the prior distribution, i.e. \n\\bf x \\sim \\mathcal{N}({\\bf 0}, \\prcov)\n, then, using the low rank approximation of the posterior covariance, we compute a sample \n{\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1})\n as \n\n\n  {\\bf v} = \\big\\{ \\Vr \\big[ (\\matrix{\\Lambda}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1}  + \\bf I \\big\\} {\\bf x} \n\n\n\n\n\nThis tutorial shows:\n\n\n\n\ndescription of the inverse problem (the forward problem, the prior, and the misfit functional)\n\n\nconvergence of the inexact Newton-CG algorithm\n\n\nlow-rank-based approximation of the posterior covariance (built on a low-rank\napproximation of the Hessian of the data misfit) \n\n\nhow to construct the low-rank approximation of the Hessian of the data misfit\n\n\nhow to apply the inverse and square-root inverse Hessian to a vector efficiently\n\n\nsamples from the Gaussian approximation of the posterior\n\n\n\n\nGoals:\n\n\nBy the end of this notebook, you should be able to:\n\n\n\n\nunderstand the Bayesian inverse framework\n\n\nvisualise and understand the results\n\n\nmodify the problem and code\n\n\n\n\nMathematical tools used:\n\n\n\n\nFinite element method\n\n\nDerivation of gradiant and Hessian via the adjoint method\n\n\ninexact Newton-CG\n\n\nArmijo line search\n\n\nBayes' formula\n\n\nrandomized eigensolvers\n\n\n\n\nList of software used:\n\n\n\n\nFEniCS\n, a parallel finite element element library for the discretization of partial differential equations\n\n\nPETSc\n, for scalable and efficient linear algebra operations and solvers\n\n\nMatplotlib\n, A great python package that I used for plotting many of the results\n\n\nNumpy\n, A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook.\n\n\n\n\n1. Load modules\n\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nsys.path.append( \n../\n )\nfrom hippylib import *\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)\n\n\n\n\n2. Generate the true parameter\n\n\nThis function generates a random field with a prescribed anysotropic covariance function.\n\n\ndef true_model(Vh, gamma, delta, anis_diff):\n    prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff )\n    noise = dl.Vector()\n    prior.init_vector(noise,\nnoise\n)\n    noise_size = noise.array().shape[0]\n    noise.set_local( np.random.randn( noise_size ) )\n    atrue = dl.Vector()\n    prior.init_vector(atrue, 0)\n    prior.sample(noise,atrue)\n    return atrue\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nWe compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the \nstate\n and \nadjoint\n variable and P1 for the \nparameter\n.\n\n\nndim = 2\nnx = 64\nny = 64\nmesh = dl.UnitSquareMesh(nx, ny)\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh = [Vh2, Vh1, Vh2]\nprint \nNumber of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\n.format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())\n\n\n\n\nNumber of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641\n\n\n\n4. Set up the forward problem\n\n\nTo set up the forward problem we use the \nPDEVariationalProblem\n class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables \nVh\n\n- the pde in weak form \npde_varf\n\n- the boundary conditions \nbc\n for the forward problem and \nbc0\n for the adjoint and incremental problems.\n\n\nThe \nPDEVariationalProblem\n class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.\n\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and ( x[1] \n dl.DOLFIN_EPS or x[1] \n 1.0 - dl.DOLFIN_EPS)\n\nu_bdr = dl.Expression(\nx[1]\n, degree=1)\nu_bdr0 = dl.Constant(0.0)\nbc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Constant(0.0)\n\ndef pde_varf(u,a,p):\n    return dl.exp(a)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n\n\n\n4. Set up the prior\n\n\nTo obtain the synthetic true paramter \na_{\\rm true}\n we generate a realization of a Gaussian random field with zero average and covariance matrix \n\\mathcal{C} = \\widetilde{\\mathcal{A}}^{-2}\n, where \n\\widetilde{\\mathcal{A}}\n is a differential operator of the form\n\n \\widetilde{\\mathcal{A}} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I. \n\nHere \n\\Theta\n is an s.p.d. anisotropic tensor of the form\n\n \\Theta =\n\\begin{bmatrix}\n\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n\\end{bmatrix} \n\n\n\n\nFor the prior model, we assume that we can measure the log-permeability coefficient at \nN\n locations, and we denote with \na^1_{\\rm true}\n, \n\\ldots\n, \na^N_{\\rm true}\n such measures.\nWe also introduce the mollifier functions\n\n \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N,\n\nand we let\n\n \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M},\n\nwhere \np\n is a penalization costant (10 for this problem) and \n \\mathcal{M} = \\sum_{i=1}^N \\delta_i I\n.\n\n\nWe then compute \na_{\\rm pr}\n, the  mean  of  the  prior  measure,  as  a  regularized\nleast-squares fit of these point observations by solving\n\n\na_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle a, \\widetilde{\\mathcal{A}} a\\rangle + \\frac{p}{2}\\langle a_{\\rm true} - a, \\mathcal{M}(a_{\\rm true}- a) \\rangle.\n\n\n\n\n\nFinally the prior distribution is \n\\mathcal{N}(a_{\\rm pr}, \\mathcal{C}_{\\rm prior})\n, with \n\\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2}\n.\n\n\ngamma = .1\ndelta = .5\n\nanis_diff = dl.Expression(code_AnisTensor2D, degree=1)\nanis_diff.theta0 = 2.\nanis_diff.theta1 = .5\nanis_diff.alpha = math.pi/4\natrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff)\n\nlocations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]])\npen = 1e1\nprior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, atrue, anis_diff, pen)\n\nprint \nPrior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2}\n.format(delta, gamma,2)    \n\nobjs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [\nTrue Parameter\n, \nPrior mean\n]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\nmodel = Model(pde,prior, misfit)\n\n\n\n\nPrior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2\n\n\n\n\n\n5. Set up the misfit functional and generate synthetic observations\n\n\nTo setup the observation operator, we generate \nntargets\n random locations where to evaluate the value of the state.\n\n\nTo generate the synthetic observation, we first solve the forward problem using the true parameter \na_{\\rm true}\n. Synthetic observations are obtained by perturbing the state variable at the observation points with a random gaussian noise.\n\nrel_noise\n is the signal to noise ratio.\n\n\nntargets = 300\nrel_noise = 0.01\n\n\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nprint \nNumber of observation points: {0}\n.format(ntargets)\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, atrue, None]\npde.solveFwd(x[STATE], x, 1e-9)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm(\nlinf\n)\nnoise_std_dev = rel_noise * MAX\nrandn_perturb(misfit.d, noise_std_dev)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle=\nTrue State\n, subplot_loc=121, vmin=vmin, vmax=vmax)\nnb.plot_pts(targets, misfit.d, mytitle=\nObservations\n, subplot_loc=122, vmin=vmin, vmax=vmax)\nplt.show()\n\n\n\n\nNumber of observation points: 300\n\n\n\n\n\n6. Set up the model and test gradient and Hessian\n\n\nThe model is defined by three component:\n- the \nPDEVariationalProblem\n \npde\n which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n- the \nPrior\n \nprior\n which provides methods to apply the regularization (\nprecision\n) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n- the \nMisfit\n \nmisfit\n which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.\n\n\nTo test gradient and the Hessian of the model we use forward finite differences.\n\n\nmodel = Model(pde, prior, misfit)\n\na0 = dl.interpolate(dl.Expression(\nsin(x[0])\n, degree=5), Vh[PARAMETER])\nmodelVerify(model, a0.vector(), 1e-12)\n\n\n\n\n(yy, H xx) - (xx, H yy) =  7.4275188982e-15\n\n\n\n\n\n7. Compute the MAP point\n\n\nWe used the globalized Newtown-CG method to compute the MAP point.\n\n\na0 = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[\nrel_tolerance\n] = 1e-9\nsolver.parameters[\nabs_tolerance\n] = 1e-12\nsolver.parameters[\nmax_iter\n]      = 25\nsolver.parameters[\ninner_rel_tolerance\n] = 1e-15\nsolver.parameters[\nc_armijo\n] = 1e-4\nsolver.parameters[\nGN_iter\n] = 5\n\nx = solver.solve(a0)\n\nif solver.converged:\n    print \n\\nConverged in \n, solver.it, \n iterations.\n\nelse:\n    print \n\\nNot Converged\n\n\nprint \nTermination reason: \n, solver.termination_reasons[solver.reason]\nprint \nFinal gradient norm: \n, solver.final_grad_norm\nprint \nFinal cost: \n, solver.final_cost\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle=\nState\n)\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle=\nParameter\n)\nplt.show()\n\n\n\n\nIt  cg_it  cost          misfit        reg           (g,da)       ||g||L2      alpha     tolcg         \n  1   1    1.2065e+03    1.2062e+03    3.1473e-01   -1.5705e+04   1.0425e+05   1.0e+00   5.0000e-01\n  2   3    3.4593e+02    3.4468e+02    1.2536e+00   -1.8467e+03   1.4314e+04   1.0e+00   3.7054e-01\n  3   1    2.7467e+02    2.7336e+02    1.3092e+00   -1.4246e+02   1.0037e+04   1.0e+00   3.1028e-01\n  4   7    1.6917e+02    1.6476e+02    4.4157e+00   -2.1284e+02   3.8710e+03   1.0e+00   1.9269e-01\n  5   6    1.5732e+02    1.5229e+02    5.0283e+00   -2.3456e+01   1.8211e+03   1.0e+00   1.3216e-01\n  6  14    1.4249e+02    1.2974e+02    1.2743e+01   -2.9882e+01   1.1574e+03   1.0e+00   1.0536e-01\n  7   2    1.4216e+02    1.2941e+02    1.2746e+01   -6.6442e-01   7.3260e+02   1.0e+00   8.3827e-02\n  8  22    1.4079e+02    1.2532e+02    1.5471e+01   -2.7344e+00   4.4098e+02   1.0e+00   6.5037e-02\n  9  14    1.4078e+02    1.2537e+02    1.5404e+01   -1.7445e-02   5.0571e+01   1.0e+00   2.2024e-02\n 10  29    1.4078e+02    1.2533e+02    1.5451e+01   -1.8196e-03   1.5024e+01   1.0e+00   1.2004e-02\n 11  38    1.4078e+02    1.2533e+02    1.5451e+01   -2.5035e-07   1.3906e-01   1.0e+00   1.1549e-03\n 12  53    1.4078e+02    1.2533e+02    1.5451e+01   -1.3812e-12   2.6266e-04   1.0e+00   5.0194e-05\n\nConverged in  12  iterations.\nTermination reason:  Norm of the gradient less than tolerance\nFinal gradient norm:  1.63019593759e-08\nFinal cost:  140.781736843\n\n\n\n\n\n8. Compute the low rank Gaussian approximation of the posterior\n\n\nWe used the \ndouble pass\n algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve\n\n\n\n\n \\Hmisfit {\\bf u} = \\lambda \\prcov^{-1} {\\bf u}. \n\n\n\n\nThe Figure shows the largest \nk\n generalized eigenvectors of the Hessian misfit.\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1).\nThe effective rank is independent of the mesh size.\n\n\nmodel.setPointForHessianEvaluations(x)\nHmisfit = ReducedHessian(model, solver.parameters[\ninner_rel_tolerance\n], gauss_newton_approx=False, misfit_only=True)\nk = 50\np = 20\nprint \nSingle/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\n.format(k,p)\nOmega = np.random.randn(x[PARAMETER].array().shape[0], k+p)\nd, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nposterior = GaussianLRPosterior(prior, d, U)\nposterior.mean = x[PARAMETER]\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\nEigenvector\n, which=[0,1,2,5,10,15])\n\n\n\n\nSingle/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.\n\n\n\n\n\n\n\n9. Prior and posterior pointwise variance fields\n\n\ncompute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\nEstimator\n, tol=5e-2, min_iter=20, max_iter=2000)\n    print \nPosterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e}\n.format(post_tr, prior_tr, corr_tr)\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(\nExact\n)\n\nobjs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n        dl.Function(Vh[PARAMETER], post_pw_variance)]\nmytitles = [\nPrior variance\n, \nPosterior variance\n]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()\n\n\n\n\nPosterior trace 1.143368e-01; Prior trace 4.034355e-01; Correction trace 2.890987e-01\n\n\n\n\n\n10. Generate samples from Prior and Posterior\n\n\nnsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\nnoise\n)\nnoise_size = noise.array().shape[0]\ns_prior = dl.Function(Vh[PARAMETER], name=\nsample_prior\n)\ns_post = dl.Function(Vh[PARAMETER], name=\nsample_post\n)\n\npr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\nps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121, mytitle=\nPrior sample\n,     vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post,  subplot_loc=122, mytitle=\nPosterior sample\n, vmin=ps_min, vmax=ps_max)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "Subsurface Bayesian"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#example-bayesian-quantification-of-parameter-uncertainty", 
            "text": "", 
            "title": "Example: Bayesian quantification of parameter uncertainty:"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#estimating-the-gaussian-posterior-pdf-of-the-coefficient-parameter-field-in-an-elliptic-pde", 
            "text": "In this example we tackle the problem of quantifying the\nuncertainty in the solution of an inverse problem governed by an\nelliptic PDE via the Bayesian inference framework. \nHence, we state the inverse problem as a\nproblem of statistical inference over the space of uncertain\nparameters, which are to be inferred from data and a physical\nmodel.  The resulting solution to the statistical inverse problem\nis a posterior distribution that assigns to any candidate set of\nparameter fields our belief (expressed as a probability) that a\nmember of this candidate set is the ``true'' parameter field that\ngave rise to the observed data.  For simplicity, in what follows we give finite-dimensional expressions (i.e., after\ndiscretization of the parameter space) for the Bayesian\nformulation of the inverse problem.  Bayes' Theorem:  The posterior probability distribution combines the prior pdf \\pi_{\\text{prior}}(\\m)  over the parameter space, which encodes\nany knowledge or assumptions about the parameter space that we may\nwish to impose before the data are considered, with a likelihood pdf \\pi_{\\text{like}}(\\vec{d}_{\\text{obs}} \\; | \\; \\m) , which explicitly\nrepresents the probability that a given set of parameters  \\m \nmight give rise to the observed data  \\vec{d}_{\\text{obs}} \\in\n\\mathbb{R}^m , namely:   \n\\begin{align}\n\\pi_{\\text{post}}(\\m | \\data) \\propto\n\\pi_{\\text{prior}}(\\m) \\pi_{\\text{like}}(\\data | \\m).\n\\end{align}   Note that infinite-dimensional analog of Bayes' formula requires the use Radon-Nikodym derivatives instead of probability density functions.  Gaussian prior and noise:  The prior:  We consider a Gaussian prior with mean  \\vec m_{\\text prior}  and covariance  \\bf \\Gamma_{\\text{prior}} . The covariance is given by the discretization of the inverse of differential operator  \\mathcal{A}^{-2} = (-\\gamma \\Delta + \\delta I)^{-2} , where  \\gamma ,  \\delta > 0  control the correlation length and the variance of the prior operator. This choice of prior ensures that it is a trace-class operator, guaranteeing bounded pointwise variance and a well-posed infinite-dimensional Bayesian inverse problem  The likelihood:   \n\\data =  \\bf{f}(\\m) + \\bf{e }, \\;\\;\\;  \\bf{e} \\sim \\mathcal{N}(\\bf{0}, \\bf \\Gamma_{\\text{noise}} )    \n\\pi_{\\text like}(\\data \\; | \\; \\m)  = \\exp \\left( - \\tfrac{1}{2} (\\bf{f}(\\m) - \\data)^T \\bf \\Gamma_{\\text{noise}}^{-1} (\\bf{f}(\\m) - \\data)\\right)   Here  \\bf f  is the parameter-to-observable map that takes a parameter vector  \\m  and maps\nit to the space observation vector  \\data .  The posterior:   \n\\pi_{\\text{post}}(\\m \\; | \\; \\data)  \\propto \\exp \\left( - \\tfrac{1}{2} \\parallel \\bf{f}(\\m) - \\data \\parallel^{2}_{\\bf  \\Gamma_{\\text{noise}}^{-1}} \\! - \\tfrac{1}{2}\\parallel \\m - \\m_{\\text prior} \\parallel^{2}_{\\bf \\Gamma_{\\text{prior}}^{-1}} \\right)   The Gaussian approximation of the posterior:  \\mathcal{N}(\\vec{\\map},\\bf \\Gamma_{\\text{post}})   The mean of this posterior distribution,  \\vec{\\map} , is the\nparameter vector maximizing the posterior, and\nis known as the maximum a posteriori (MAP) point.  It can be found\nby minimizing the negative log of the posterior, which amounts to\nsolving a deterministic inverse problem) with appropriately weighted norms,   \n\\map := \\underset{\\m}{\\arg \\min} \\; \\mathcal{J}(\\m) \\;:=\\;\n\\Big( \n\\frac{1}{2} \\| \\bf f(\\m) - \\data \\|^2_{\\bf \\Gamma_{\\text{noise}}^{-1}} \n+\\frac{1}{2} \\| \\m -\\m_{\\text prior} \\|^2_{\\bf \\Gamma_{\\text{prior}}^{-1}} \n\\Big).   The posterior covariance matrix is then given by the inverse of\nthe Hessian matrix of  \\mathcal{J}  at  \\map , namely   \n\\bf \\Gamma_{\\text{post}} = \\left(\\Hmisfit(\\map) + \\bf \\Gamma_{\\text{prior}}^{-1} \\right)^{-1}   The prior-preconditioned Hessian of the data misfit:   \n  \\HT := \\prcov^{1/2} \\Hmisfit \\prcov^{1/2}   The generalized eigenvalue problem:   \n \\Hmisfit \\matrix{V} = \\prcov^{-1} \\matrix{V} \\matrix{\\Lambda},   where  \\matrix{\\Lambda} = diag(\\lambda_i) \\in \\mathbb{R}^{n\\times n} \ncontains the generalized eigenvalues and the columns of  \\matrix V\\in\n\\mathbb R^{n\\times n}  the generalized eigenvectors such that  \\matrix{V}^T \\prcov^{-1} \\matrix{V} = \\matrix{I} .  Randomized eigensolvers to construct the approximate spectral decomposition:  When the generalized eigenvalues  \\{\\lambda_i\\}  decay rapidly, we can\nextract a low-rank approximation of  \\Hmisfit  by retaining only the  r \nlargest eigenvalues and corresponding eigenvectors,   \n \\HT = \\prcov^{-1/2} \\matrix{V}_r \\matrix{\\Lambda}_r \\matrix{V}^T_r \\prcov^{-1/2},   Here,  \\matrix{V}_r \\in \\mathbb{R}^{n\\times r}  contains only the  r \ngeneralized eigenvectors of  \\Hmisfit  that correspond to the  r  largest eigenvalues,\nwhich are assembled into the diagonal matrix  \\matrix{\\Lambda}_r = \\diag\n(\\lambda_i) \\in \\mathbb{R}^{r \\times r} .  The approximate posterior covariance::  Using the Sherman\u2013Morrison\u2013Woodbury formula, we write \n\\begin{align}\n  \\notag \\postcov = \\left(\\Hmisfit+ \\prcov^{-1}\\right)^{-1}\n  = \\prcov^{-1}-\\matrix{V}_r \\matrix{D}_r \\matrix{V}_r^T +\n  \\mathcal{O}\\left(\\sum_{i=r+1}^{n} \\frac{\\lambda_i}{\\lambda_i +\n    1}\\right),\n\\end{align}   where  \\matrix{D}_r :=\\diag(\\lambda_i/(\\lambda_i+1)) \\in\n\\mathbb{R}^{r\\times r} . The last term in this expression captures the\nerror due to truncation in terms of the discarded eigenvalues; this\nprovides a criterion for truncating the spectrum, namely that  r  is\nchosen such that  \\lambda_r  is small relative to 1.   Therefore we can approximate the posterior covariance as   \n\\postcov \\approx \\prcov - \\matrix{V}_r \\matrix{D}_r\n\\matrix{V}_r^T   Drawing samples from a Gaussian distribution with covariance  \\H^{-1}   Let  \\bf x  be a sample for the prior distribution, i.e.  \\bf x \\sim \\mathcal{N}({\\bf 0}, \\prcov) , then, using the low rank approximation of the posterior covariance, we compute a sample  {\\bf v} \\sim \\mathcal{N}({\\bf 0}, \\H^{-1})  as  \n  {\\bf v} = \\big\\{ \\Vr \\big[ (\\matrix{\\Lambda}_r +\n    \\Ir)^{-1/2} - \\Ir \\big] \\Vr^T\\prcov^{-1}  + \\bf I \\big\\} {\\bf x}", 
            "title": "Estimating the (Gaussian) posterior pdf of the coefficient parameter field in an elliptic PDE"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#this-tutorial-shows", 
            "text": "description of the inverse problem (the forward problem, the prior, and the misfit functional)  convergence of the inexact Newton-CG algorithm  low-rank-based approximation of the posterior covariance (built on a low-rank\napproximation of the Hessian of the data misfit)   how to construct the low-rank approximation of the Hessian of the data misfit  how to apply the inverse and square-root inverse Hessian to a vector efficiently  samples from the Gaussian approximation of the posterior", 
            "title": "This tutorial shows:"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#goals", 
            "text": "By the end of this notebook, you should be able to:   understand the Bayesian inverse framework  visualise and understand the results  modify the problem and code", 
            "title": "Goals:"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#mathematical-tools-used", 
            "text": "Finite element method  Derivation of gradiant and Hessian via the adjoint method  inexact Newton-CG  Armijo line search  Bayes' formula  randomized eigensolvers", 
            "title": "Mathematical tools used:"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#list-of-software-used", 
            "text": "FEniCS , a parallel finite element element library for the discretization of partial differential equations  PETSc , for scalable and efficient linear algebra operations and solvers  Matplotlib , A great python package that I used for plotting many of the results  Numpy , A python package for linear algebra.  While extensive, this is mostly used to compute means and sums in this notebook.", 
            "title": "List of software used:"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#1-load-modules", 
            "text": "import dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nsys.path.append(  ../  )\nfrom hippylib import *\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(seed=1)", 
            "title": "1. Load modules"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#2-generate-the-true-parameter", 
            "text": "This function generates a random field with a prescribed anysotropic covariance function.  def true_model(Vh, gamma, delta, anis_diff):\n    prior = BiLaplacianPrior(Vh, gamma, delta, anis_diff )\n    noise = dl.Vector()\n    prior.init_vector(noise, noise )\n    noise_size = noise.array().shape[0]\n    noise.set_local( np.random.randn( noise_size ) )\n    atrue = dl.Vector()\n    prior.init_vector(atrue, 0)\n    prior.sample(noise,atrue)\n    return atrue", 
            "title": "2. Generate the true parameter"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#3-set-up-the-mesh-and-finite-element-spaces", 
            "text": "We compute a two dimensional mesh of a unit square with nx by ny elements.\nWe define a P2 finite element space for the  state  and  adjoint  variable and P1 for the  parameter .  ndim = 2\nnx = 64\nny = 64\nmesh = dl.UnitSquareMesh(nx, ny)\nVh2 = dl.FunctionSpace(mesh, 'Lagrange', 2)\nVh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\nVh = [Vh2, Vh1, Vh2]\nprint  Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2} .format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())  Number of dofs: STATE=16641, PARAMETER=4225, ADJOINT=16641", 
            "title": "3. Set up the mesh and finite element spaces"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#4-set-up-the-forward-problem", 
            "text": "To set up the forward problem we use the  PDEVariationalProblem  class, which requires the following inputs\n- the finite element spaces for the state, parameter, and adjoint variables  Vh \n- the pde in weak form  pde_varf \n- the boundary conditions  bc  for the forward problem and  bc0  for the adjoint and incremental problems.  The  PDEVariationalProblem  class offer the following functionality:\n- solving the forward/adjoint and incremental problems\n- evaluate first and second partial derivative of the forward problem with respect to the state, parameter, and adojnt variables.  def u_boundary(x, on_boundary):\n    return on_boundary and ( x[1]   dl.DOLFIN_EPS or x[1]   1.0 - dl.DOLFIN_EPS)\n\nu_bdr = dl.Expression( x[1] , degree=1)\nu_bdr0 = dl.Constant(0.0)\nbc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\nbc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\nf = dl.Constant(0.0)\n\ndef pde_varf(u,a,p):\n    return dl.exp(a)*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx - f*p*dl.dx\n\npde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)", 
            "title": "4. Set up the forward problem"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#4-set-up-the-prior", 
            "text": "To obtain the synthetic true paramter  a_{\\rm true}  we generate a realization of a Gaussian random field with zero average and covariance matrix  \\mathcal{C} = \\widetilde{\\mathcal{A}}^{-2} , where  \\widetilde{\\mathcal{A}}  is a differential operator of the form  \\widetilde{\\mathcal{A}} = \\gamma {\\rm div}\\, \\Theta\\, {\\rm grad} + \\delta I.  \nHere  \\Theta  is an s.p.d. anisotropic tensor of the form  \\Theta =\n\\begin{bmatrix}\n\\theta_1 \\sin(\\alpha)^2 & (\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} \\\\\n(\\theta_1-\\theta_2) \\sin(\\alpha) \\cos{\\alpha} & \\theta_2 \\cos(\\alpha)^2.\n\\end{bmatrix}    For the prior model, we assume that we can measure the log-permeability coefficient at  N  locations, and we denote with  a^1_{\\rm true} ,  \\ldots ,  a^N_{\\rm true}  such measures.\nWe also introduce the mollifier functions  \\delta_i(x) = \\exp\\left( -\\frac{\\gamma^2}{\\delta^2} \\| x - x_i \\|^2_{\\Theta^{-1}}\\right), \\quad i = 1, \\ldots, N, \nand we let  \\mathcal{A} = \\widetilde{\\mathcal{A}} + p \\sum_{i=1}^N \\delta_i I = \\widetilde{\\mathcal{A}} + p \\mathcal{M}, \nwhere  p  is a penalization costant (10 for this problem) and   \\mathcal{M} = \\sum_{i=1}^N \\delta_i I .  We then compute  a_{\\rm pr} , the  mean  of  the  prior  measure,  as  a  regularized\nleast-squares fit of these point observations by solving \na_{\\rm pr} = arg\\min_{m} \\frac{1}{2}\\langle a, \\widetilde{\\mathcal{A}} a\\rangle + \\frac{p}{2}\\langle a_{\\rm true} - a, \\mathcal{M}(a_{\\rm true}- a) \\rangle.   Finally the prior distribution is  \\mathcal{N}(a_{\\rm pr}, \\mathcal{C}_{\\rm prior}) , with  \\mathcal{C}_{\\rm prior} = \\mathcal{A}^{-2} .  gamma = .1\ndelta = .5\n\nanis_diff = dl.Expression(code_AnisTensor2D, degree=1)\nanis_diff.theta0 = 2.\nanis_diff.theta1 = .5\nanis_diff.alpha = math.pi/4\natrue = true_model(Vh[PARAMETER], gamma, delta,anis_diff)\n\nlocations = np.array([[0.1, 0.1], [0.1, 0.9], [.5,.5], [.9, .1], [.9, .9]])\npen = 1e1\nprior = MollifiedBiLaplacianPrior(Vh[PARAMETER], gamma, delta, locations, atrue, anis_diff, pen)\n\nprint  Prior regularization: (delta_x - gamma*Laplacian)^order: delta={0}, gamma={1}, order={2} .format(delta, gamma,2)    \n\nobjs = [dl.Function(Vh[PARAMETER],atrue), dl.Function(Vh[PARAMETER],prior.mean)]\nmytitles = [ True Parameter ,  Prior mean ]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\nmodel = Model(pde,prior, misfit)  Prior regularization: (delta_x - gamma*Laplacian)^order: delta=0.5, gamma=0.1, order=2", 
            "title": "4. Set up the prior"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#5-set-up-the-misfit-functional-and-generate-synthetic-observations", 
            "text": "To setup the observation operator, we generate  ntargets  random locations where to evaluate the value of the state.  To generate the synthetic observation, we first solve the forward problem using the true parameter  a_{\\rm true} . Synthetic observations are obtained by perturbing the state variable at the observation points with a random gaussian noise. rel_noise  is the signal to noise ratio.  ntargets = 300\nrel_noise = 0.01\n\n\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nprint  Number of observation points: {0} .format(ntargets)\nmisfit = PointwiseStateObservation(Vh[STATE], targets)\n\nutrue = pde.generate_state()\nx = [utrue, atrue, None]\npde.solveFwd(x[STATE], x, 1e-9)\nmisfit.B.mult(x[STATE], misfit.d)\nMAX = misfit.d.norm( linf )\nnoise_std_dev = rel_noise * MAX\nrandn_perturb(misfit.d, noise_std_dev)\nmisfit.noise_variance = noise_std_dev*noise_std_dev\n\nvmax = max( utrue.max(), misfit.d.max() )\nvmin = min( utrue.min(), misfit.d.min() )\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], utrue), mytitle= True State , subplot_loc=121, vmin=vmin, vmax=vmax)\nnb.plot_pts(targets, misfit.d, mytitle= Observations , subplot_loc=122, vmin=vmin, vmax=vmax)\nplt.show()  Number of observation points: 300", 
            "title": "5. Set up the misfit functional and generate synthetic observations"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#6-set-up-the-model-and-test-gradient-and-hessian", 
            "text": "The model is defined by three component:\n- the  PDEVariationalProblem   pde  which provides methods for the solution of the forward problem, adjoint problem, and incremental forward and adjoint problems.\n- the  Prior   prior  which provides methods to apply the regularization ( precision ) operator to a vector or to apply the prior covariance operator (i.e. to solve linear system with the regularization operator)\n- the  Misfit   misfit  which provides methods to compute the cost functional and its partial derivatives with respect to the state and parameter variables.  To test gradient and the Hessian of the model we use forward finite differences.  model = Model(pde, prior, misfit)\n\na0 = dl.interpolate(dl.Expression( sin(x[0]) , degree=5), Vh[PARAMETER])\nmodelVerify(model, a0.vector(), 1e-12)  (yy, H xx) - (xx, H yy) =  7.4275188982e-15", 
            "title": "6. Set up the model and test gradient and Hessian"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#7-compute-the-map-point", 
            "text": "We used the globalized Newtown-CG method to compute the MAP point.  a0 = prior.mean.copy()\nsolver = ReducedSpaceNewtonCG(model)\nsolver.parameters[ rel_tolerance ] = 1e-9\nsolver.parameters[ abs_tolerance ] = 1e-12\nsolver.parameters[ max_iter ]      = 25\nsolver.parameters[ inner_rel_tolerance ] = 1e-15\nsolver.parameters[ c_armijo ] = 1e-4\nsolver.parameters[ GN_iter ] = 5\n\nx = solver.solve(a0)\n\nif solver.converged:\n    print  \\nConverged in  , solver.it,   iterations. \nelse:\n    print  \\nNot Converged \n\nprint  Termination reason:  , solver.termination_reasons[solver.reason]\nprint  Final gradient norm:  , solver.final_grad_norm\nprint  Final cost:  , solver.final_cost\n\nplt.figure(figsize=(15,5))\nnb.plot(dl.Function(Vh[STATE], x[STATE]), subplot_loc=121,mytitle= State )\nnb.plot(dl.Function(Vh[PARAMETER], x[PARAMETER]), subplot_loc=122,mytitle= Parameter )\nplt.show()  It  cg_it  cost          misfit        reg           (g,da)       ||g||L2      alpha     tolcg         \n  1   1    1.2065e+03    1.2062e+03    3.1473e-01   -1.5705e+04   1.0425e+05   1.0e+00   5.0000e-01\n  2   3    3.4593e+02    3.4468e+02    1.2536e+00   -1.8467e+03   1.4314e+04   1.0e+00   3.7054e-01\n  3   1    2.7467e+02    2.7336e+02    1.3092e+00   -1.4246e+02   1.0037e+04   1.0e+00   3.1028e-01\n  4   7    1.6917e+02    1.6476e+02    4.4157e+00   -2.1284e+02   3.8710e+03   1.0e+00   1.9269e-01\n  5   6    1.5732e+02    1.5229e+02    5.0283e+00   -2.3456e+01   1.8211e+03   1.0e+00   1.3216e-01\n  6  14    1.4249e+02    1.2974e+02    1.2743e+01   -2.9882e+01   1.1574e+03   1.0e+00   1.0536e-01\n  7   2    1.4216e+02    1.2941e+02    1.2746e+01   -6.6442e-01   7.3260e+02   1.0e+00   8.3827e-02\n  8  22    1.4079e+02    1.2532e+02    1.5471e+01   -2.7344e+00   4.4098e+02   1.0e+00   6.5037e-02\n  9  14    1.4078e+02    1.2537e+02    1.5404e+01   -1.7445e-02   5.0571e+01   1.0e+00   2.2024e-02\n 10  29    1.4078e+02    1.2533e+02    1.5451e+01   -1.8196e-03   1.5024e+01   1.0e+00   1.2004e-02\n 11  38    1.4078e+02    1.2533e+02    1.5451e+01   -2.5035e-07   1.3906e-01   1.0e+00   1.1549e-03\n 12  53    1.4078e+02    1.2533e+02    1.5451e+01   -1.3812e-12   2.6266e-04   1.0e+00   5.0194e-05\n\nConverged in  12  iterations.\nTermination reason:  Norm of the gradient less than tolerance\nFinal gradient norm:  1.63019593759e-08\nFinal cost:  140.781736843", 
            "title": "7. Compute the MAP point"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#8-compute-the-low-rank-gaussian-approximation-of-the-posterior", 
            "text": "We used the  double pass  algorithm to compute a low-rank decomposition of the Hessian Misfit.\nIn particular, we solve    \\Hmisfit {\\bf u} = \\lambda \\prcov^{-1} {\\bf u}.    The Figure shows the largest  k  generalized eigenvectors of the Hessian misfit.\nThe effective rank of the Hessian misfit is the number of eigenvalues above the red line (y=1).\nThe effective rank is independent of the mesh size.  model.setPointForHessianEvaluations(x)\nHmisfit = ReducedHessian(model, solver.parameters[ inner_rel_tolerance ], gauss_newton_approx=False, misfit_only=True)\nk = 50\np = 20\nprint  Single/Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}. .format(k,p)\nOmega = np.random.randn(x[PARAMETER].array().shape[0], k+p)\nd, U = doublePassG(Hmisfit, prior.R, prior.Rsolver, Omega, k)\n\nposterior = GaussianLRPosterior(prior, d, U)\nposterior.mean = x[PARAMETER]\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh[PARAMETER], U, mytitle= Eigenvector , which=[0,1,2,5,10,15])  Single/Double Pass Algorithm. Requested eigenvectors: 50; Oversampling 20.", 
            "title": "8. Compute the low rank Gaussian approximation of the posterior"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#9-prior-and-posterior-pointwise-variance-fields", 
            "text": "compute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method= Estimator , tol=5e-2, min_iter=20, max_iter=2000)\n    print  Posterior trace {0:5e}; Prior trace {1:5e}; Correction trace {2:5e} .format(post_tr, prior_tr, corr_tr)\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance( Exact )\n\nobjs = [dl.Function(Vh[PARAMETER], pr_pw_variance),\n        dl.Function(Vh[PARAMETER], post_pw_variance)]\nmytitles = [ Prior variance ,  Posterior variance ]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()  Posterior trace 1.143368e-01; Prior trace 4.034355e-01; Correction trace 2.890987e-01", 
            "title": "9. Prior and posterior pointwise variance fields"
        }, 
        {
            "location": "/tutorials/3_SubsurfaceBayesian/#10-generate-samples-from-prior-and-posterior", 
            "text": "nsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise, noise )\nnoise_size = noise.array().shape[0]\ns_prior = dl.Function(Vh[PARAMETER], name= sample_prior )\ns_post = dl.Function(Vh[PARAMETER], name= sample_post )\n\npr_max   =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min   = -2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.min()\nps_max   =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min   = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121, mytitle= Prior sample ,     vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post,  subplot_loc=122, mytitle= Posterior sample , vmin=ps_min, vmax=ps_max)\n    plt.show()       Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "10. Generate samples from Prior and Posterior"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/", 
            "text": "\\def\\D{\\mathcal{D}}\n\\def\\ipar{m}\n\\def\\R{\\mathbb{R}}\n\\def\\del{\\partial}\n\\def\\vec{\\bf}\n\\def\\priorm{\\mu_0}\n\\def\\C{\\mathcal{C}}\n\\def\\Acal{\\mathcal{A}}\n\\def\\postm{\\mu_{\\rm{post}}}\n\\def\\iparpost{\\ipar_\\text{post}}\n\\def\\obs{\\vec{d}} \n\\def\\yobs{\\obs^{\\text{obs}}}\n\\def\\obsop{\\mathcal{B}}\n\\def\\dd{\\vec{\\bar{d}}}\n\\def\\iFF{\\mathcal{F}}\n\\def\\iFFadj{\\mathcal{F}^*}\n\\def\\ncov{\\Gamma_{\\mathrm{noise}}}\n\n\n\n\n\nExample: Bayesian initial condition inversion in an advection-diffusion problem\n\n\nIn this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.\n\n\nThe Bayesian inverse problem:\n\n\nFollowing the Bayesian framework, we utilize \na Gaussian prior measure \n\\priorm = \\mathcal{N}(\\ipar_0,\\C_0)\n,\nwith \n\\C_0=\\Acal^{-2}\n where \n\\Acal\n is an elliptic differential operator as \ndescribed in the PoissonBayesian example, and use an additive\nGaussian noise model. Therefore, the solution of the Bayesian inverse\nproblem is the posterior measure, \n\\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post})\n with\n\n\\iparpost\n and \n\\C_\\text{post}\n.\n\n\n\n\nThe posterior mean \n\\iparpost\n is characterized as the minimizer of\n\n\n\n\n\n\n\n\\begin{aligned}\n& \\mathcal{J}(\\ipar) :=\n  \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs  \\right\\|^2_{\\ncov^{-1}}\n  + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)},\n\\end{aligned}\n\n\n\n\n\nwhich can also be interpreted as the regularized functional to be\nminimized in deterministic inversion. The observation operator \n\\mathcal{B}\n extracts the values of the forward solution \nu\n on a set of\nlocations \n\\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D\n at\ntimes \n\\{t_1, \\ldots, t_N\\} \\subset [0, T]\n.\n\n\n\n\nThe posterior covariance \n\\C_{\\text{post}}\n is the inverse of the Hessian of \n\\mathcal{J}(\\ipar)\n, i.e.,\n\n\n\n\n\n\n\n\\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.\n\n\n\n\n\nThe forward problem:\n\n\nThe PDE in the parameter-to-observable map \n\\iFF\n models diffusive transport\nin a domain \n\\D \\subset \\R^d\n (\nd \\in \\{2, 3\\}\n):\n\n\n\n\n\n\\begin{split}\nu_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0     & \\quad \\text{in } \\D\\times(0,T),\\\\\n                                 u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\\n                \\kappa \\nabla u\\cdot \\vec{n} &= 0     & \\quad \\text{on } \\partial\\D \\times (0,T).\n\\end{split}\n\n\n\n\n\nHere, \n\\kappa > 0\n is the diffusion coefficient and \nT > 0\n is the final\ntime. The velocity field\n\n\\vec{v}\n is computed by solving the following steady-state\nNavier-Stokes equation with the side walls driving the flow:\n\n\n\n\n\n\\begin{aligned}\n- \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\\n\\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\\n\\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D.\n\\end{aligned}\n\n\n\n\n\nHere, \nq\n is pressure, \n\\text{Re}\n is the Reynolds number. The Dirichlet boundary data\n\n\\vec{g} \\in \\R^d\n is given by \n\n\\vec{g} = \\vec{e}_2\n on the left wall of the domain, \n\n\\vec{g}=-\\vec{e}_2\n on the right wall,  and \n\\vec{g} = \\vec{0}\n everywhere else.\n\n\nThe adjoint problem:\n\n\n\n\n\n\\begin{aligned}\n-p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p  &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\\n                                      p(\\cdot, T) &= 0             &       &\\text{ in } \\D,\\\\ \n(\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n}            &= 0             &       &\\text{ on } \\partial\\D\\times (0,T).\n\\end{aligned}\n\n\n\n\n\n1. Load modules\n\n\nimport dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nsys.path.append( \n../\n )\nfrom hippylib import *\nsys.path.append( \n../applications/ad_diff/\n )\nfrom model_ad_diff import TimeDependentAD\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(1)\n\n\n\n\n2. Construct the velocity field\n\n\ndef v_boundary(x,on_boundary):\n    return on_boundary\n\ndef q_boundary(x,on_boundary):\n    return x[0] \n dl.DOLFIN_EPS and x[1] \n dl.DOLFIN_EPS\n\ndef computeVelocityField(mesh):\n    Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2)\n    Wh = dl.FunctionSpace(mesh, 'Lagrange', 1)\n    if dlversion() \n= (1,6,0):\n        XW = dl.MixedFunctionSpace([Xh, Wh])\n    else:\n        mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()])\n        XW = dl.FunctionSpace(mesh, mixed_element)\n\n    Re = 1e2\n\n    g = dl.Expression(('0.0','(x[0] \n 1e-14) - (x[0] \n 1 - 1e-14)'), degree=1)\n    bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary)\n    bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise')\n    bcs = [bc1, bc2]\n\n    vq = dl.Function(XW)\n    (v,q) = dl.split(vq)\n    (v_test, q_test) = dl.TestFunctions (XW)\n\n    def strain(v):\n        return dl.sym(dl.nabla_grad(v))\n\n    F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test)\n           - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx\n\n    dl.solve(F == 0, vq, bcs, solver_parameters={\nnewton_solver\n:\n                                         {\nrelative_tolerance\n:1e-4, \nmaximum_iterations\n:100}})\n\n    plt.figure(figsize=(15,5))\n    vh = dl.project(v,Xh)\n    qh = dl.project(q,Wh)\n    nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle=\nVelocity\n)\n    nb.plot(qh, subplot_loc=122,mytitle=\nPressure\n)\n    plt.show()\n\n    return v\n\n\n\n\n3. Set up the mesh and finite element spaces\n\n\nmesh = dl.refine( dl.Mesh(\nad_20.xml\n) )\nwind_velocity = computeVelocityField(mesh)\nVh = dl.FunctionSpace(mesh, \nLagrange\n, 1)\nprint \nNumber of dofs: {0}\n.format( Vh.dim() )\n\n\n\n\n\n\nNumber of dofs: 2023\n\n\n\n4. Set up model (prior, true/proposed initial condition)\n\n\n#gamma = 1\n#delta = 10\n#prior = LaplacianPrior(Vh, gamma, delta)\n\ngamma = 1\ndelta = 8\nprior = BiLaplacianPrior(Vh, gamma, delta)\n\nprior.mean = dl.interpolate(dl.Constant(0.5), Vh).vector()\ntrue_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))', degree=5), Vh).vector()\nproblem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior)\n\nobjs = [dl.Function(Vh,true_initial_condition),\n        dl.Function(Vh,prior.mean)]\nmytitles = [\nTrue Initial Condition\n, \nPrior mean\n]\nnb.multi1_plot(objs, mytitles)\nplt.show()\n\n\n\n\n\n\n5. Generate the synthetic observations\n\n\nrel_noise = 0.001\nutrue = problem.generate_vector(STATE)\nx = [utrue, true_initial_condition, None]\nproblem.solveFwd(x[STATE], x, 1e-9)\nMAX = utrue.norm(\nlinf\n, \nlinf\n)\nnoise_std_dev = rel_noise * MAX\nproblem.ud.copy(utrue)\nproblem.ud.randn_perturb(noise_std_dev)\nproblem.noise_variance = noise_std_dev*noise_std_dev\n\nnb.show_solution(Vh, true_initial_condition, utrue, \nSolution\n)\n\n\n\n\n\n\n6. Test the gradient and the Hessian of the cost (negative log posterior)\n\n\na0 = true_initial_condition.copy()\nmodelVerify(problem, a0, 1e-12, is_quadratic=True)\n\n\n\n\n(yy, H xx) - (xx, H yy) =  -4.75341826113e-14\n\n\n\n\n\n7. Evaluate the gradient\n\n\n[u,a,p] = problem.generate_vector()\nproblem.solveFwd(u, [u,a,p], 1e-12)\nproblem.solveAdj(p, [u,a,p], 1e-12)\nmg = problem.generate_vector(PARAMETER)\ngrad_norm = problem.evalGradientParameter([u,a,p], mg)\n\nprint \n(g,g) = \n, grad_norm\n\n\n\n\n(g,g) =  1.66716039169e+12\n\n\n\n8. The Gaussian approximation of the posterior\n\n\nH = ReducedHessian(problem, 1e-12, gauss_newton_approx=False, misfit_only=True) \n\nk = 80\np = 20\nprint \nSingle Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\n.format(k,p)\nOmega = np.random.randn(a.array().shape[0], k+p)\nd, U = singlePassG(H, prior.R, prior.Rsolver, Omega, k)\n\n\nposterior = GaussianLRPosterior( prior, d, U )\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh, U, mytitle=\nEigenvector\n, which=[0,1,2,5,10,20,30,45,60])\n\n\n\n\nSingle Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.\n\n\n\n\n\n\n\n9. Compute the MAP point\n\n\nH.misfit_only = False\n\nsolver = CGSolverSteihaug()\nsolver.set_operator(H)\nsolver.set_preconditioner( posterior.Hlr )\nsolver.parameters[\nprint_level\n] = 1\nsolver.parameters[\nrel_tolerance\n] = 1e-6\nsolver.solve(a, -mg)\nproblem.solveFwd(u, [u,a,p], 1e-12)\n\ntotal_cost, reg_cost, misfit_cost = problem.cost([u,a,p])\nprint \nTotal cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g}\n.format(total_cost, reg_cost, misfit_cost)\n\nposterior.mean = a\n\nplt.figure(figsize=(7.5,5))\nnb.plot(dl.Function(Vh, a), mytitle=\nInitial Condition\n)\nplt.show()\n\nnb.show_solution(Vh, a, u, \nSolution\n)\n\n\n\n\n Iterartion :  0  (B r, r) =  30140.7469691\n Iteration :  1  (B r, r) =  0.0653735328531\n Iteration :  2  (B r, r) =  6.27997089018e-06\n Iteration :  3  (B r, r) =  9.56996790758e-10\nRelative/Absolute residual less than tol\nConverged in  3  iterations with final norm  3.09353647264e-05\nTotal cost 84.2612; Reg Cost 68.8823; Misfit 15.3789\n\n\n\n\n\n\n\n10. Prior and posterior pointwise variance fields\n\n\ncompute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method=\nEstimator\n, tol=5e-2, min_iter=20, max_iter=2000)\n    print \nPosterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g}\n.format(post_tr, prior_tr, corr_tr)\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance(\nExact\n)\n\nobjs = [dl.Function(Vh, pr_pw_variance),\n        dl.Function(Vh, post_pw_variance)]\nmytitles = [\nPrior Variance\n, \nPosterior Variance\n]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()\n\n\n\n\nPosterior trace 0.000465642; Prior trace 0.0284301; Correction trace 0.0279644\n\n\n\n\n\n11. Draw samples from the prior and posterior distributions\n\n\nnsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise,\nnoise\n)\nnoise_size = noise.array().shape[0]\ns_prior = dl.Function(Vh, name=\nsample_prior\n)\ns_post = dl.Function(Vh, name=\nsample_post\n)\n\npr_max =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min()\nps_max =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle=\nPrior sample\n, vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle=\nPosterior sample\n, vmin=ps_min, vmax=ps_max)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "Advection-Diffusion Bayesian"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#example-bayesian-initial-condition-inversion-in-an-advection-diffusion-problem", 
            "text": "In this example we tackle the problem of quantifying the uncertainty in the solution of an inverse problem governed by a parabolic PDE via the Bayesian inference framework. The underlying PDE is a time-dependent advection-diffusion equation in which we seek to infer an unknown initial condition from spatio-temporal point measurements.  The Bayesian inverse problem:  Following the Bayesian framework, we utilize \na Gaussian prior measure  \\priorm = \\mathcal{N}(\\ipar_0,\\C_0) ,\nwith  \\C_0=\\Acal^{-2}  where  \\Acal  is an elliptic differential operator as \ndescribed in the PoissonBayesian example, and use an additive\nGaussian noise model. Therefore, the solution of the Bayesian inverse\nproblem is the posterior measure,  \\postm = \\mathcal{N}(\\iparpost,\\C_\\text{post})  with \\iparpost  and  \\C_\\text{post} .   The posterior mean  \\iparpost  is characterized as the minimizer of    \n\\begin{aligned}\n& \\mathcal{J}(\\ipar) :=\n  \\frac{1}{2} \\left\\| \\mathcal{B}u(\\ipar) -\\obs  \\right\\|^2_{\\ncov^{-1}}\n  + \\frac 12 \\left\\| \\Acal(\\ipar - \\ipar_0 \\right)\\|^2_{L^2(\\D)},\n\\end{aligned}   which can also be interpreted as the regularized functional to be\nminimized in deterministic inversion. The observation operator  \\mathcal{B}  extracts the values of the forward solution  u  on a set of\nlocations  \\{\\vec{x}_1, \\ldots, \\vec{x}_n\\} \\subset \\D  at\ntimes  \\{t_1, \\ldots, t_N\\} \\subset [0, T] .   The posterior covariance  \\C_{\\text{post}}  is the inverse of the Hessian of  \\mathcal{J}(\\ipar) , i.e.,    \n\\C_{\\text{post}} = (\\iFFadj \\ncov^{-1} \\iFF + \\C_0^{-1})^{-1}.   The forward problem:  The PDE in the parameter-to-observable map  \\iFF  models diffusive transport\nin a domain  \\D \\subset \\R^d  ( d \\in \\{2, 3\\} ):   \n\\begin{split}\nu_t - \\kappa\\Delta u + \\bf{v} \\cdot \\nabla u &= 0     & \\quad \\text{in } \\D\\times(0,T),\\\\\n                                 u(\\cdot, 0) &= \\ipar & \\quad \\text{in } \\D,\\\\\n                \\kappa \\nabla u\\cdot \\vec{n} &= 0     & \\quad \\text{on } \\partial\\D \\times (0,T).\n\\end{split}   Here,  \\kappa > 0  is the diffusion coefficient and  T > 0  is the final\ntime. The velocity field \\vec{v}  is computed by solving the following steady-state\nNavier-Stokes equation with the side walls driving the flow:   \n\\begin{aligned}\n- \\frac{1}{\\operatorname{Re}} \\Delta \\bf{v} + \\nabla q + \\bf{v} \\cdot \\nabla \\bf{v} &= 0 &\\quad&\\text{ in }\\D,\\\\\n\\nabla \\cdot \\bf{v} &= 0 &&\\text{ in }\\D,\\\\\n\\bf{v} &= \\bf{g} &&\\text{ on } \\partial\\D.\n\\end{aligned}   Here,  q  is pressure,  \\text{Re}  is the Reynolds number. The Dirichlet boundary data \\vec{g} \\in \\R^d  is given by  \\vec{g} = \\vec{e}_2  on the left wall of the domain,  \\vec{g}=-\\vec{e}_2  on the right wall,  and  \\vec{g} = \\vec{0}  everywhere else.  The adjoint problem:   \n\\begin{aligned}\n-p_t - \\nabla \\cdot (p \\vec{v}) - \\kappa \\Delta p  &= -\\obsop^* (\\obsop u - \\obs) & \\quad &\\text{ in } \\D\\times (0,T),\\\\\n                                      p(\\cdot, T) &= 0             &       &\\text{ in } \\D,\\\\ \n(\\vec{v}p+\\kappa\\nabla p)\\cdot \\vec{n}            &= 0             &       &\\text{ on } \\partial\\D\\times (0,T).\n\\end{aligned}", 
            "title": "Example: Bayesian initial condition inversion in an advection-diffusion problem"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#1-load-modules", 
            "text": "import dolfin as dl\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sys\nsys.path.append(  ../  )\nfrom hippylib import *\nsys.path.append(  ../applications/ad_diff/  )\nfrom model_ad_diff import TimeDependentAD\n\nimport nb\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\nnp.random.seed(1)", 
            "title": "1. Load modules"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#2-construct-the-velocity-field", 
            "text": "def v_boundary(x,on_boundary):\n    return on_boundary\n\ndef q_boundary(x,on_boundary):\n    return x[0]   dl.DOLFIN_EPS and x[1]   dl.DOLFIN_EPS\n\ndef computeVelocityField(mesh):\n    Xh = dl.VectorFunctionSpace(mesh,'Lagrange', 2)\n    Wh = dl.FunctionSpace(mesh, 'Lagrange', 1)\n    if dlversion()  = (1,6,0):\n        XW = dl.MixedFunctionSpace([Xh, Wh])\n    else:\n        mixed_element = dl.MixedElement([Xh.ufl_element(), Wh.ufl_element()])\n        XW = dl.FunctionSpace(mesh, mixed_element)\n\n    Re = 1e2\n\n    g = dl.Expression(('0.0','(x[0]   1e-14) - (x[0]   1 - 1e-14)'), degree=1)\n    bc1 = dl.DirichletBC(XW.sub(0), g, v_boundary)\n    bc2 = dl.DirichletBC(XW.sub(1), dl.Constant(0), q_boundary, 'pointwise')\n    bcs = [bc1, bc2]\n\n    vq = dl.Function(XW)\n    (v,q) = dl.split(vq)\n    (v_test, q_test) = dl.TestFunctions (XW)\n\n    def strain(v):\n        return dl.sym(dl.nabla_grad(v))\n\n    F = ( (2./Re)*dl.inner(strain(v),strain(v_test))+ dl.inner (dl.nabla_grad(v)*v, v_test)\n           - (q * dl.div(v_test)) + ( dl.div(v) * q_test) ) * dl.dx\n\n    dl.solve(F == 0, vq, bcs, solver_parameters={ newton_solver :\n                                         { relative_tolerance :1e-4,  maximum_iterations :100}})\n\n    plt.figure(figsize=(15,5))\n    vh = dl.project(v,Xh)\n    qh = dl.project(q,Wh)\n    nb.plot(nb.coarsen_v(vh), subplot_loc=121,mytitle= Velocity )\n    nb.plot(qh, subplot_loc=122,mytitle= Pressure )\n    plt.show()\n\n    return v", 
            "title": "2. Construct the velocity field"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#3-set-up-the-mesh-and-finite-element-spaces", 
            "text": "mesh = dl.refine( dl.Mesh( ad_20.xml ) )\nwind_velocity = computeVelocityField(mesh)\nVh = dl.FunctionSpace(mesh,  Lagrange , 1)\nprint  Number of dofs: {0} .format( Vh.dim() )   Number of dofs: 2023", 
            "title": "3. Set up the mesh and finite element spaces"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#4-set-up-model-prior-trueproposed-initial-condition", 
            "text": "#gamma = 1\n#delta = 10\n#prior = LaplacianPrior(Vh, gamma, delta)\n\ngamma = 1\ndelta = 8\nprior = BiLaplacianPrior(Vh, gamma, delta)\n\nprior.mean = dl.interpolate(dl.Constant(0.5), Vh).vector()\ntrue_initial_condition = dl.interpolate(dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))', degree=5), Vh).vector()\nproblem = TimeDependentAD(mesh, [Vh,Vh,Vh], 0., 4., 1., .2, wind_velocity, True, prior)\n\nobjs = [dl.Function(Vh,true_initial_condition),\n        dl.Function(Vh,prior.mean)]\nmytitles = [ True Initial Condition ,  Prior mean ]\nnb.multi1_plot(objs, mytitles)\nplt.show()", 
            "title": "4. Set up model (prior, true/proposed initial condition)"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#5-generate-the-synthetic-observations", 
            "text": "rel_noise = 0.001\nutrue = problem.generate_vector(STATE)\nx = [utrue, true_initial_condition, None]\nproblem.solveFwd(x[STATE], x, 1e-9)\nMAX = utrue.norm( linf ,  linf )\nnoise_std_dev = rel_noise * MAX\nproblem.ud.copy(utrue)\nproblem.ud.randn_perturb(noise_std_dev)\nproblem.noise_variance = noise_std_dev*noise_std_dev\n\nnb.show_solution(Vh, true_initial_condition, utrue,  Solution )", 
            "title": "5. Generate the synthetic observations"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#6-test-the-gradient-and-the-hessian-of-the-cost-negative-log-posterior", 
            "text": "a0 = true_initial_condition.copy()\nmodelVerify(problem, a0, 1e-12, is_quadratic=True)  (yy, H xx) - (xx, H yy) =  -4.75341826113e-14", 
            "title": "6. Test the gradient and the Hessian of the cost (negative log posterior)"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#7-evaluate-the-gradient", 
            "text": "[u,a,p] = problem.generate_vector()\nproblem.solveFwd(u, [u,a,p], 1e-12)\nproblem.solveAdj(p, [u,a,p], 1e-12)\nmg = problem.generate_vector(PARAMETER)\ngrad_norm = problem.evalGradientParameter([u,a,p], mg)\n\nprint  (g,g) =  , grad_norm  (g,g) =  1.66716039169e+12", 
            "title": "7. Evaluate the gradient"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#8-the-gaussian-approximation-of-the-posterior", 
            "text": "H = ReducedHessian(problem, 1e-12, gauss_newton_approx=False, misfit_only=True) \n\nk = 80\np = 20\nprint  Single Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}. .format(k,p)\nOmega = np.random.randn(a.array().shape[0], k+p)\nd, U = singlePassG(H, prior.R, prior.Rsolver, Omega, k)\n\n\nposterior = GaussianLRPosterior( prior, d, U )\n\nplt.plot(range(0,k), d, 'b*', range(0,k+1), np.ones(k+1), '-r')\nplt.yscale('log')\nplt.xlabel('number')\nplt.ylabel('eigenvalue')\n\nnb.plot_eigenvectors(Vh, U, mytitle= Eigenvector , which=[0,1,2,5,10,20,30,45,60])  Single Pass Algorithm. Requested eigenvectors: 80; Oversampling 20.", 
            "title": "8. The Gaussian approximation of the posterior"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#9-compute-the-map-point", 
            "text": "H.misfit_only = False\n\nsolver = CGSolverSteihaug()\nsolver.set_operator(H)\nsolver.set_preconditioner( posterior.Hlr )\nsolver.parameters[ print_level ] = 1\nsolver.parameters[ rel_tolerance ] = 1e-6\nsolver.solve(a, -mg)\nproblem.solveFwd(u, [u,a,p], 1e-12)\n\ntotal_cost, reg_cost, misfit_cost = problem.cost([u,a,p])\nprint  Total cost {0:5g}; Reg Cost {1:5g}; Misfit {2:5g} .format(total_cost, reg_cost, misfit_cost)\n\nposterior.mean = a\n\nplt.figure(figsize=(7.5,5))\nnb.plot(dl.Function(Vh, a), mytitle= Initial Condition )\nplt.show()\n\nnb.show_solution(Vh, a, u,  Solution )   Iterartion :  0  (B r, r) =  30140.7469691\n Iteration :  1  (B r, r) =  0.0653735328531\n Iteration :  2  (B r, r) =  6.27997089018e-06\n Iteration :  3  (B r, r) =  9.56996790758e-10\nRelative/Absolute residual less than tol\nConverged in  3  iterations with final norm  3.09353647264e-05\nTotal cost 84.2612; Reg Cost 68.8823; Misfit 15.3789", 
            "title": "9. Compute the MAP point"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#10-prior-and-posterior-pointwise-variance-fields", 
            "text": "compute_trace = True\nif compute_trace:\n    post_tr, prior_tr, corr_tr = posterior.trace(method= Estimator , tol=5e-2, min_iter=20, max_iter=2000)\n    print  Posterior trace {0:5g}; Prior trace {1:5g}; Correction trace {2:5g} .format(post_tr, prior_tr, corr_tr)\npost_pw_variance, pr_pw_variance, corr_pw_variance = posterior.pointwise_variance( Exact )\n\nobjs = [dl.Function(Vh, pr_pw_variance),\n        dl.Function(Vh, post_pw_variance)]\nmytitles = [ Prior Variance ,  Posterior Variance ]\nnb.multi1_plot(objs, mytitles, logscale=True)\nplt.show()  Posterior trace 0.000465642; Prior trace 0.0284301; Correction trace 0.0279644", 
            "title": "10. Prior and posterior pointwise variance fields"
        }, 
        {
            "location": "/tutorials/4_AdvectionDiffusionBayesian/#11-draw-samples-from-the-prior-and-posterior-distributions", 
            "text": "nsamples = 5\nnoise = dl.Vector()\nposterior.init_vector(noise, noise )\nnoise_size = noise.array().shape[0]\ns_prior = dl.Function(Vh, name= sample_prior )\ns_post = dl.Function(Vh, name= sample_post )\n\npr_max =  2.5*math.sqrt( pr_pw_variance.max() ) + prior.mean.max()\npr_min = -2.5*math.sqrt( pr_pw_variance.min() ) + prior.mean.min()\nps_max =  2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.max()\nps_min = -2.5*math.sqrt( post_pw_variance.max() ) + posterior.mean.min()\n\nfor i in range(nsamples):\n    noise.set_local( np.random.randn( noise_size ) )\n    posterior.sample(noise, s_prior.vector(), s_post.vector())\n    plt.figure(figsize=(15,5))\n    nb.plot(s_prior, subplot_loc=121,mytitle= Prior sample , vmin=pr_min, vmax=pr_max)\n    nb.plot(s_post, subplot_loc=122,mytitle= Posterior sample , vmin=ps_min, vmax=ps_max)\n    plt.show()       Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "11. Draw samples from the prior and posterior distributions"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/", 
            "text": "Spectrum of the Reduced Hessian\n\n\nThe linear source inversion problem\n\n\nWe consider the following linear source inversion problem.\nFind the state \nu \\in H^1_{\\Gamma_D}(\\Omega)\n and the source (\nparameter\n) \na \\in H^1(\\Omega)\n that solves\n\n\\begin{align*}\n{} & \\min_a \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|a-a_0|^2 + \\gamma|\\nabla (a - a_0)|^2 \\right] dx & {}\\\\\n{\\rm s.t.} & {} &{} \\\\\n{} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = a & {\\rm in} \\; \\Omega\\\\\n{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n\\end{align*}\n\n\n\n\nHere:\n\n\n\n\n\n\n\n\nu_d\n is a \nn_{\\rm obs}\n finite dimensional vector that denotes noisy observations of the state \nu\n in \nn_{\\rm obs}\n locations \n\\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}}\n. More specifically, \nu_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i\n, where \n\\eta_i\n are i.i.d. \n\\mathcal{N}(0, \\sigma^2)\n.\n\n\n\n\n\n\n\n\nB: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}}\n is the linear operator that evaluates the state \nu\n at the observation locations \n\\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}}\n.\n\n\n\n\n\n\n\n\n\\delta\n and \n\\gamma\n are the parameters of the regularization penalizing the \nL^2(\\Omega)\n and \nH^1(\\Omega)\n norm of \na-a_0\n, respectively.\n\n\n\n\n\n\n\n\nk\n, \n{\\bf v}\n, \nc\n are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively.\n\n\n\n\n\n\n\n\n\\Gamma_D \\subset \\partial \\Omega\n, \n\\Gamma_N \\subset \\partial \\Omega\n represents the subdomain of \n\\partial\\Omega\n where we impose Dirichlet or Neumann boundary conditions, respectively.\n\n\n\n\n\n\n1. Load modules\n\n\nimport dolfin as dl\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nimport sys\nsys.path.append(\n../\n)\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)\n\n\n\n\n2. The linear source inversion problem\n\n\ndef pde_varf(u,a,p):\n    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n           + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\\n           + c*u*p*dl.dx \\\n           - a*p*dl.dx\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and x[1] \n dl.DOLFIN_EPS\n\ndef solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True):\n    np.random.seed(seed=2)\n    mesh = dl.UnitSquareMesh(nx, ny)\n    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n\n    Vh = [Vh1, Vh1, Vh1]\n    if verbose:\n        print \nNumber of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2}\n.format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())\n\n\n    u_bdr = dl.Constant(0.0)\n    u_bdr0 = dl.Constant(0.0)\n    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n    bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\n    atrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector()\n    a0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector()\n\n    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n    if verbose:\n        print \nNumber of observation points: {0}\n.format(targets.shape[0])\n\n    misfit = PointwiseStateObservation(Vh[STATE], targets)\n\n    reg = LaplacianPrior(Vh[PARAMETER], gamma, delta)\n\n    #Generate synthetic observations\n    utrue = pde.generate_state()\n    x = [utrue, atrue, None]\n    pde.solveFwd(x[STATE], x, 1e-9)\n    misfit.B.mult(x[STATE], misfit.d)\n    MAX = misfit.d.norm(\nlinf\n)\n    noise_std_dev = rel_noise * MAX\n    randn_perturb(misfit.d, noise_std_dev)\n    misfit.noise_variance = noise_std_dev*noise_std_dev\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], atrue), mytitle = \nTrue source\n, subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], utrue), mytitle=\nTrue state\n, subplot_loc=132)\n        nb.plot_pts(targets, misfit.d,mytitle=\nObservations\n, subplot_loc=133)\n        plt.show()\n\n    model = Model(pde, reg, misfit)\n    u = model.generate_vector(STATE)\n    a = a0.copy()\n    p = model.generate_vector(ADJOINT)\n    x = [u,a,p]\n    mg = model.generate_vector(PARAMETER)\n    model.solveFwd(u, x)\n    model.solveAdj(p, x)\n    model.evalGradientParameter(x, mg)\n    model.setPointForHessianEvaluations(x)\n\n    H = ReducedHessian(model, 1e-12)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(H)\n    solver.set_preconditioner( reg.Rsolver )\n    solver.parameters[\nprint_level\n] = -1\n    solver.parameters[\nrel_tolerance\n] = 1e-9\n    solver.solve(a, -mg)\n\n    if solver.converged:\n        if verbose:\n            print \nCG converged in \n, solver.iter, \n iterations.\n\n    else:\n        print \nCG did not converged.\n\n        raise\n\n    model.solveFwd(u, x, 1e-12)\n\n    total_cost, reg_cost, misfit_cost = model.cost(x)\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], a), mytitle = \nReconstructed source\n, subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], u), mytitle=\nReconstructed state\n, subplot_loc=132)\n        nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle=\nMisfit\n, subplot_loc=133)\n        plt.show()\n\n    H.misfit_only = True\n    k_evec = 80\n    p_evec = 5\n    if verbose:\n        print \nDouble Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}.\n.format(k_evec,p_evec)\n    Omega = np.random.randn(a.array().shape[0], k_evec+p_evec)\n    d, U = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec)\n\n    if verbose:\n        plt.figure()\n        nb.plot_eigenvalues(d, mytitle=\nGeneralized Eigenvalues\n)\n        nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle=\nEigenvectors\n, which=[0,1,2,5,10,15])\n        plt.show()\n\n    return d, U, Vh[PARAMETER], solver.iter\n\n\n\n\n\n3. Solution of the source inversion problem\n\n\nndim = 2\nnx = 32\nny = 32\n\nntargets = 300\nnp.random.seed(seed=1)\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nrel_noise = 0.01\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nd, U, Va, nit = solve(nx,ny, targets, rel_noise, gamma, delta)\n\n\n\n\nNumber of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089\nNumber of observation points: 300\n\n\n\n\n\nCG converged in  75  iterations.\n\n\n\n\n\nDouble Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.\n\n\n\n\n\n\n\n4. Mesh independence of the spectrum of the preconditioned Hessian\n\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nn = [16,32,64]\nd1, U1, Va1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False)\n\nprint \nNumber of Iterations: \n, niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\nEigenvalues Mesh {0} by {1}\n.format(n[0],n[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\nEigenvalues Mesh {0} by {1}\n.format(n[1],n[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\nEigenvalues Mesh {0} by {1}\n.format(n[2],n[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\nMesh {0} by {1} Eigen\n.format(n[0],n[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\nMesh {0} by {1} Eigen\n.format(n[1],n[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\nMesh {0} by {1} Eigen\n.format(n[2],n[2]), which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  62 73 78\n\n\n\n\n\n\n\n\n\n\n\n5. Dependence on the noise level\n\n\nWe solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization.\n\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nrel_noise = [1e-3,1e-2,1e-1]\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False)\n\nprint \nNumber of Iterations: \n, niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\nEigenvalues rel_noise {0:g}\n.format(rel_noise[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\nEigenvalues rel_noise {0:g}\n.format(rel_noise[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\nEigenvalues rel_noise {0:g}\n.format(rel_noise[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\nrel_noise {0:g} Eigen\n.format(rel_noise[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\nrel_noise {0:g} Eigen\n.format(rel_noise[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\nrel_noise {0:g} Eigen\n.format(rel_noise[2]), which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  162 75 21\n\n\n\n\n\n\n\n\n\n\n\n6. Dependence on the PDE coefficients\n\n\nAssume a constant reaction term \nc = 1\n, and we consider different values for the diffusivity coefficient \nk\n.\n\n\nThe smaller the value of \nk\n the slower the decay in the spectrum.\n\n\nrel_noise = 0.01\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(1.0)\n\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.1)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.01)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\n\nprint \nNumber of Iterations: \n, niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle=\nEigenvalues k=1.0\n, subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle=\nEigenvalues k=0.1\n, subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle=\nEigenvalues k=0.01\n, subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle=\nk=1. Eigen\n, which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle=\nk=0.1 Eigen\n, which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle=\nk=0.01 Eigen\n, which=[0,1,5])\n\nplt.show()\n\n\n\n\nNumber of Iterations:  91 148 250\n\n\n\n\n\n\n\n\n\n\n\nCopyright (c) 2016, The University of Texas at Austin \n University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.\n\n\nThis file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.\n\n\nhIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "Hessian Spectrum"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/#spectrum-of-the-reduced-hessian", 
            "text": "", 
            "title": "Spectrum of the Reduced Hessian"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/#the-linear-source-inversion-problem", 
            "text": "We consider the following linear source inversion problem.\nFind the state  u \\in H^1_{\\Gamma_D}(\\Omega)  and the source ( parameter )  a \\in H^1(\\Omega)  that solves \\begin{align*}\n{} & \\min_a \\frac{1}{2\\sigma^2} \\| Bu - u_d \\|^2 + \\frac{1}{2} \\int_\\Omega \\left[ \\delta|a-a_0|^2 + \\gamma|\\nabla (a - a_0)|^2 \\right] dx & {}\\\\\n{\\rm s.t.} & {} &{} \\\\\n{} & -{\\rm div}(k \\nabla u) + {\\bf v}\\cdot \\nabla u + cu = a & {\\rm in} \\; \\Omega\\\\\n{} & u = 0 & {\\rm on } \\; \\Gamma_D\\\\\n{} & k \\frac{\\partial u}{\\partial n} = 0 & {\\rm on } \\; \\Gamma_N\\\\\n\\end{align*}   Here:     u_d  is a  n_{\\rm obs}  finite dimensional vector that denotes noisy observations of the state  u  in  n_{\\rm obs}  locations  \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} . More specifically,  u_d(i) = u_{\\rm true}( {\\bf x}_i ) + \\eta_i , where  \\eta_i  are i.i.d.  \\mathcal{N}(0, \\sigma^2) .     B: H^1_0(\\Omega) \\rightarrow \\mathbb{R}^{n_{\\rm obs}}  is the linear operator that evaluates the state  u  at the observation locations  \\{ {\\bf x}_i\\}_{i=1}^{n_{\\rm obs}} .     \\delta  and  \\gamma  are the parameters of the regularization penalizing the  L^2(\\Omega)  and  H^1(\\Omega)  norm of  a-a_0 , respectively.     k ,  {\\bf v} ,  c  are given coefficients representing the diffusivity coefficient, the advective velocity and the reaction term, respectively.     \\Gamma_D \\subset \\partial \\Omega ,  \\Gamma_N \\subset \\partial \\Omega  represents the subdomain of  \\partial\\Omega  where we impose Dirichlet or Neumann boundary conditions, respectively.", 
            "title": "The linear source inversion problem"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/#1-load-modules", 
            "text": "import dolfin as dl\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport nb\n\nimport sys\nsys.path.append( ../ )\nfrom hippylib import *\n\nimport logging\nlogging.getLogger('FFC').setLevel(logging.WARNING)\nlogging.getLogger('UFL').setLevel(logging.WARNING)\ndl.set_log_active(False)", 
            "title": "1. Load modules"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/#2-the-linear-source-inversion-problem", 
            "text": "def pde_varf(u,a,p):\n    return k*dl.inner(dl.nabla_grad(u), dl.nabla_grad(p))*dl.dx \\\n           + dl.inner(dl.nabla_grad(u), v*p)*dl.dx \\\n           + c*u*p*dl.dx \\\n           - a*p*dl.dx\n\ndef u_boundary(x, on_boundary):\n    return on_boundary and x[1]   dl.DOLFIN_EPS\n\ndef solve(nx,ny, targets, rel_noise, gamma, delta, verbose=True):\n    np.random.seed(seed=2)\n    mesh = dl.UnitSquareMesh(nx, ny)\n    Vh1 = dl.FunctionSpace(mesh, 'Lagrange', 1)\n\n    Vh = [Vh1, Vh1, Vh1]\n    if verbose:\n        print  Number of dofs: STATE={0}, PARAMETER={1}, ADJOINT={2} .format(Vh[STATE].dim(), Vh[PARAMETER].dim(), Vh[ADJOINT].dim())\n\n\n    u_bdr = dl.Constant(0.0)\n    u_bdr0 = dl.Constant(0.0)\n    bc = dl.DirichletBC(Vh[STATE], u_bdr, u_boundary)\n    bc0 = dl.DirichletBC(Vh[STATE], u_bdr0, u_boundary)\n\n    atrue = dl.interpolate( dl.Expression('min(0.5,exp(-100*(pow(x[0]-0.35,2) +  pow(x[1]-0.7,2))))',degree=5), Vh[PARAMETER]).vector()\n    a0 = dl.interpolate(dl.Constant(0.0), Vh[PARAMETER]).vector()\n\n    pde = PDEVariationalProblem(Vh, pde_varf, bc, bc0, is_fwd_linear=True)\n\n    if verbose:\n        print  Number of observation points: {0} .format(targets.shape[0])\n\n    misfit = PointwiseStateObservation(Vh[STATE], targets)\n\n    reg = LaplacianPrior(Vh[PARAMETER], gamma, delta)\n\n    #Generate synthetic observations\n    utrue = pde.generate_state()\n    x = [utrue, atrue, None]\n    pde.solveFwd(x[STATE], x, 1e-9)\n    misfit.B.mult(x[STATE], misfit.d)\n    MAX = misfit.d.norm( linf )\n    noise_std_dev = rel_noise * MAX\n    randn_perturb(misfit.d, noise_std_dev)\n    misfit.noise_variance = noise_std_dev*noise_std_dev\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], atrue), mytitle =  True source , subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], utrue), mytitle= True state , subplot_loc=132)\n        nb.plot_pts(targets, misfit.d,mytitle= Observations , subplot_loc=133)\n        plt.show()\n\n    model = Model(pde, reg, misfit)\n    u = model.generate_vector(STATE)\n    a = a0.copy()\n    p = model.generate_vector(ADJOINT)\n    x = [u,a,p]\n    mg = model.generate_vector(PARAMETER)\n    model.solveFwd(u, x)\n    model.solveAdj(p, x)\n    model.evalGradientParameter(x, mg)\n    model.setPointForHessianEvaluations(x)\n\n    H = ReducedHessian(model, 1e-12)\n\n    solver = CGSolverSteihaug()\n    solver.set_operator(H)\n    solver.set_preconditioner( reg.Rsolver )\n    solver.parameters[ print_level ] = -1\n    solver.parameters[ rel_tolerance ] = 1e-9\n    solver.solve(a, -mg)\n\n    if solver.converged:\n        if verbose:\n            print  CG converged in  , solver.iter,   iterations. \n    else:\n        print  CG did not converged. \n        raise\n\n    model.solveFwd(u, x, 1e-12)\n\n    total_cost, reg_cost, misfit_cost = model.cost(x)\n\n    if verbose:\n        plt.figure(figsize=(18,4))\n        nb.plot(dl.Function(Vh[PARAMETER], a), mytitle =  Reconstructed source , subplot_loc=131)\n        nb.plot(dl.Function(Vh[STATE], u), mytitle= Reconstructed state , subplot_loc=132)\n        nb.plot_pts(targets, misfit.B*u - misfit.d, mytitle= Misfit , subplot_loc=133)\n        plt.show()\n\n    H.misfit_only = True\n    k_evec = 80\n    p_evec = 5\n    if verbose:\n        print  Double Pass Algorithm. Requested eigenvectors: {0}; Oversampling {1}. .format(k_evec,p_evec)\n    Omega = np.random.randn(a.array().shape[0], k_evec+p_evec)\n    d, U = doublePassG(H, reg.R, reg.Rsolver, Omega, k_evec)\n\n    if verbose:\n        plt.figure()\n        nb.plot_eigenvalues(d, mytitle= Generalized Eigenvalues )\n        nb.plot_eigenvectors(Vh[PARAMETER], U, mytitle= Eigenvectors , which=[0,1,2,5,10,15])\n        plt.show()\n\n    return d, U, Vh[PARAMETER], solver.iter", 
            "title": "2. The linear source inversion problem"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/#3-solution-of-the-source-inversion-problem", 
            "text": "ndim = 2\nnx = 32\nny = 32\n\nntargets = 300\nnp.random.seed(seed=1)\ntargets = np.random.uniform(0.1,0.9, [ntargets, ndim] )\nrel_noise = 0.01\n\ngamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nd, U, Va, nit = solve(nx,ny, targets, rel_noise, gamma, delta)  Number of dofs: STATE=1089, PARAMETER=1089, ADJOINT=1089\nNumber of observation points: 300   CG converged in  75  iterations.   Double Pass Algorithm. Requested eigenvectors: 80; Oversampling 5.", 
            "title": "3. Solution of the source inversion problem"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/#4-mesh-independence-of-the-spectrum-of-the-preconditioned-hessian", 
            "text": "gamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nn = [16,32,64]\nd1, U1, Va1, niter1 = solve(n[0],n[0], targets, rel_noise, gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(n[1],n[1], targets, rel_noise, gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(n[2],n[2], targets, rel_noise, gamma, delta,verbose=False)\n\nprint  Number of Iterations:  , niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle= Eigenvalues Mesh {0} by {1} .format(n[0],n[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle= Eigenvalues Mesh {0} by {1} .format(n[1],n[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle= Eigenvalues Mesh {0} by {1} .format(n[2],n[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle= Mesh {0} by {1} Eigen .format(n[0],n[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle= Mesh {0} by {1} Eigen .format(n[1],n[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle= Mesh {0} by {1} Eigen .format(n[2],n[2]), which=[0,1,5])\n\nplt.show()  Number of Iterations:  62 73 78", 
            "title": "4. Mesh independence of the spectrum of the preconditioned Hessian"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/#5-dependence-on-the-noise-level", 
            "text": "We solve the problem for different noise levels. The higher the noise level the more important becomes the effect of the regularization.  gamma = 70.\ndelta = 1e-1\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(0.)\n\nrel_noise = [1e-3,1e-2,1e-1]\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise[0], gamma, delta,verbose=False)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise[1], gamma, delta,verbose=False)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise[2], gamma, delta,verbose=False)\n\nprint  Number of Iterations:  , niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle= Eigenvalues rel_noise {0:g} .format(rel_noise[0]), subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle= Eigenvalues rel_noise {0:g} .format(rel_noise[1]), subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle= Eigenvalues rel_noise {0:g} .format(rel_noise[2]), subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle= rel_noise {0:g} Eigen .format(rel_noise[0]), which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle= rel_noise {0:g} Eigen .format(rel_noise[1]), which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle= rel_noise {0:g} Eigen .format(rel_noise[2]), which=[0,1,5])\n\nplt.show()  Number of Iterations:  162 75 21", 
            "title": "5. Dependence on the noise level"
        }, 
        {
            "location": "/tutorials/5_HessianSpectrum/#6-dependence-on-the-pde-coefficients", 
            "text": "Assume a constant reaction term  c = 1 , and we consider different values for the diffusivity coefficient  k .  The smaller the value of  k  the slower the decay in the spectrum.  rel_noise = 0.01\n\nk = dl.Constant(1.0)\nv = dl.Constant((0.0, 0.0))\nc = dl.Constant(1.0)\n\nd1, U1, Va1, niter1 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.1)\nd2, U2, Va2, niter2 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\nk = dl.Constant(0.01)\nd3, U3, Va3, niter3 = solve(nx,ny, targets, rel_noise, gamma, delta,verbose=False)\n\nprint  Number of Iterations:  , niter1, niter2, niter3\nplt.figure(figsize=(18,4))\nnb.plot_eigenvalues(d1, mytitle= Eigenvalues k=1.0 , subplot_loc=131)\nnb.plot_eigenvalues(d2, mytitle= Eigenvalues k=0.1 , subplot_loc=132)\nnb.plot_eigenvalues(d3, mytitle= Eigenvalues k=0.01 , subplot_loc=133)\n\nnb.plot_eigenvectors(Va1, U1, mytitle= k=1. Eigen , which=[0,1,5])\nnb.plot_eigenvectors(Va2, U2, mytitle= k=0.1 Eigen , which=[0,1,5])\nnb.plot_eigenvectors(Va3, U3, mytitle= k=0.01 Eigen , which=[0,1,5])\n\nplt.show()  Number of Iterations:  91 148 250      Copyright (c) 2016, The University of Texas at Austin   University of California, Merced.\nAll Rights reserved.\nSee file COPYRIGHT for details.  This file is part of the hIPPYlib library. For more information and source code\navailability see https://hippylib.github.io.  hIPPYlib is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License (as published by the Free Software Foundation) version 2.0 dated June 1991.", 
            "title": "6. Dependence on the PDE coefficients"
        }, 
        {
            "location": "/building/", 
            "text": "Installation\n\n\nDownload hippylib\n\n\nRelease 1.3.0 of hIPPYlib is available for download \nhere\n.\nSimply decompress the file \nhippylib-1.3.0.tgz\n in your working directory.\n\n\nThere is no installation necessary.\nYou can directly run the examples from the \napplication\n folder or view the notebooks from the \ntutorial\n folder. \n\n\nTo checkout the development version of hippylib use the command\n\n\ngit clone https://github.com/hippylib/hippylib.git \n\n\n\n\nhIPPYlib depends on \nFEniCS\n version 1.6 or\nabove.  The suggested version of FEniCS to use with hIPPYlib is\n2016.2.\n\n\nFEniCS needs to be built with the following dependecies:\n\n\n\n\nnumpy, scipy, matplotlib\n\n\nPETSc and petsc4py (version 3.7.0 or above)\n\n\nSLEPc and slepc4py (version 3.7.0 or above)\n\n\nPETSc dependencies: parmetis, scotch, suitesparse, superlu_dist, ml\n\n\n(optional): mshr, jupyter\n\n\n\n\nBuid FEniCS from source using hashdist (Linux and MacOS)\n\n\nTo build FEniCS from source we suggest using the scripts and profile\nfiles in \nfenics-hashdist\n.\n\n\nThese scripts and profile files contain\nsmall modifications with respect to the ones provided by the FEniCS\ncommunity to ensure that all the dependencies needed by hIPPYlib are\ninstalled.\n\n\nStep-by-step build\n\n\n\n\nSelect the correct hashdist profile file\n\n\n\n\nIf you are running MacOS\n\n\nln -s local-darwin.yaml local.yaml\n\n\n\n\nIf you are running Linux:\n\n\nln -s local-linux.yaml local.yaml\n\n\n\n\n\n\nBuild FEniCS and all its dependencies\n\n\n\n\nchmod +x fenics-install.sh\n./fenics-install.sh\n\n\n\n\nThis can take several hours.\n\n\nWhen it completes, a file \nfenics.custom\n will be generated.\nThis files contains all the paths you need to add to your enviroment to run FEniCS.\n\n\n\n\nSource the fenics configuration file\n\n\n\n\nEverytime you open a new shell, you will have to add all the FEniCS\npaths to your enviroment before you can use FEniCS.\n\n\nsource \nHIPPYLIB_BASE_DIR\n/fenics-hashdist/fenics.custom\n\n\n\n\nwhere \nHIPPYLIB_BASE_DIR\n is the absolute path to the folder where\nhIPPYlib resides.\n\n\nSee \nfenics-hashdist/README.md\n\nfor further details.\n\n\nRun FEniCS from Docker (Linux, MacOS, Windows)\n\n\nAn easy way to run FEniCS is to use their prebuilt Docker images.\n\n\nFirst you will need to install \nDocker\n on\nyour system.  MacOS and Windows users should preferably use \nDocker\nfor Mac\n or \nDocker for Windows\n --- if it is compatible with their\nsystem --- instead of the legacy version \nDocker Toolbox\n.\n\n\nAmong the many docker's workflow discussed\n\nhere\n,\nwe suggest using the \nJupyter notebook\n\n\none\n.\n\n\nDocker for Mac, Docker for Windows and Linux users (\nSetup and first use\n)\n\n\nWe first create a new Docker container to run the \njupyter-notebook\n\ncommand and to expose port \n8888\n.\n\n\nFrom a command line shell, go to the \nhippylib\n folder and type:\n\n\ndocker run --name hippylib-nb -w /home/fenics/hippylib -v $(pwd):/home/fenics/hippylib -d -p 127.0.0.1:8888:8888 quay.io/fenicsproject/stable:2016.2.0 'jupyter-notebook --ip=0.0.0.0'\ndocker logs hippylib-nb\n\n\n\n\nThe notebook will be available at\n\nhttp://localhost:8888/?token=\nsecurity_token_for_first_time_connection\n\nin your web browser.\n\n\nFrom there you can run the interactive notebooks\nor create a new shell (directly from your browser) to run python\nscripts.\n\n\nDocker Toolbox users on Mac/Windows (\nSetup and first use\n)\n\n\nDocker Toolbox is for older Mac and Windows systems that do not meet\nthe requirements of \nDocker for Mac\n or \nDocker for Windows\n.  Docker\nToolbox will first create a lightweight linux virtual machine on your\nsystem and run docker from the virtual machine.  This has implications\non the workflow presented above.\n\n\nWe first create a new Docker container to run the 'jupyter-notebook' command and to expose port \n8888\n on the virtual machine.\n\n\nFrom a command line shell, go to the \nhippylib\n folder and type:\n\n\ndocker run --name hippylib-nb -w /home/fenics/hippylib -v $(pwd):/home/fenics/hippylib -d -p $(docker-machine ip $(docker-machine active)):8888:8888 quay.io/fenicsproject/stable:2016.2.0 'jupyter-notebook --ip=0.0.0.0'\ndocker logs hippylib-nb\n\n\n\n\nTo find out the IP of the virtual machine we type:\n\n\ndocker-machine ip $(docker-machine active)\n\n\n\n\nThe notebook will be available at \nhttp://\nip-of-virtual-machine\n:8888/?token=\nsecurity_token_for_first_time_connection\n in your web browser.\nFrom there you can run the interactive notebooks or create a new shell (directly from your browser) to run python scripts.\n\n\nSubsequent uses\n\n\nThe docker container will continue to run in the background until we stop it:\n\n\ndocker stop hippylib-nb\n\n\n\n\nTo start it again just run:\n\n\ndocker start hippylib-nb\n\n\n\n\nIf you would like to see the log output from the Jupyter notebook server (e.g. if you need the security token) type:\n\n\ndocker logs hippylib-nb\n\n\n\n\nOther ways to build FEniCS\n\n\nFor instructions on other ways to build FEniCS (e.g. using \nAnaconda\n\nor \napt-get\n in Ubuntu), we refer to the FEniCS project \ndownload\npage\n.  Note that this\ninstructions always refer to the latest version of FEniCS which may or\nmay not be yet supported by hIPPYlib. Always check the hIPPYlib\nwebsite for supported FEniCS versions.", 
            "title": "Installation"
        }, 
        {
            "location": "/building/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/building/#download-hippylib", 
            "text": "Release 1.3.0 of hIPPYlib is available for download  here .\nSimply decompress the file  hippylib-1.3.0.tgz  in your working directory.  There is no installation necessary.\nYou can directly run the examples from the  application  folder or view the notebooks from the  tutorial  folder.   To checkout the development version of hippylib use the command  git clone https://github.com/hippylib/hippylib.git   hIPPYlib depends on  FEniCS  version 1.6 or\nabove.  The suggested version of FEniCS to use with hIPPYlib is\n2016.2.  FEniCS needs to be built with the following dependecies:   numpy, scipy, matplotlib  PETSc and petsc4py (version 3.7.0 or above)  SLEPc and slepc4py (version 3.7.0 or above)  PETSc dependencies: parmetis, scotch, suitesparse, superlu_dist, ml  (optional): mshr, jupyter", 
            "title": "Download hippylib"
        }, 
        {
            "location": "/building/#buid-fenics-from-source-using-hashdist-linux-and-macos", 
            "text": "To build FEniCS from source we suggest using the scripts and profile\nfiles in  fenics-hashdist .  These scripts and profile files contain\nsmall modifications with respect to the ones provided by the FEniCS\ncommunity to ensure that all the dependencies needed by hIPPYlib are\ninstalled.  Step-by-step build   Select the correct hashdist profile file   If you are running MacOS  ln -s local-darwin.yaml local.yaml  If you are running Linux:  ln -s local-linux.yaml local.yaml   Build FEniCS and all its dependencies   chmod +x fenics-install.sh\n./fenics-install.sh  This can take several hours.  When it completes, a file  fenics.custom  will be generated.\nThis files contains all the paths you need to add to your enviroment to run FEniCS.   Source the fenics configuration file   Everytime you open a new shell, you will have to add all the FEniCS\npaths to your enviroment before you can use FEniCS.  source  HIPPYLIB_BASE_DIR /fenics-hashdist/fenics.custom  where  HIPPYLIB_BASE_DIR  is the absolute path to the folder where\nhIPPYlib resides.  See  fenics-hashdist/README.md \nfor further details.", 
            "title": "Buid FEniCS from source using hashdist (Linux and MacOS)"
        }, 
        {
            "location": "/building/#run-fenics-from-docker-linux-macos-windows", 
            "text": "An easy way to run FEniCS is to use their prebuilt Docker images.  First you will need to install  Docker  on\nyour system.  MacOS and Windows users should preferably use  Docker\nfor Mac  or  Docker for Windows  --- if it is compatible with their\nsystem --- instead of the legacy version  Docker Toolbox .  Among the many docker's workflow discussed here ,\nwe suggest using the  Jupyter notebook  one .  Docker for Mac, Docker for Windows and Linux users ( Setup and first use )  We first create a new Docker container to run the  jupyter-notebook \ncommand and to expose port  8888 .  From a command line shell, go to the  hippylib  folder and type:  docker run --name hippylib-nb -w /home/fenics/hippylib -v $(pwd):/home/fenics/hippylib -d -p 127.0.0.1:8888:8888 quay.io/fenicsproject/stable:2016.2.0 'jupyter-notebook --ip=0.0.0.0'\ndocker logs hippylib-nb  The notebook will be available at http://localhost:8888/?token= security_token_for_first_time_connection \nin your web browser.  From there you can run the interactive notebooks\nor create a new shell (directly from your browser) to run python\nscripts.  Docker Toolbox users on Mac/Windows ( Setup and first use )  Docker Toolbox is for older Mac and Windows systems that do not meet\nthe requirements of  Docker for Mac  or  Docker for Windows .  Docker\nToolbox will first create a lightweight linux virtual machine on your\nsystem and run docker from the virtual machine.  This has implications\non the workflow presented above.  We first create a new Docker container to run the 'jupyter-notebook' command and to expose port  8888  on the virtual machine.  From a command line shell, go to the  hippylib  folder and type:  docker run --name hippylib-nb -w /home/fenics/hippylib -v $(pwd):/home/fenics/hippylib -d -p $(docker-machine ip $(docker-machine active)):8888:8888 quay.io/fenicsproject/stable:2016.2.0 'jupyter-notebook --ip=0.0.0.0'\ndocker logs hippylib-nb  To find out the IP of the virtual machine we type:  docker-machine ip $(docker-machine active)  The notebook will be available at  http:// ip-of-virtual-machine :8888/?token= security_token_for_first_time_connection  in your web browser.\nFrom there you can run the interactive notebooks or create a new shell (directly from your browser) to run python scripts.  Subsequent uses  The docker container will continue to run in the background until we stop it:  docker stop hippylib-nb  To start it again just run:  docker start hippylib-nb  If you would like to see the log output from the Jupyter notebook server (e.g. if you need the security token) type:  docker logs hippylib-nb", 
            "title": "Run FEniCS from Docker (Linux, MacOS, Windows)"
        }, 
        {
            "location": "/building/#other-ways-to-build-fenics", 
            "text": "For instructions on other ways to build FEniCS (e.g. using  Anaconda \nor  apt-get  in Ubuntu), we refer to the FEniCS project  download\npage .  Note that this\ninstructions always refer to the latest version of FEniCS which may or\nmay not be yet supported by hIPPYlib. Always check the hIPPYlib\nwebsite for supported FEniCS versions.", 
            "title": "Other ways to build FEniCS"
        }, 
        {
            "location": "/download/", 
            "text": "Download\n\n\nLatest release\n\n\n\n\nDownload \nhippylib-1.3.0.tar.gz\n\n\n\n\nAll releases\n\n\n\n\n\n\n\n\nFilename\n\n\nVersion\n\n\nRelease Date\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\n\nhippylib-1.3.0.tar.gz\n\n\nv1.3.0\n\n\nJune 2017\n\n\nSupport for FEniCS 2017.1; GLPv2\n\n\n\n\n\n\n\n\nhippylib-1.2.0.tar.gz\n\n\nv1.2.0\n\n\nApr 2017\n\n\nSupport for FEniCS 2016.2\n\n\n\n\n\n\n\n\nhippylib-1.1.0.tar.gz\n\n\nv1.1.0\n\n\nNov 2016\n\n\nSupport for FEniCS 2016.1\n\n\n\n\n\n\n\n\nhippylib-1.0.2.tar.gz\n\n\nv1.0.2\n\n\nSept 2016\n\n\nEnchantment\n\n\n\n\n\n\n\n\nhippylib-1.0.1.tar.gz\n\n\nv1.0.1\n\n\nAug 2016\n\n\nBugfix\n\n\n\n\n\n\n\n\nhippylib-1.0.0.tar.gz\n\n\nv1.0.0\n\n\nAug 2016\n\n\nInitial release", 
            "title": "Download"
        }, 
        {
            "location": "/download/#download", 
            "text": "", 
            "title": "Download"
        }, 
        {
            "location": "/download/#latest-release", 
            "text": "Download  hippylib-1.3.0.tar.gz", 
            "title": "Latest release"
        }, 
        {
            "location": "/download/#all-releases", 
            "text": "Filename  Version  Release Date  Notes       hippylib-1.3.0.tar.gz  v1.3.0  June 2017  Support for FEniCS 2017.1; GLPv2     hippylib-1.2.0.tar.gz  v1.2.0  Apr 2017  Support for FEniCS 2016.2     hippylib-1.1.0.tar.gz  v1.1.0  Nov 2016  Support for FEniCS 2016.1     hippylib-1.0.2.tar.gz  v1.0.2  Sept 2016  Enchantment     hippylib-1.0.1.tar.gz  v1.0.1  Aug 2016  Bugfix     hippylib-1.0.0.tar.gz  v1.0.0  Aug 2016  Initial release", 
            "title": "All releases"
        }, 
        {
            "location": "/research/", 
            "text": "Research\n\n\nApplications\n\n\n\n\n\n\nInference, prediction and optimization under uncertainty for turbulent combustion\n\n\n\n\n\n\nInversion and control for CO\n2\n sequestration with poroelastic models\n\n\n\n\n\n\nJoint seismic-electromagnetic inversion\n\n\n\n\n\n\nInference of basal boundary conditions for ice sheet flow\n\n\n\n\n\n\nInversion for coupled ice-ocean interaction\n\n\n\n\n\n\nInversion for material properties of cardiac tissue\n\n\n\n\n\n\nInference of constitutive laws in mechanics of nano-scale filaments\n\n\n\n\n\n\nPublications\n\n\n\n\nU. Villa, N. Petra, O. Ghattas, \nhIPPYlib: An extensible software framework for large-scale deterministic and linearized Bayesian inverse problems\n, 2017\n\n\n\n\nOral Presentations\n\n\n\n\n\n\nU. Villa, \nTaylor Approximation for PDE-constrained Optimal Control Problems under High-dimensional Uncertainty\n, SIAM Control, July 10 - 12, 2017, Pittsburgh, Pa, US\n\n\n\n\n\n\nU. Villa, \nDerivative-Informed MCMC for Bayesian Calibration of Stochastic PDE Models\n, SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US\n\n\n\n\n\n\nU. Villa, \nHessian-based sampling techniques for Bayesian inverse problems with stochastic PDE forward model\n, Applied Inverse Problems, May 29 - June 2, 2017, Hangzhou, China\n\n\n\n\n\n\nU. Villa, \nBayesian Calibration of Inadequate Stochastic PDE Models\n, SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nP. Chen, \nTaylor Approximation for PDE-Constrained Optimal Control Problems Under High-Dimensional Uncertainty: Application to a Turbulence Model\n, SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nB. Crestel, \nScalable Solvers for Joint Inversion with Several Structural Coupling Terms\n, SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nAmal Alghamdi, \nBayesian Inversion for Subsurface Properties from Poroelastic Forward Models and Surface Deformation Data\n, SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nU. Villa, \nAn Analytical Technique for Forward and Inverse Propagation of Uncertainty\n, SIAM UQ, April 5-8, 2016, Lausanne, Switzerland\n\n\n\n\n\n\nPoster Presentations\n\n\n\n\n\n\nK. Koval, G. Stadler, \nComputational Approaches for Linear Goal-Oriented Bayesian Inverse Problems\n, SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US\n\n\n\n\n\n\nJ. Chen, A. Drach, U. Villa, R. Avazmohammadi, D. Li, O. Ghattas and M. Sacks, \nIdentification of Mechanical Properties of 3D Myocardial Tissue: An Inverse Modeling and Optimal Experimental Design Problem\n, FEniCS'17, June 12-14, 2017, University of Luxembourg, Luxembourg\n\n\n\n\n\n\nT. O\u2019Leary-Roseberry, U. Villa, O. Ghattas, P. Heimbach, \nAn Adjoint Capable Solver for the Stefan Problem: a Bilevel Optimization and Level Set Approach\n, SIAM CSE, Feb. 27 - March 3, 2017, Atlanta, GA, US\n\n\n\n\n\n\nO. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa, \nIntegrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion\n, NSF-SI2 PI meeting, Feb. 21-22, 2017, Arlington, VA, US", 
            "title": "Research"
        }, 
        {
            "location": "/research/#research", 
            "text": "", 
            "title": "Research"
        }, 
        {
            "location": "/research/#applications", 
            "text": "Inference, prediction and optimization under uncertainty for turbulent combustion    Inversion and control for CO 2  sequestration with poroelastic models    Joint seismic-electromagnetic inversion    Inference of basal boundary conditions for ice sheet flow    Inversion for coupled ice-ocean interaction    Inversion for material properties of cardiac tissue    Inference of constitutive laws in mechanics of nano-scale filaments", 
            "title": "Applications"
        }, 
        {
            "location": "/research/#publications", 
            "text": "U. Villa, N. Petra, O. Ghattas,  hIPPYlib: An extensible software framework for large-scale deterministic and linearized Bayesian inverse problems , 2017", 
            "title": "Publications"
        }, 
        {
            "location": "/research/#oral-presentations", 
            "text": "U. Villa,  Taylor Approximation for PDE-constrained Optimal Control Problems under High-dimensional Uncertainty , SIAM Control, July 10 - 12, 2017, Pittsburgh, Pa, US    U. Villa,  Derivative-Informed MCMC for Bayesian Calibration of Stochastic PDE Models , SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US    U. Villa,  Hessian-based sampling techniques for Bayesian inverse problems with stochastic PDE forward model , Applied Inverse Problems, May 29 - June 2, 2017, Hangzhou, China    U. Villa,  Bayesian Calibration of Inadequate Stochastic PDE Models , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US    P. Chen,  Taylor Approximation for PDE-Constrained Optimal Control Problems Under High-Dimensional Uncertainty: Application to a Turbulence Model , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US    B. Crestel,  Scalable Solvers for Joint Inversion with Several Structural Coupling Terms , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US    Amal Alghamdi,  Bayesian Inversion for Subsurface Properties from Poroelastic Forward Models and Surface Deformation Data , SIAM CSE, Feb 27-March 3, 2017, Atlanta, GA, US    U. Villa,  An Analytical Technique for Forward and Inverse Propagation of Uncertainty , SIAM UQ, April 5-8, 2016, Lausanne, Switzerland", 
            "title": "Oral Presentations"
        }, 
        {
            "location": "/research/#poster-presentations", 
            "text": "K. Koval, G. Stadler,  Computational Approaches for Linear Goal-Oriented Bayesian Inverse Problems , SIAM Annual, July 10 - 14, 2017, Pittsburgh, Pa, US    J. Chen, A. Drach, U. Villa, R. Avazmohammadi, D. Li, O. Ghattas and M. Sacks,  Identification of Mechanical Properties of 3D Myocardial Tissue: An Inverse Modeling and Optimal Experimental Design Problem , FEniCS'17, June 12-14, 2017, University of Luxembourg, Luxembourg    T. O\u2019Leary-Roseberry, U. Villa, O. Ghattas, P. Heimbach,  An Adjoint Capable Solver for the Stefan Problem: a Bilevel Optimization and Level Set Approach , SIAM CSE, Feb. 27 - March 3, 2017, Atlanta, GA, US    O. Ghattas, Y. Marzouk, M. Parno, N. Petra, U. Villa,  Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SI2 PI meeting, Feb. 21-22, 2017, Arlington, VA, US", 
            "title": "Poster Presentations"
        }, 
        {
            "location": "/outreach/", 
            "text": "Outreach\n\n\nSchools and workshops\n\n\n\n\n\n\nO. Ghattas, Y. Marzouk, M. Parno, N. Petra, G. Stadler, U. Villa, \nInverse Problems:\nSystematic Integration of Data with Models under Uncertainty\n, Gene Golub SIAM Summer School, Breckenridge, CO, June 2018. \nEarly announement flyer\n; more details coming soon.\n\n\n\n\n\n\nN. Petra, \nSAMSI Optimization Program Summer School\n, Research Triangle Park, NC,  August 8-12, 2016\n\n\n\n\n\n\nN. Petra and O. Ghattas, \nIDEALab: Inverse Problems and Uncertainty Quantification\n, Brown University, Providence, RD, July 6-10, 2015\n\n\n\n\n\n\nGraduate level courses\n\n\n\n\n\n\nA. Alexanderian @NC State, \nInverse problems\n, Fall 2016\n\n\n\n\n\n\nG. Stadler @NYU, \nAdvanced Topics in Numerical Analysis: Computational and Variational Methods for Inverse Problems\n, Spring 2016 \nlink\n\n\n\n\n\n\nN. Petra @UC Merced, \nSpecial Topics: Computational and Variational Inverse Problems\n, Fall 2015 \nlink\n\n\n\n\n\n\nO. Ghattas @UT Austin, \nComputational and Variational Inverse Problems\n, Fall 2015 \nlink\n\n\n\n\n\n\nGrants\n\n\n\n\n\n\n\nPIs: O. Ghattas, M. Parno, N. Petra, Y. Marzouk, U. Villa, \nIntegrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion\n, NSF-SSI2", 
            "title": "Outreach"
        }, 
        {
            "location": "/outreach/#outreach", 
            "text": "", 
            "title": "Outreach"
        }, 
        {
            "location": "/outreach/#schools-and-workshops", 
            "text": "O. Ghattas, Y. Marzouk, M. Parno, N. Petra, G. Stadler, U. Villa,  Inverse Problems:\nSystematic Integration of Data with Models under Uncertainty , Gene Golub SIAM Summer School, Breckenridge, CO, June 2018.  Early announement flyer ; more details coming soon.    N. Petra,  SAMSI Optimization Program Summer School , Research Triangle Park, NC,  August 8-12, 2016    N. Petra and O. Ghattas,  IDEALab: Inverse Problems and Uncertainty Quantification , Brown University, Providence, RD, July 6-10, 2015", 
            "title": "Schools and workshops"
        }, 
        {
            "location": "/outreach/#graduate-level-courses", 
            "text": "A. Alexanderian @NC State,  Inverse problems , Fall 2016    G. Stadler @NYU,  Advanced Topics in Numerical Analysis: Computational and Variational Methods for Inverse Problems , Spring 2016  link    N. Petra @UC Merced,  Special Topics: Computational and Variational Inverse Problems , Fall 2015  link    O. Ghattas @UT Austin,  Computational and Variational Inverse Problems , Fall 2015  link", 
            "title": "Graduate level courses"
        }, 
        {
            "location": "/outreach/#grants", 
            "text": "PIs: O. Ghattas, M. Parno, N. Petra, Y. Marzouk, U. Villa,  Integrating Data with Complex Predictive Models under Uncertainty: An Extensible Software Framework for Large-Scale Bayesian Inversion , NSF-SSI2", 
            "title": "Grants"
        }, 
        {
            "location": "/postdoc_position/", 
            "text": "Announcements\n\n\nOpen Postdoc Position at UC Merced\n\n\nThere is an opening for a postdoc position in Professor \nNoemi Petra\n's\nresearch group in the \nSchool of Natural Sciences\n at the \nUniversity of\nCalifornia, Merced\n.\n\n\nThe postdoctoral researcher will work under an\nNSF-funded Collaborative Research (with UC Merced, UT Austin and MIT)\nentitled: \nIntegrating Data with Complex Predictive Models under\nUncertainty: An Extensible Software Framework for Large-Scale Bayesian\nInversion\n (see \nhere\n for a general overview of the project).\n\n\nThe postdoctoral researcher will perform research in the\nfield of large-scale Bayesian inverse problems, and on the\nimplementation of new features in hIPPYlib. The postdoc will contribute to\nthe research dissemination and will also help build the user/developer\ncommunity by attending and speaking at conferences, workshops and\nsummer schools at local and international events.\n\n\nInterested candidates should contact Noemi Petra at\n\nnpetra@ucmerced.edu\n.\n\n\nApply at \nhttps://aprecruit.ucmerced.edu/apply/JPF00505\n.", 
            "title": "Announcements"
        }, 
        {
            "location": "/postdoc_position/#announcements", 
            "text": "", 
            "title": "Announcements"
        }, 
        {
            "location": "/postdoc_position/#open-postdoc-position-at-uc-merced", 
            "text": "There is an opening for a postdoc position in Professor  Noemi Petra 's\nresearch group in the  School of Natural Sciences  at the  University of\nCalifornia, Merced .  The postdoctoral researcher will work under an\nNSF-funded Collaborative Research (with UC Merced, UT Austin and MIT)\nentitled:  Integrating Data with Complex Predictive Models under\nUncertainty: An Extensible Software Framework for Large-Scale Bayesian\nInversion  (see  here  for a general overview of the project).  The postdoctoral researcher will perform research in the\nfield of large-scale Bayesian inverse problems, and on the\nimplementation of new features in hIPPYlib. The postdoc will contribute to\nthe research dissemination and will also help build the user/developer\ncommunity by attending and speaking at conferences, workshops and\nsummer schools at local and international events.  Interested candidates should contact Noemi Petra at npetra@ucmerced.edu .  Apply at  https://aprecruit.ucmerced.edu/apply/JPF00505 .", 
            "title": "Open Postdoc Position at UC Merced"
        }, 
        {
            "location": "/about/", 
            "text": "About hIPPYlib\n\n\nAuthors\n\n\n\n\nOmar Ghattas\n\n\nNoemi Petra\n\n\nUmberto Villa\n\n\n\n\nContributors\n\n\n\n\nAmal Alghamdi\n\n\nJoshua Chen\n\n\nPeng Chen\n\n\nBen Crestel\n\n\nTom O'Leary-Roseberry\n\n\nVishwas Rao\n\n\n\n\nCopyright\n\n\n 2016 The University of Texas at Austin, University of California Merced.\n\n\nLicense\n\n\n\n\nGNU General Public License version 2 (GPL)\n\n\nOlder Releases (1.2.0 or older): \nGNU General Public License version 3 (GPL)\n\n\n\n\n\n\n\n\nWebsite built with \nMkDocs\n, \nbootstrap\n, \nbootswatch\n, and \nMathJax\n.\nHosted on \nGitHub\n.", 
            "title": "About"
        }, 
        {
            "location": "/about/#about-hippylib", 
            "text": "", 
            "title": "About hIPPYlib"
        }, 
        {
            "location": "/about/#authors", 
            "text": "Omar Ghattas  Noemi Petra  Umberto Villa", 
            "title": "Authors"
        }, 
        {
            "location": "/about/#contributors", 
            "text": "Amal Alghamdi  Joshua Chen  Peng Chen  Ben Crestel  Tom O'Leary-Roseberry  Vishwas Rao", 
            "title": "Contributors"
        }, 
        {
            "location": "/about/#copyright", 
            "text": "2016 The University of Texas at Austin, University of California Merced.", 
            "title": "Copyright"
        }, 
        {
            "location": "/about/#license", 
            "text": "GNU General Public License version 2 (GPL)  Older Releases (1.2.0 or older):  GNU General Public License version 3 (GPL)     Website built with  MkDocs ,  bootstrap ,  bootswatch , and  MathJax .\nHosted on  GitHub .", 
            "title": "License"
        }
    ]
}